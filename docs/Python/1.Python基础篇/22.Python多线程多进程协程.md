# 22.Python 多线程多进程协程

## 1.多线程与多进程

> 进程和线程的概念。

进程（Process）实际上表示的就是计算机正在进行的一个任务，比如，打开一个浏览器便是启动一个浏览器进程，打开一个记事本便是启动一个记事本进程。

但是，一个进程未必只能进行一件事，就像一个 Word 进程，在打字的同时还会有拼写检查，这些在进程内部同时进行的多个“子任务”，就称为线程（Thread）。

进程和线程的主要差别在于它们是不同的操作系统资源管理方式。

进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。

线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。

在以往的单核 CPU 上，系统执行多进程的方式是通过不断的在多个进程中切换——例如任务 1 执行 0.01 秒，切到任务 2 执行 0.01 秒再切到任务 3……以此类推，而在多核 CPU 出现后，真正的并行执行多任务才真正的得以实现，但绕是如此，一台计算机同时进行的进程是非常之多的，远远大于 CPU 的核心数量，因此，操作系统依然会将这些任务轮流调度到每个核心上运行。

如果我们要同时进行多个任务，我们有以下三种方案：

1. 写多个程序，然后同时运行
2. 在一个程序中运行多个线程
3. 多进程+多线程

### 1.1 多进程

要让 Python 程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。

Unix/Linux 操作系统提供了一个 fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是 fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

子进程永远返回 0，而父进程返回子进程的 ID。这样做的理由是，一个父进程可以 fork 出很多子进程，所以，父进程要记下每个子进程的 ID，而子进程只需要调用 getppid()就可以拿到父进程的 ID。

Python 的 os 模块封装了常见的系统调用，其中就包括 fork，可以在 Python 程序中轻松创建子进程：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print "我是子进程%s, 我的父进程是%s 我开始工作了..." % (os.getpid(), os.getppid())
    time.sleep(3)
else:
    print('我的进程id是 (%s) 我创建了一个子进程，子进程id是 (%s).' % (os.getpid(), pid))
    time.sleep(4)
```

运行结果如下：

```sh
$ python 01.os-fork-sample.py
Process (1695) start...
我的进程id是 (1695) 我创建了一个子进程，子进程id是 (1696).
我是子进程1696, 我的父进程是1695 我开始工作了...
```

由于 Windows 没有 fork 调用，上面的代码在 Windows 上无法运行。由于 Mac 系统是基于 BSD（Unix 的一种）内核，所以，在 Mac 下运行是没有问题的，推荐大家用 Mac！

有了 fork 调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的 Apache 服务器就是由父进程监听端口，每当有新的 http 请求时，就 fork 出子进程来处理新的 http 请求。

#### multiprocessing

如果你打算编写多进程的服务程序，Unix/Linux 无疑是正确的选择。由于 Windows 没有 fork 调用，难道在 Windows 上无法用 Python 编写多进程的程序？

由于 Python 是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing 模块就是跨平台版本的多进程模块。

##### Process

multiprocessing 模块提供了一个 Process 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from multiprocessing import Process
import os


# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))


if __name__ == '__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果如下：

```sh
$ /usr/bin/python 02.Process-sample.py
Parent process 1712.
Child process will start.
Run child process test (1723)...
Child process end.
```

Process 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个 Process 实例，用 start()方法启动，这样创建进程比 fork()还要简单。
join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

##### Process 子类创建进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time
from multiprocessing import Process


class Download(Process):
    def __init__(self, interval):
        Process.__init__(self)
        self.interval = interval

    # 重写Process类中的run()方法
    def run(self):
        # 开启这个进程所需执行的代码
        t_start = time.time()
        # time.sleep(3)     # 模拟阻塞的一个实现方式
        print("开启进程：%s进行下载操作" % os.getpid())
        print("子进程（%s）开始执行，父进程为（%s）" % (os.getpid(), os.getppid()))
        time.sleep(self.interval)
        t_stop = time.time()
        print("子进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))


if __name__ == '__main__':
    t_start = time.time()
    print("当前进程（%s）" % os.getpid())
    p = Download(2)
    p.start()
    # p.join(10)        # join 父进程等待子进程执行完毕后立刻执行
    time.sleep(10)      # 模拟阻塞，保证子进程完毕后父进程在执行
    t_stop = time.time()
    print("主进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))
```

##### 使用进程池 Pool 创建进程

如果要启动大量的子进程，可以用进程池的方式批量创建子进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import multiprocessing as mp
import os
import time


def task(name):
    print("子进程 ({})执行的任务是 ({})".format(os.getpid(), name))
    time.sleep(1)


if __name__ == '__main__':
    print("父进程 ({})开始执行".format(os.getpid()))
    p = mp.Pool(4)
    for i in range(10):
        p.apply_async(task, args=(i,))

    p.close()
    p.join()
    print("所有子进程结束.....")
```

请注意输出的结果，

```sh
$ /usr/bin/python 04.Process-pool-sample.p
父进程 (2033)开始执行
子进程 (2047)执行的任务是 (0)
子进程 (2044)执行的任务是 (1)
子进程 (2045)执行的任务是 (2)
子进程 (2046)执行的任务是 (3)
子进程 (2047)执行的任务是 (4)
子进程 (2044)执行的任务是 (5)
子进程 (2045)执行的任务是 (6)
子进程 (2046)执行的任务是 (7)
子进程 (2047)执行的任务是 (8)
子进程 (2044)执行的任务是 (9)
所有子进程结束.....
```

说明：

1. 进程池 Pool 被创建出来后， `p.apply_async(task)` 语句不停地循环执行，相当于向进程池中提交了 10 个请求，它们会被放到一个队列中。
2. p = mp.Pool(4) 执行完毕后创建了 4 条进程，但尚未给它们分配各自的任务；也就意味着，无论有多少任务，实际的进程数只有 4 条，每次最多 4 条进程并行。
3. 当 Pool 中有进程任务执行完毕后，这条进程资源会被释放，Pool 会按先进先出的原则取出一个新的请求给空闲的进程继续执行。
4. 当 Pool 所有的进程任务完成后，会产生 4 个僵尸进程，如果主进程/主线程不结束，系统不会自动回收资源，需要调用 join 函数负责回收。
5. 在创建 Pool 进程池时，若不指定进程的最大数量，默认创建的进程数为系统的内核数量
6. 如果采用 `p.apply(task)`阻塞方式添加任务，其每次只能向进程池中添加一条任务，然后 for 循环会被阻塞等待，直到添加的任务被执行完毕，进程池中的 4 个进程交替执行新来的任务，此时相当于单进程。

进程池扫描主机端口实例

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# @auther:   18793
# @Date：    2020/6/22 11:21
# @filename: Process_Pool.py
# @Email:    1879324764@qq.com
# @Software: PyCharm

"""
进程池扫描主机端口实例
代码利用单进程扫描主机端口，如果要扫描的端口范围比较大，则需要耗费比较长的时间。
利用多个进程同时扫描不同的端口范围，可以缩短程序运行时间。
进程池技术可以一次创建多个子进程，适合于子进程数量事先预知的情况。
代码利用进程池一次创建16个进程，然后利用这些进程扫描主机所有端口（0～65535），
每个进程扫描4096个端口。
"""
from multiprocessing import Pool
import os
import socket


def scan_port(ports):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    for port in range(ports, ports + 4096):
        result = s.connect_ex((ip, port))
        if result == 0:
            print("I am process %d,port %d is openned!" % (os.getpid(), port))
    s.close()


ip = "192.168.0.109"
p = Pool(16)

for k in range(16):
    p.apply_async(scan_port, args=(k * 4096,))
p.close()
p.join()
print("All subprocesses had finished!")
```

##### concurrent.futures 进程池

concurrent.futures - 启动并行任务

版本 3.2 中的新功能。

ProcessPoolExecutor 示例

官方示例

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```

以下是使用 `concurrent.futures.ProcessPoolExecutor` 来并行扫描主机端口的简单示例代码：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket
import multiprocessing


# 定义一个函数，用于扫描指定主机和端口是否开放
def scan_port(host, port):
    try:
        # 创建套接字对象
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            # 设置超时时间
            s.settimeout(1)
            # 尝试连接指定主机和端口
            result = s.connect_ex((host, port))
            # 如果连接成功，则端口开放
            if result == 0:
                return port, True
            else:
                return port, False
    except Exception as e:
        return port, False


if __name__ == '__main__':
    # 设置要扫描的主机和端口范围
    host = 'localhost'
    ports = range(1, 1025)  # 扫描1到1024端口

    # 获取系统的CPU核心数量
    max_workers = multiprocessing.cpu_count()

    # 创建进程池执行器，并设置最大工作进程数量
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        # 提交任务给进程池执行器，并获取结果
        futures = {executor.submit(scan_port, host, port): port for port in ports}

        # 遍历结果，打印出开放的端口
        for future in concurrent.futures.as_completed(futures):
            port, status = future.result()
            if status:
                print(f"Port {port} is open")
```

executor.map() 和 executor.submit() 是 concurrent.futures.ProcessPoolExecutor 提供的两种不同的任务提交方式，它们之间的核心区别在于：

1. 返回结果的方式：

- executor.map() 会返回一个迭代器，通过迭代器可以逐个获取每个任务的结果。
- executor.submit() 返回一个 concurrent.futures.Future 对象，通过它可以获取单个任务的结果。

2. 任务参数传递方式：

- executor.map() 接收一个可迭代对象作为参数，该可迭代对象的每个元素都是传递给目标函数的参数，然后按顺序执行函数。
- executor.submit() 接收一个函数和函数参数作为参数，可以灵活地传递任意数量的参数给目标函数。

3. 适用场景：

- executor.map() 适用于需要并行执行同一个函数，但参数不同的情况。例如，扫描多个端口或处理多个文件。
- executor.submit() 更加灵活，适用于需要每个任务具有不同函数、参数或处理逻辑的情况。

4. 并发性：

- executor.map() 在执行任务时，会按照顺序提交任务，并行执行，但无法控制每个任务的执行顺序。
- executor.submit() 允许更灵活的控制任务的提交和执行顺序，因为它返回的是 Future 对象，可以通过 Future 对象的方法来控制任务的状态和执行顺序。

总的来说，如果需要并行执行一组相似的任务，并且这些任务的参数可以通过迭代器提供，那么使用 executor.map() 更加简洁方便；如果需要对每个任务进行更精细的控制，例如指定特定的函数、参数或处理逻辑，那么使用 executor.submit() 更为合适。

##### 进程池效率对比

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket
from multiprocessing import Pool


def scan_port(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    result = s.connect_ex((ip, port))
    s.close()
    return port, result == 0  # 返回端口和状态


# 使用多进程 concurrent.futures.ProcessPoolExecutor map 方法执行
def multi_process_map_method():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = executor.map(scan_port, [ip] * len(ports), ports)  # 传递 ip 和 ports
        for port, status in results:
            if status:
                print(f"Port {port} is open")


# 使用多进程 concurrent.futures.ProcessPoolExecutor submit 方法执行
def multi_process_submit_method():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = {executor.submit(scan_port, ip, port): port for port in ports}  # 传递 ip 和 port
        for future in concurrent.futures.as_completed(futures):
            port, status = future.result()
            if status:
                print(f"Port {port} is open")


# 使用 进程池 Pool.apply_async 方法执行
def pool_apply_async():
    with Pool() as pool:
        results = [pool.apply_async(scan_port, (ip, port)) for port in ports]  # 传递 ip 和 port
        for result in results:
            port, status = result.get()
            if status:
                print(f"Port {port} is open")


if __name__ == "__main__":
    # 全局变量
    ip = "127.0.0.1"
    ports = range(1, 1024)  # 改为 range，从 1 到 1024
    import time

    start_time = time.time()
    multi_process_map_method()
    map_execution_time = time.time() - start_time
    print(f"Execution time using multi_process map method: {map_execution_time:.2f} seconds")

    start_time = time.time()
    multi_process_submit_method()
    submit_execution_time = time.time() - start_time
    print(f"Execution time using multi_process submit method: {submit_execution_time:.2f} seconds")

    start_time = time.time()
    pool_apply_async()
    pool_execution_time = time.time() - start_time
    print(f"Execution time using Pool.apply_async method: {pool_execution_time:.2f} seconds")
```

执行结果如下：

```sh
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using map method: 51.90 seconds
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using submit method: 51.92 seconds
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using Pool.apply_async method: 51.87 seconds
```

multiprocessing 模块使用的是进程池而不是线程池，创建和销毁进程的开销相对较大，导致性能很差。上面代码只是展示使用方法。对于端口扫描 I/O 密集型操作使用多线程才算正确的。

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket


def scan_port(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    result = s.connect_ex((ip, port))
    s.close()
    return port, result == 0  # 返回端口和状态


# 使用多进程 concurrent.futures.ThreadPoolExecutor map 方法执行
def multi_thread_map_method():
    with concurrent.futures.ThreadPoolExecutor(len(ports)) as executor:
        results = executor.map(scan_port, [ip] * len(ports), ports)  # 传递 ip 和 ports
        for port, status in results:
            if status:
                print(f"Port {port} is open")


if __name__ == "__main__":
    # 全局变量
    ip = "127.0.0.1"
    ports = range(1, 1024)  # 改为 range，从 1 到 1024
    import time

    start_time = time.time()
    multi_thread_map_method()
    map_execution_time = time.time() - start_time
    print(f"Execution time using multi_thread map method: {map_execution_time:.2f} seconds")
```

执行结果

```sh
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using multi_thread map method: 1.18 seconds
```

#### 进程间通信

- Python 提供了多种进程间通信的方式，例如 Queue、Pipe、Value+Array 等

Queue 和 Pipe 的区别在于

- Pipe 常用来在两个进程间通信
- Queue 用来在多个进程间实现通信。

##### Queue 多进程队列的使用

Queue 模块可以用来进行线程间的通信，让各个线程之间共享数据。

Python 的 Queue 模块提供了同步、线程安全的队列类， 包括 `FIFO（先入先出）队列` `Queue、LIFO（后入先出）队列` `LifoQueue` 和`优先级队列 PriorityQueue`。

这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列实现线程间的同步。

| 方法名             | 描述                                        |
| ------------------ | ------------------------------------------- |
| Queue()            | 创建一个空的队列                            |
| q.put(item)        | 将 item 放入队列                            |
| q.get()            | 从队列中移除并返回一个 item                 |
| q.empty()          | 如果队列为空，返回 True；否则返回 False     |
| q.full()           | 如果队列已满，返回 True；否则返回 False     |
| q.qsize()          | 返回队列中的 item 数量                      |
| q.put_nowait(item) | 将 item 放入队列，不等待队列可用            |
| q.get_nowait()     | 从队列中移除并返回一个 item，不等待队列可用 |
| q.task_done()      | 表示之前入队的任务已经完成                  |
| q.join()           | 阻塞直到队列中所有的 item 都被处理完毕      |

代码示例 1

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
from multiprocessing import Process, Queue
import time

'''
2个子进程在队列中进行写入和读取数据，实现进程之间的通信
'''


def write(q):
    if not q.full():
        for i in range(5):
            message = "消息" + str(i)
            q.put(message)
            print("写入:{}".format(message))


def read(q):
    time.sleep(1)
    while not q.empty():
        print("读取:{}".format(q.get(True, 2)))


if __name__ == '__main__':
    print("主进程开始".center(100, "*"))
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    pw.start()
    pr.start()
    pw.join()
    pr.join()
    print("主进程结束".center(100, "*"))

```

代码示例 2

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 10:55
# filename: 进程间通信01.py
from multiprocessing import Process, Queue
import os, time, random


# 写数据进程执行的代码
def proc_write(q, urls):
    print('Process(%s) is writing...' % os.getpid())
    for url in urls:
        q.put(url)
        print('Put %s to queue...' % url)
        time.sleep(random.random())


# 读进程执行的代码
def proc_read(q):
    print('Process(%s) is reading...' % os.getpid())
    while True:
        url = q.get(True)
        print('Get %s from queue.' % url)


if __name__ == '__main__':
    # 父进程创建Queue，并传给各个子进程
    q = Queue()
    proc_writer1 = Process(target=proc_write, args=(q, ['url1', 'url2', 'url3']))
    proc_writer2 = Process(target=proc_write, args=(q, ['url4', 'url5', 'url6']))
    proc_reader = Process(target=proc_read, args=(q,))
    # 启动子进程proc_writeer 写入
    proc_writer1.start()
    proc_writer2.start()
    # 启动子进程proc_reader,读取
    proc_reader.start()
    # 等待子进程proc_writer结束
    proc_writer1.join()
    proc_writer2.join()
    # proc_reader进程里是死循环，无法等待其结束，要强行终止
    proc_reader.terminate()

"""
Process(10608) is writing...
Put url1 to queue...
Process(7808) is writing...
Put url4 to queue...
Process(13840) is reading...
Get url1 from queue.
Get url4 from queue.
Put url5 to queue...
Get url5 from queue.
Put url2 to queue...
Get url2 from queue.
Put url6 to queue...
Get url6 from queue.
Put url3 to queue...
Get url3 from queue.
"""
```

##### Pipe 常用来在两个进程间进行通信

两个进程分别位于管道的两端。 Pipe 方法返回（conn1，conn2）代表一个管道的两个端。

Pipe 方法有 duplex 参数，如果 duplex 参数为 True（默认值），那么这个管道是全双工模式，也就是说 conn1 和 conn2 均可收发。

若 duplex 为 False，conn1 只负责接收消息，conn2 只负责发送消息。 send 和 recv 方法分别是发送和接收消息的方法。 例如，在全双工模式下，可以调用 conn1.send 发送消息，conn1.recv 接收消息。

如果没有消息可接收，recv 方法会一直阻塞。如果管道已经被关闭，那么 recv 方法会抛出 EOFError。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 11:08
# filename: sample01.py
import multiprocessing
import random
import time, os


def proc_send(pipe, urls):
    for url in urls:
        print("Process(%s) send :%s" % (os.getpid(), url))
        pipe.send(url)
        time.sleep(random.random())


def proc_recv(pipe):
    while True:
        print("Process(%s) rev:%s" % (os.getpid(), pipe.recv()))
        time.sleep(random.random())


if __name__ == '__main__':
    pipe = multiprocessing.Pipe()
    p1 = multiprocessing.Process(target=proc_send, args=(pipe[0], ["url_" + str(i) for i in range(10)]))
    p2 = multiprocessing.Process(target=proc_recv, args=(pipe[1],))
    p1.start()
    p2.start()
    p1.join()
    p1.join()
    p2.terminate()

"""
Process(17008) send :url_0
Process(13264) rev:url_0
Process(17008) send :url_1
Process(17008) send :url_2
Process(13264) rev:url_1
Process(17008) send :url_3
Process(17008) send :url_4
Process(13264) rev:url_2
Process(17008) send :url_5
Process(13264) rev:url_3
Process(17008) send :url_6
Process(17008) send :url_7
Process(13264) rev:url_4
Process(13264) rev:url_5
Process(17008) send :url_8
Process(17008) send :url_9
Process(13264) rev:url_6
"""
```

#### 分布式进程

分布式进程指的是将 Process 进程分布到多台机器上，充分利用多台机器的性能完成复杂的任务。 我们可以将这一点应用到分布式爬虫的开发中。

分布式进程在 Python 中依然要用到 multiprocessing 模块。 multiprocessing 模块不但支持多进程，其中 managers 子模块还支持把多进程分布到多台机器上。 可以写一个服务进程作为调度者，将任务分布到其他多个进程中，依靠网络通信进行管理。

举个例子：

在做爬虫程序时，常常会遇到这样的场景，我们想抓取某个网站的所有图片，如果使用多进程的话，
一般是一个进程负责抓取图片的链接地址，将链接地址存放到 Queue 中，
另外的进程负责从 Queue 中读取链接地址进行下载和存储到本地。
现在把这个过程做成分布式，一台机器上的进程负责抓取链接，
其他机器上的进程负责下载存储。
那么遇到的主要问题是将 Queue 暴露到网络中，
让其他机器进程都可以访问，分布式进程就是将这一个过程进行了封装，
我们可以将这个过程称为本地队列的网络化。

分布式进程 要实现上面例子的功能，创建分布式进程需要分为六个步骤：

1）建立队列 Queue，用来进行进程间的通信。服务进程创建任务队列 task_queue，用来作为传递任务给任务进程的通道；服务进程创建结果队列 result_queue，作为任务进程完成任务后回复服务进程的通道。在分布式多进程环境下，必须通过由 Queuemanager 获得的 Queue 接口来添加任务。

2）把第一步中建立的队列在网络上注册，暴露给其他进程（主机），注册后获得网络队列，相当于本地队列的映像。

3）建立一个对象（Queuemanager（BaseManager））实例 manager，绑定端口和验证口令。

4）启动第三步中建立的实例，即启动管理 manager，监管信息通道。

5）通过管理实例的方法获得通过网络访问的 Queue 对象，即再把网络队列实体化成可以使用的本地队列。

6）创建任务到“本地”队列中，自动上传任务到网络队列中，分配给任务进程进行处理。

##### 分布式进程案例

接下来通过程序实现上面的例子（Linux 版），首先编写的是服务进程（taskManager.py），代码如下：

`task_Manager_Linux版.py`

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 12:50
# filename: task_Manager_Linux版.py

import random, time
import queue as Queue
from multiprocessing.managers import BaseManager

# 实现第一步：建立task_queue和result_queue，用来存放任务和结果
task_queue = Queue.Queue()
result_queue = Queue.Queue()


class Queuemanager(BaseManager):
    pass


# 实现第二步：把创建的两个队列注册在网络上，利用register方法，callable参数关联了Queue对象，
# 将Queue对象在网络中暴露
Queuemanager.register('get_task_queue', callable=lambda: task_queue)
Queuemanager.register('get_result_queue', callable=lambda: result_queue)
# 实现第三步：绑定端口8001，设置验证口令‘qiye’。这个相当于对象的初始化
manager = Queuemanager(address=('', 8001), authkey=b'admin#123')
# 实现第四步：启动管理，监听信息通道
manager.start()
# 实现第五步：通过管理实例的方法获得通过网络访问的Queue对象
task = manager.get_task_queue()
result = manager.get_result_queue()
# 实现第六步：添加任务
for url in ["ImageUrl_" + str(i) for i in range(10)]:
    print('put task %s ...' % url)
    task.put(url)
# 获取返回结果
print('try get result...')
for i in range(10):
    print('result is %s' % result.get(timeout=10))
# 关闭管理
manager.shutdown()
```

`taskManager_Windows版.py`

```python
# -*- coding: utf-8 -*-
import queue as Queue
from multiprocessing.managers import BaseManager
from multiprocessing import freeze_support

# 任务个数
task_number = 10
# 定义收发队列
task_queue = Queue.Queue(task_number)
result_queue = Queue.Queue(task_number)


def get_task():
    return task_queue


def get_result():
    return result_queue


# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass


def win_run():
    # windows下绑定调用接口不能使用lambda，所以只能先定义函数再绑定
    QueueManager.register('get_task_queue', callable=get_task)
    QueueManager.register('get_result_queue', callable=get_result)
    # 绑定端口并设置验证口令，windows下需要填写ip地址，linux下不填默认为本地
    manager = QueueManager(address=('127.0.0.1', 8001), authkey=b'admin#123')
    # 启动
    manager.start()
    try:
        # 通过网络获取任务队列和结果队列
        task = manager.get_task_queue()
        result = manager.get_result_queue()
        # 添加任务
        for url in ["ImageUrl_" + str(i) for i in range(10)]:
            print('put task %s ...' % url)
            task.put(url)
        print('try get result...')
        for i in range(10):
            print('result is %s' % result.get(timeout=10))
    except:
        print('Manager error')
    finally:
        # 一定要关闭，否则会爆管道未关闭的错误
        manager.shutdown()


if __name__ == '__main__':
    # windows下多进程可能会有问题，添加这句可以缓解
    freeze_support()
    win_run()
```

`task_worker.py`

```python
# -*- coding: utf-8 -*-
import time
from multiprocessing.managers import BaseManager


# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass


# 实现第一步：使用QueueManager注册获取Queue的方法名称
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')
# 实现第二步：连接到服务器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证口令注意保持与服务进程设置的完全一致:
m = QueueManager(address=(server_addr, 8001), authkey=b'admin#123')
# 从网络连接:
m.connect()
# 实现第三步：获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 实现第四步：从task队列取任务,并把结果写入result队列:
while (not task.empty()):
    image_url = task.get(True, timeout=5)
    print('run task download %s...' % image_url)
    time.sleep(1)
    result.put('%s--->success' % image_url)
# 处理结束:
print('worker exit.')
```

执行 taskManager_Windows 版.py 的输出如下：

```sh
put task ImageUrl_0 ...
put task ImageUrl_1 ...
put task ImageUrl_2 ...
put task ImageUrl_3 ...
put task ImageUrl_4 ...
put task ImageUrl_5 ...
put task ImageUrl_6 ...
put task ImageUrl_7 ...
put task ImageUrl_8 ...
put task ImageUrl_9 ...
try get result...
```

执行 task_worker.py 的输出如下：

```sh
Connect to server 127.0.0.1...
run task download ImageUrl_0...
run task download ImageUrl_1...
run task download ImageUrl_2...
run task download ImageUrl_3...
run task download ImageUrl_4...
run task download ImageUrl_5...
# .......
```

taskManager_Windows 版.py 的输出

```sh
result is ImageUrl_0--->success
result is ImageUrl_1--->success
result is ImageUrl_2--->success
result is ImageUrl_3--->success
result is ImageUrl_4--->success
result is ImageUrl_5--->success
# .....
```

### 1.2 多线程

线程线程（Thread，有时被称为轻量级进程）跟进程有些相似，不同的是所有线程运行在同一个进程中，共享运行环境。

线程有开始、顺序执行和结束 3 部分，有一个自己的指令指针，记录运行到什么地方。

线程的运行可能被抢占（中断）或暂时被挂起（睡眠），从而让其他线程运行，这叫作让步。

一个进程中的各个线程之间共享同一块数据空间，所以线程之间可以比进程之间更方便地共享数据和相互通信。

线程一般是并发执行的。正是由于这种并行和数据共享的机制，使得多个任务的合作变得可能。

实际上，在单 CPU 系统中，真正的并发并不可能，每个线程会被安排成每次只运行一小会儿，然后就把 CPU 让出来，让其他线程运行。

在进程的整个运行过程中，每个线程都只做自己的事，需要时再跟其他线程共享运行结果。多个线程共同访问同一块数据不是完全没有危险的，由于访问数据的顺序不一样，因此有可能导致数据结果不一致的问题，这叫作竞态条件。大多数线程库都带有一系列同步原语，用于控制线程的执行和数据的访问

Python 的标准库提供了两个模块： \_thread 和 threading ， \_thread 是低级模块， threading 是高级模块，对 \_thread 进行了封装。

绝大多数情况下，我们只需要使用 threading 这个高级模块。启动一个线程就是把一个函数传入并创建 Thread 实例，然后调用 start() 开始执行：

#### 调用 Thread 类来创建多线程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import threading
import time


# 线程体函数
def thread_bady():
    # 当前线程对象
    t = threading.current_thread()

    for n in range(5):
        # 当前线程名
        print("第{}次执行线程:{}".format(n, t.name))
        # 线程休眠，如果不休眠，线程对象t1结束后才会执行线程对象t2线程将
        time.sleep(1)
    print("线程:{}执行完成！".format(t.name))


# 主函数
def main():
    # 创建线程对象t1
    t1 = threading.Thread(target=thread_bady, name="hu_thread")
    # 启动线程t1
    t1.start()

    # 创建线程对象t2
    t2 = threading.Thread(target=thread_bady, name="xiaojian_thread")
    # 启动线程t2
    t2.start()


if __name__ == '__main__':
    main()
```

#### 继承 Thread 类创建多线程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import threading
import time

import threading
import time


class MyThread(threading.Thread):
    def __init__(self, name=None):
        super(MyThread, self).__init__(name=name)

    # 线程体函数
    def run(self):
        # 当前线程对象
        t = threading.current_thread()
        for n in range(5):
            # 当前线程名
            print("第{}次执行线程:{}".format(n, t.name))
            # 线程休眠
            time.sleep(1)
        print("线程{}执行完毕！".format(t.name))


def main():
    # 创建线程对象t1
    t1 = MyThread(name="t1-thread")
    # 启动线程t1
    t1.start()

    # 创建线程对象t2
    t2 = MyThread(name="t2-thread")
    # 启动线程t2
    t2.start()


if __name__ == '__main__':
    main()
```

执行结果

```sh
第0次执行线程:t1-thread
第0次执行线程:t2-thread
第1次执行线程:t1-thread
第1次执行线程:t2-thread
第2次执行线程:t1-thread
第2次执行线程:t2-thread
第3次执行线程:t1-thread
第3次执行线程:t2-thread
第4次执行线程:t1-thread
第4次执行线程:t2-thread
线程t1-thread执行完毕！
线程t2-thread执行完毕！
```

- 演示 deamon 属性的作用 后台线程

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2019/12/21 18:27
# filename: 04.deamon属性使用.py
import threading
import time


class myThread(threading.Thread):
    def __init__(self, mynum):
        super(myThread, self).__init__()
        self.mynum = mynum

    def run(self):
        time.sleep(1)
        for i in range(self.mynum, self.mynum + 5):
            print(str(i * i) + ";")


def main():
    """
    main()主函数运行结束时，ma和mb在后台运行，无法输出运行结果
    :return:
    """
    print("start............")
    ma = myThread(1)
    mb = myThread(16)
    ma.daemon = True
    mb.daemon = True
    ma.start()
    mb.start()
    print("end...........")


if __name__ == '__main__':
    main()

"""
start............
end...........
"""
```

- 等待线程结束

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2019/6/23 14:37
# filename: 等待线程结束.py
"""
join()方法，当前线程t1调用join()方法时，会阻塞当前线程，等到t1线程结束，如果t1线程结束
或者等待超时，则当前线程回到活动状态继续执行。
join(timeout=None)
参数timeout 设置超时时间，单位是秒。如果没有设置timeout时间，则可以一直等待

使用join()方法的场景是：一个线程依赖另一个线程的运行结果，所以调用另一个线程的join()方法等待它的运行完成
"""
import threading
import time

# 共享变量0
value = 0


# 线程体函数
def thread_body():
    global value
    # 当前线程对象
    print("ThreadA 开始.....")
    for n in range(2):
        print("ThreadA 执行.......")
        value += 1
        # 线程休眠
        time.sleep(1)
        print("ThreadA 结束.......")


def main():
    print("主线程 开始........")
    t1 = threading.Thread(target=thread_body, name="ThreadA")
    # 启动线程
    t1.start()
    # 主线程被阻塞，等待t1线程结束
    t1.join()
    print("value = {0}".format(value))
    print("主线程  结束.....")


if __name__ == '__main__':
    main()
```

输出信息:

```sh
主线程 开始........
ThreadA 开始.....
ThreadA 执行.......
ThreadA 结束.......
ThreadA 执行.......
ThreadA 结束.......
value = 2
主线程  结束.....
```

#### 什么是互斥锁

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# threading.Lock()
# 使用互斥锁可以防止多个线程同时读取内存的某一个区域,互斥锁保证了每个线程同一时间只有一个在使用内存资源

"""
从系统的角度来看。锁的作用其实是将多线程变回到单线程，这是以牺牲性能，来换取程序的准确性。

在代码设计中，应该最大化地避免使用锁。即使加了锁，也要让被保护的区域尽量地少，在满足准确性的同时实现性能最大化。
在代码中，有“加锁”操作，就一定要有与之对应的“解锁”操作，否则代码失去多线程的优势。

在Python中，使用threading.RLock类来创建锁。threading.RLock类有两个方法--acquire与release

* acquire负责开始对代码进行保护，在acquire之后的代码，都将只允许一个线程进行执行。

* release方法用于停止保护（即释放锁资源）。在release之后的代码又恢复到原来的样子，可以被多线程交叉执行。
"""

from threading import Thread, Lock
import time

'''
# 互斥锁的使用

#创建锁
mutex = threading.Lock()

#锁定
mutex.acquire([blocking])

#释放锁
mutex.release()

'''
# 计数器，总票数
num = 20


def task(arg):
    global num       # 使用全局变量
    mutex.acquire()  # 锁定线程，只有1个线程可以抢用
    time.sleep(0.5)
    num -= 1
    print("{}号用户【线程】，购买成功，剩余{}张电影票".format(arg, num))
    mutex.release()  # 释放，其他线程可以进行操作


if __name__ == '__main__':
    mutex = Lock()  # 创建锁
    t_l = []
    for i in range(10):
        t = Thread(target=task, args=(i,))
        t_l.append(t)
        t.start()

    for t in t_l:
        t.join()

print("main thread end..!")

# 0号用户【线程】，购买成功，剩余19张电影票
# 1号用户【线程】，购买成功，剩余18张电影票
# 2号用户【线程】，购买成功，剩余17张电影票
# 3号用户【线程】，购买成功，剩余16张电影票
# 4号用户【线程】，购买成功，剩余15张电影票
# 5号用户【线程】，购买成功，剩余14张电影票
# 6号用户【线程】，购买成功，剩余13张电影票
# 7号用户【线程】，购买成功，剩余12张电影票
# 8号用户【线程】，购买成功，剩余11张电影票
# 9号用户【线程】，购买成功，剩余10张电影票
# main thread end..!
```

#### 使用线程池提升运行效率

在 python 中使用线程池有两种方式，一种是基于第三方库 threadpool，另一种是基于 python3 新引入的库 `concurrent.futures.ThreadPoolExecutor`，这里我们介绍一下后一种。

concurrent.futures.ThreadPoolExecutor，在提交任务的时候有两种方式，一种是`submit()`函数，另一种是`map()`函数，两者的主要区别在于：

1.map 可以保证输出的顺序, submit 输出的顺序是乱的。

2.如果你要提交的任务的函数是一样的，就可以简化成 map。但是假如提交的任务函数是不一样的，或者执行的过程之可能出现异常（使用 map 执行过程中发现问题会直接抛出错误）就要用到 submit（）。

3.submit 和 map 的参数是不同的，submit 每次都需要提交一个目标函数和对应的参数，map 只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2019/12/21 19:29
# filename: 08.使用线程池提升运行效率.py
"""

在需要频繁创建线程的系统中， 一般都会使用线程池技术。原因有两点：
    1.每一个线程的创建都是需要占用系统资源的， 是一件相对耗时的事情。同样在销毁线程时还需要回收线程资源。
    线程池技术， 可以省去创建与回收过程中所浪费的系统开销。

    2.在某些系统中需要为每个子任务来创建对应的线程(例如爬虫系统中的子链接)。
    这种情况会导致线程数量失控性暴涨， 直到程序崩溃。线程池技术可以很好地固定线程的数量保持程序稳定。


实现线程池

Python中，使用conncurrent.futures 模块下的ThreadPoolExecutor 类来实现线程池。在实例化时， 会将需要的线程个数传入。
系统就会为该线程池初始化相应个数的线程。线程池的使用有两种方式。

    * 抢占式： 线程池中的线程执行顺序不固定。该方式使用ThreadPooIExecutor 的submit方法实现。

    * 非抢占式： 线程将按照调用的顺序执行。此方式使用ThreadPoolExecutor 的map方法来实现。

从使用角度来看： 抢占式更灵活； 非抢占式更严格。


· 抢占式， 允许池中线程的处理函数不一样。如执行过程中某个线程出现异常， 也不影响其他线程。
· 非抢占式， 要求线程池中的线程必须执行同样的处理函数。而且一旦某个线程出现异常,其他线程也会停止。
"""
from concurrent.futures import ThreadPoolExecutor
import time


def printperson(p):
    '''
    定义线程池处理函数
    :param p:
    :return:
    '''
    print(p)
    time.sleep(2)


person = ["hujianli1", "hujianli2", "hujianli3"]

start_time = time.time()
for p in person:
    printperson(p)

end_time = time.time()
printperson("all spend time :{}".format(end_time - start_time))

"""
hujianli1
hujianli2
hujianli3
all spend time :6.00168251991272
"""
```

##### 实现抢占线程池

```python
start2 = time.time()
with ThreadPoolExecutor(3) as executor:
    for p in person:
        executor.submit(printperson, p)
end2 = time.time()
printperson("all spend time :{}".format(end2 - start2))

"""
hujianli1
hujianli2
hujianli3
all spend time :2.0018222332000732
"""
```

##### 实现非抢占线程池

```python
start3 = time.time()
with ThreadPoolExecutor(3) as executorl:
    executorl.map(printperson, person)
end3 = time.time()
printperson("all spend time :{}".format(end3 - start3))
"""
hujianli1
hujianli2
hujianli3
all spend time :2.001864433288574
"""
```

代码示例

```python
from concurrent.futures import ThreadPoolExecutor
from threading import Thread, currentThread
from time import time


def task(i):
    print("{} 在执行任务{}".format(currentThread().name, i))
    time.sleep(1)


if __name__ == '__main__':
    pool = ThreadPoolExecutor(4)  # 进程池里有4个进程
    for i in range(20):  # 20个任务
        pool.submit(task, i)  # 进程池里当前执行的任务i，池子里的4个进程一次一次执行任务
```

使用 map()方法启动线程，并收集线程任务的返回值。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/1/14 22:19
# filename: 09-1.线程池02.py
from concurrent.futures import ThreadPoolExecutor
import threading
import time


def action(max):
    my_sum = 0
    for i in range(max):
        print(threading.current_thread().name + " " + str(i))
        my_sum += 1
    return my_sum


# 创建一个包含4个线程的线程池
with ThreadPoolExecutor(max_workers=4) as pool:
    # 使用线程执行map计算
    # 后面的元祖有3个元素，因此程序启动了3个线程来执行action函数
    results = pool.map(action, (50, 100, 150))
    print("---------------------------------------------")
    for i in results:
        print(i)
```

##### 线程池实现并发执行命令

```python
def mulit_run(func, max_workers, args):
    """
    多线程执行命令
    :param func:  执行函数
    :param max_workers: 最多线程数
    :param args: 可迭代对象
    :return:
    """
    from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETED
    executor = ThreadPoolExecutor(max_workers=max_workers)
    all_task = [executor.submit(func, i) for i in args]
    wait(all_task, return_when=ALL_COMPLETED)
```

调用方法

```sh
mulit_run(run_cmd, len(all_images), ['docker pull %s' % i for i in set(all_images)])
```

示例：

```python
import concurrent.futures
import subprocess

def pull_docker_image(image_name):
    command = f"docker pull {image_name}"
    try:
        # 执行命令
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        # 输出命令执行结果
        print(f"Successfully pulled image: {image_name}")
        print("Output:", result.stdout)
    except subprocess.CalledProcessError as e:
        # 输出错误信息
        print(f"Failed to pull image: {image_name}")
        print("Error:", e.stderr)

def multi_pull_docker_images(image_names, num_threads):
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        # 使用map方法并行执行函数
        executor.map(pull_docker_image, image_names)

# 假设这是你的所有镜像列表
all_images = ['ubuntu', 'nginx', 'redis', 'mysql', 'mongo']

# 指定线程数目
num_threads = 3

# 批量拉取 Docker 镜像
multi_pull_docker_images(all_images, num_threads)
```

##### 参考文献

https://www.cnblogs.com/yeyuzhuanjia/p/18286041
