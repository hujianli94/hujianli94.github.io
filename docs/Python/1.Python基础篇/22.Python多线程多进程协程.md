# 22.Python 多线程多进程协程

## 1. 多线程与多进程

> 进程和线程的概念。

进程（Process）实际上表示的就是计算机正在进行的一个任务，比如，打开一个浏览器便是启动一个浏览器进程，打开一个记事本便是启动一个记事本进程。

但是，一个进程未必只能进行一件事，就像一个 Word 进程，在打字的同时还会有拼写检查，这些在进程内部同时进行的多个“子任务”，就称为线程（Thread）。

进程和线程的主要差别在于它们是不同的操作系统资源管理方式。

进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。

线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。

在以往的单核 CPU 上，系统执行多进程的方式是通过不断的在多个进程中切换——例如任务 1 执行 0.01 秒，切到任务 2 执行 0.01 秒再切到任务 3……以此类推，而在多核 CPU 出现后，真正的并行执行多任务才真正的得以实现，但绕是如此，一台计算机同时进行的进程是非常之多的，远远大于 CPU 的核心数量，因此，操作系统依然会将这些任务轮流调度到每个核心上运行。

如果我们要同时进行多个任务，我们有以下三种方案：

1. 写多个程序，然后同时运行
2. 在一个程序中运行多个线程
3. 多进程+多线程

## 2. 多进程

要让 Python 程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。

Unix/Linux 操作系统提供了一个 fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是 fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

子进程永远返回 0，而父进程返回子进程的 ID。这样做的理由是，一个父进程可以 fork 出很多子进程，所以，父进程要记下每个子进程的 ID，而子进程只需要调用 getppid()就可以拿到父进程的 ID。

Python 的 os 模块封装了常见的系统调用，其中就包括 fork，可以在 Python 程序中轻松创建子进程：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print "我是子进程%s, 我的父进程是%s 我开始工作了..." % (os.getpid(), os.getppid())
    time.sleep(3)
else:
    print('我的进程id是 (%s) 我创建了一个子进程，子进程id是 (%s).' % (os.getpid(), pid))
    time.sleep(4)
```

运行结果如下：

```sh
$ python 01.os-fork-sample.py
Process (1695) start...
我的进程id是 (1695) 我创建了一个子进程，子进程id是 (1696).
我是子进程1696, 我的父进程是1695 我开始工作了...
```

由于 Windows 没有 fork 调用，上面的代码在 Windows 上无法运行。由于 Mac 系统是基于 BSD（Unix 的一种）内核，所以，在 Mac 下运行是没有问题的，推荐大家用 Mac！

有了 fork 调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的 Apache 服务器就是由父进程监听端口，每当有新的 http 请求时，就 fork 出子进程来处理新的 http 请求。

### multiprocessing

如果你打算编写多进程的服务程序，Unix/Linux 无疑是正确的选择。由于 Windows 没有 fork 调用，难道在 Windows 上无法用 Python 编写多进程的程序？

由于 Python 是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing 模块就是跨平台版本的多进程模块。

#### Process

multiprocessing 模块提供了一个 Process 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from multiprocessing import Process
import os


# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))


if __name__ == '__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果如下：

```sh
$ /usr/bin/python 02.Process-sample.py
Parent process 1712.
Child process will start.
Run child process test (1723)...
Child process end.
```

Process 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个 Process 实例，用 start()方法启动，这样创建进程比 fork()还要简单。
join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

#### Process 子类创建进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time
from multiprocessing import Process


class Download(Process):
    def __init__(self, interval):
        Process.__init__(self)
        self.interval = interval

    # 重写Process类中的run()方法
    def run(self):
        # 开启这个进程所需执行的代码
        t_start = time.time()
        # time.sleep(3)     # 模拟阻塞的一个实现方式
        print("开启进程：%s进行下载操作" % os.getpid())
        print("子进程（%s）开始执行，父进程为（%s）" % (os.getpid(), os.getppid()))
        time.sleep(self.interval)
        t_stop = time.time()
        print("子进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))


if __name__ == '__main__':
    t_start = time.time()
    print("当前进程（%s）" % os.getpid())
    p = Download(2)
    p.start()
    # p.join(10)        # join 父进程等待子进程执行完毕后立刻执行
    time.sleep(10)      # 模拟阻塞，保证子进程完毕后父进程在执行
    t_stop = time.time()
    print("主进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))
```

#### 使用进程池 Pool 创建进程

如果要启动大量的子进程，可以用进程池的方式批量创建子进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import multiprocessing as mp
import os
import time


def task(name):
    print("子进程 ({})执行的任务是 ({})".format(os.getpid(), name))
    time.sleep(1)


if __name__ == '__main__':
    print("父进程 ({})开始执行".format(os.getpid()))
    p = mp.Pool(4)
    for i in range(10):
        p.apply_async(task, args=(i,))

    p.close()
    p.join()
    print("所有子进程结束.....")
```

请注意输出的结果，

```sh
$ /usr/bin/python 04.Process-pool-sample.p
父进程 (2033)开始执行
子进程 (2047)执行的任务是 (0)
子进程 (2044)执行的任务是 (1)
子进程 (2045)执行的任务是 (2)
子进程 (2046)执行的任务是 (3)
子进程 (2047)执行的任务是 (4)
子进程 (2044)执行的任务是 (5)
子进程 (2045)执行的任务是 (6)
子进程 (2046)执行的任务是 (7)
子进程 (2047)执行的任务是 (8)
子进程 (2044)执行的任务是 (9)
所有子进程结束.....
```

说明：

1. 进程池 Pool 被创建出来后， `p.apply_async(task)` 语句不停地循环执行，相当于向进程池中提交了 10 个请求，它们会被放到一个队列中。
2. p = mp.Pool(4) 执行完毕后创建了 4 条进程，但尚未给它们分配各自的任务；也就意味着，无论有多少任务，实际的进程数只有 4 条，每次最多 4 条进程并行。
3. 当 Pool 中有进程任务执行完毕后，这条进程资源会被释放，Pool 会按先进先出的原则取出一个新的请求给空闲的进程继续执行。
4. 当 Pool 所有的进程任务完成后，会产生 4 个僵尸进程，如果主进程/主线程不结束，系统不会自动回收资源，需要调用 join 函数负责回收。
5. 在创建 Pool 进程池时，若不指定进程的最大数量，默认创建的进程数为系统的内核数量
6. 如果采用 `p.apply(task)`阻塞方式添加任务，其每次只能向进程池中添加一条任务，然后 for 循环会被阻塞等待，直到添加的任务被执行完毕，进程池中的 4 个进程交替执行新来的任务，此时相当于单进程。

进程池扫描主机端口实例

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# @auther:   18793
# @Date：    2020/6/22 11:21
# @filename: Process_Pool.py
# @Email:    1879324764@qq.com
# @Software: PyCharm

"""
进程池扫描主机端口实例
代码利用单进程扫描主机端口，如果要扫描的端口范围比较大，则需要耗费比较长的时间。
利用多个进程同时扫描不同的端口范围，可以缩短程序运行时间。
进程池技术可以一次创建多个子进程，适合于子进程数量事先预知的情况。
代码利用进程池一次创建16个进程，然后利用这些进程扫描主机所有端口（0～65535），
每个进程扫描4096个端口。
"""
from multiprocessing import Pool
import os
import socket


def scan_port(ports):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    for port in range(ports, ports + 4096):
        result = s.connect_ex((ip, port))
        if result == 0:
            print("I am process %d,port %d is openned!" % (os.getpid(), port))
    s.close()


ip = "192.168.0.109"
p = Pool(16)

for k in range(16):
    p.apply_async(scan_port, args=(k * 4096,))
p.close()
p.join()
print("All subprocesses had finished!")
```

#### concurrent.futures 进程池

concurrent.futures - 启动并行任务

Python 版本 3.2 中的新功能。

Python 2.6 and 2.7 中使用：

- https://github.com/agronholm/pythonfutures.git


ProcessPoolExecutor 示例

官方示例

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```

以下是使用 `concurrent.futures.ProcessPoolExecutor` 来并行扫描主机端口的简单示例代码：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket
import multiprocessing


# 定义一个函数，用于扫描指定主机和端口是否开放
def scan_port(host, port):
    try:
        # 创建套接字对象
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            # 设置超时时间
            s.settimeout(1)
            # 尝试连接指定主机和端口
            result = s.connect_ex((host, port))
            # 如果连接成功，则端口开放
            if result == 0:
                return port, True
            else:
                return port, False
    except Exception as e:
        return port, False


if __name__ == '__main__':
    # 设置要扫描的主机和端口范围
    host = 'localhost'
    ports = range(1, 1025)  # 扫描1到1024端口

    # 获取系统的CPU核心数量
    max_workers = multiprocessing.cpu_count()

    # 创建进程池执行器，并设置最大工作进程数量
    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:
        # 提交任务给进程池执行器，并获取结果
        futures = {executor.submit(scan_port, host, port): port for port in ports}

        # 遍历结果，打印出开放的端口
        for future in concurrent.futures.as_completed(futures):
            port, status = future.result()
            if status:
                print(f"Port {port} is open")
```

executor.map() 和 executor.submit() 是 concurrent.futures.ProcessPoolExecutor 提供的两种不同的任务提交方式，它们之间的核心区别在于：

1.返回结果的方式：

- executor.map() 会返回一个迭代器，通过迭代器可以逐个获取每个任务的结果。
- executor.submit() 返回一个 concurrent.futures.Future 对象，通过它可以获取单个任务的结果。

  2.任务参数传递方式：

- executor.map() 接收一个可迭代对象作为参数，该可迭代对象的每个元素都是传递给目标函数的参数，然后按顺序执行函数。
- executor.submit() 接收一个函数和函数参数作为参数，可以灵活地传递任意数量的参数给目标函数。

  3.适用场景：

- executor.map() 适用于需要并行执行同一个函数，但参数不同的情况。例如，扫描多个端口或处理多个文件。
- executor.submit() 更加灵活，适用于需要每个任务具有不同函数、参数或处理逻辑的情况。

  4.并发性：

- executor.map() 在执行任务时，会按照顺序提交任务，并行执行，但无法控制每个任务的执行顺序。
- executor.submit() 允许更灵活的控制任务的提交和执行顺序，因为它返回的是 Future 对象，可以通过 Future 对象的方法来控制任务的状态和执行顺序。

总的来说，如果需要并行执行一组相似的任务，并且这些任务的参数可以通过迭代器提供，那么使用 `executor.map()` 更加简洁方便；

如果需要对每个任务进行更精细的控制，例如指定特定的函数、参数或处理逻辑，那么使用 `executor.submit()` 更为合适。


更多参考文献：https://www.cnblogs.com/piperliu/p/18628980


#### 进程池效率对比

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket
from multiprocessing import Pool


def scan_port(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    result = s.connect_ex((ip, port))
    s.close()
    return port, result == 0  # 返回端口和状态


# 使用多进程 concurrent.futures.ProcessPoolExecutor map 方法执行
def multi_process_map_method():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = executor.map(scan_port, [ip] * len(ports), ports)  # 传递 ip 和 ports
        for port, status in results:
            if status:
                print(f"Port {port} is open")


# 使用多进程 concurrent.futures.ProcessPoolExecutor submit 方法执行
def multi_process_submit_method():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        futures = {executor.submit(scan_port, ip, port): port for port in ports}  # 传递 ip 和 port
        for future in concurrent.futures.as_completed(futures):
            port, status = future.result()
            if status:
                print(f"Port {port} is open")


# 使用 进程池 Pool.apply_async 方法执行
def pool_apply_async():
    with Pool() as pool:
        results = [pool.apply_async(scan_port, (ip, port)) for port in ports]  # 传递 ip 和 port
        for result in results:
            port, status = result.get()
            if status:
                print(f"Port {port} is open")


if __name__ == "__main__":
    # 全局变量
    ip = "127.0.0.1"
    ports = range(1, 1024)  # 改为 range，从 1 到 1024
    import time

    start_time = time.time()
    multi_process_map_method()
    map_execution_time = time.time() - start_time
    print(f"Execution time using multi_process map method: {map_execution_time:.2f} seconds")

    start_time = time.time()
    multi_process_submit_method()
    submit_execution_time = time.time() - start_time
    print(f"Execution time using multi_process submit method: {submit_execution_time:.2f} seconds")

    start_time = time.time()
    pool_apply_async()
    pool_execution_time = time.time() - start_time
    print(f"Execution time using Pool.apply_async method: {pool_execution_time:.2f} seconds")
```

执行结果如下：

```sh
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using map method: 51.90 seconds
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using submit method: 51.92 seconds
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using Pool.apply_async method: 51.87 seconds
```

multiprocessing 模块使用的是进程池而不是线程池，创建和销毁进程的开销相对较大，导致性能很差。

上面代码只是展示使用方法。对于端口扫描 I/O 密集型操作使用多线程才算正确的。

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import concurrent.futures
import socket


def scan_port(ip, port):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    result = s.connect_ex((ip, port))
    s.close()
    return port, result == 0  # 返回端口和状态


# 使用多进程 concurrent.futures.ThreadPoolExecutor map 方法执行
def multi_thread_map_method():
    with concurrent.futures.ThreadPoolExecutor(len(ports)) as executor:
        results = executor.map(scan_port, [ip] * len(ports), ports)  # 传递 ip 和 ports
        for port, status in results:
            if status:
                print(f"Port {port} is open")


if __name__ == "__main__":
    # 全局变量
    ip = "127.0.0.1"
    ports = range(1, 1024)  # 改为 range，从 1 到 1024
    import time

    start_time = time.time()
    multi_thread_map_method()
    map_execution_time = time.time() - start_time
    print(f"Execution time using multi_thread map method: {map_execution_time:.2f} seconds")
```

执行结果

```sh
Port 135 is open
Port 445 is open
Port 902 is open
Port 912 is open
Execution time using multi_thread map method: 1.18 seconds
```

更多参考：

- https://python-parallel-programmning-cookbook.readthedocs.io/zh-cn/latest/index.html


### 进程间通信

- Python 提供了多种进程间通信的方式，例如 Queue、Pipe、Value+Array 等

Queue 和 Pipe 的区别在于

- Pipe 常用来在两个进程间通信
- Queue 用来在多个进程间实现通信。

#### Queue 多进程队列的使用

Queue 模块可以用来进行线程间的通信，让各个线程之间共享数据。

Python 的 Queue 模块提供了同步、线程安全的队列类， 包括 `FIFO（先入先出）队列` `Queue、LIFO（后入先出）队列` `LifoQueue` 和`优先级队列 PriorityQueue`。

这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列实现线程间的同步。

| 方法名             | 描述                                        |
| ------------------ | ------------------------------------------- |
| Queue()            | 创建一个空的队列                            |
| q.put(item)        | 将 item 放入队列                            |
| q.get()            | 从队列中移除并返回一个 item                 |
| q.empty()          | 如果队列为空，返回 True；否则返回 False     |
| q.full()           | 如果队列已满，返回 True；否则返回 False     |
| q.qsize()          | 返回队列中的 item 数量                      |
| q.put_nowait(item) | 将 item 放入队列，不等待队列可用            |
| q.get_nowait()     | 从队列中移除并返回一个 item，不等待队列可用 |
| q.task_done()      | 表示之前入队的任务已经完成                  |
| q.join()           | 阻塞直到队列中所有的 item 都被处理完毕      |

代码示例 1

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
from multiprocessing import Process, Queue
import time

'''
2个子进程在队列中进行写入和读取数据，实现进程之间的通信
'''


def write(q):
    if not q.full():
        for i in range(5):
            message = "消息" + str(i)
            q.put(message)
            print("写入:{}".format(message))


def read(q):
    time.sleep(1)
    while not q.empty():
        print("读取:{}".format(q.get(True, 2)))


if __name__ == '__main__':
    print("主进程开始".center(100, "*"))
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    pw.start()
    pr.start()
    pw.join()
    pr.join()
    print("主进程结束".center(100, "*"))

```

代码示例 2

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 10:55
# filename: 进程间通信01.py
from multiprocessing import Process, Queue
import os, time, random


# 写数据进程执行的代码
def proc_write(q, urls):
    print('Process(%s) is writing...' % os.getpid())
    for url in urls:
        q.put(url)
        print('Put %s to queue...' % url)
        time.sleep(random.random())


# 读进程执行的代码
def proc_read(q):
    print('Process(%s) is reading...' % os.getpid())
    while True:
        url = q.get(True)
        print('Get %s from queue.' % url)


if __name__ == '__main__':
    # 父进程创建Queue，并传给各个子进程
    q = Queue()
    proc_writer1 = Process(target=proc_write, args=(q, ['url1', 'url2', 'url3']))
    proc_writer2 = Process(target=proc_write, args=(q, ['url4', 'url5', 'url6']))
    proc_reader = Process(target=proc_read, args=(q,))
    # 启动子进程proc_writeer 写入
    proc_writer1.start()
    proc_writer2.start()
    # 启动子进程proc_reader,读取
    proc_reader.start()
    # 等待子进程proc_writer结束
    proc_writer1.join()
    proc_writer2.join()
    # proc_reader进程里是死循环，无法等待其结束，要强行终止
    proc_reader.terminate()

"""
Process(10608) is writing...
Put url1 to queue...
Process(7808) is writing...
Put url4 to queue...
Process(13840) is reading...
Get url1 from queue.
Get url4 from queue.
Put url5 to queue...
Get url5 from queue.
Put url2 to queue...
Get url2 from queue.
Put url6 to queue...
Get url6 from queue.
Put url3 to queue...
Get url3 from queue.
"""
```

#### Pipe 常用来在两个进程间进行通信

两个进程分别位于管道的两端。 Pipe 方法返回（conn1，conn2）代表一个管道的两个端。

Pipe 方法有 duplex 参数，如果 duplex 参数为 True（默认值），那么这个管道是全双工模式，也就是说 conn1 和 conn2 均可收发。

若 duplex 为 False，conn1 只负责接收消息，conn2 只负责发送消息。 send 和 recv 方法分别是发送和接收消息的方法。 例如，在全双工模式下，可以调用 conn1.send 发送消息，conn1.recv 接收消息。

如果没有消息可接收，recv 方法会一直阻塞。如果管道已经被关闭，那么 recv 方法会抛出 EOFError。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 11:08
# filename: sample01.py
import multiprocessing
import random
import time, os


def proc_send(pipe, urls):
    for url in urls:
        print("Process(%s) send :%s" % (os.getpid(), url))
        pipe.send(url)
        time.sleep(random.random())


def proc_recv(pipe):
    while True:
        print("Process(%s) rev:%s" % (os.getpid(), pipe.recv()))
        time.sleep(random.random())


if __name__ == '__main__':
    pipe = multiprocessing.Pipe()
    p1 = multiprocessing.Process(target=proc_send, args=(pipe[0], ["url_" + str(i) for i in range(10)]))
    p2 = multiprocessing.Process(target=proc_recv, args=(pipe[1],))
    p1.start()
    p2.start()
    p1.join()
    p1.join()
    p2.terminate()

"""
Process(17008) send :url_0
Process(13264) rev:url_0
Process(17008) send :url_1
Process(17008) send :url_2
Process(13264) rev:url_1
Process(17008) send :url_3
Process(17008) send :url_4
Process(13264) rev:url_2
Process(17008) send :url_5
Process(13264) rev:url_3
Process(17008) send :url_6
Process(17008) send :url_7
Process(13264) rev:url_4
Process(13264) rev:url_5
Process(17008) send :url_8
Process(17008) send :url_9
Process(13264) rev:url_6
"""
```

### 分布式进程

分布式进程指的是将 Process 进程分布到多台机器上，充分利用多台机器的性能完成复杂的任务。 我们可以将这一点应用到分布式爬虫的开发中。

分布式进程在 Python 中依然要用到 multiprocessing 模块。 multiprocessing 模块不但支持多进程，其中 managers 子模块还支持把多进程分布到多台机器上。 可以写一个服务进程作为调度者，将任务分布到其他多个进程中，依靠网络通信进行管理。

举个例子：

在做爬虫程序时，常常会遇到这样的场景，我们想抓取某个网站的所有图片，如果使用多进程的话，
一般是一个进程负责抓取图片的链接地址，将链接地址存放到 Queue 中，
另外的进程负责从 Queue 中读取链接地址进行下载和存储到本地。
现在把这个过程做成分布式，一台机器上的进程负责抓取链接，
其他机器上的进程负责下载存储。
那么遇到的主要问题是将 Queue 暴露到网络中，
让其他机器进程都可以访问，分布式进程就是将这一个过程进行了封装，
我们可以将这个过程称为本地队列的网络化。

分布式进程 要实现上面例子的功能，创建分布式进程需要分为六个步骤：

1）建立队列 Queue，用来进行进程间的通信。服务进程创建任务队列 task_queue，用来作为传递任务给任务进程的通道；服务进程创建结果队列 result_queue，作为任务进程完成任务后回复服务进程的通道。在分布式多进程环境下，必须通过由 Queuemanager 获得的 Queue 接口来添加任务。

2）把第一步中建立的队列在网络上注册，暴露给其他进程（主机），注册后获得网络队列，相当于本地队列的映像。

3）建立一个对象（Queuemanager（BaseManager））实例 manager，绑定端口和验证口令。

4）启动第三步中建立的实例，即启动管理 manager，监管信息通道。

5）通过管理实例的方法获得通过网络访问的 Queue 对象，即再把网络队列实体化成可以使用的本地队列。

6）创建任务到“本地”队列中，自动上传任务到网络队列中，分配给任务进程进行处理。

#### 分布式进程案例

接下来通过程序实现上面的例子（Linux 版），首先编写的是服务进程（taskManager.py），代码如下：

`task_Manager_Linux版.py`

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 12:50
# filename: task_Manager_Linux版.py

import random, time
import queue as Queue
from multiprocessing.managers import BaseManager

# 实现第一步：建立task_queue和result_queue，用来存放任务和结果
task_queue = Queue.Queue()
result_queue = Queue.Queue()


class Queuemanager(BaseManager):
    pass


# 实现第二步：把创建的两个队列注册在网络上，利用register方法，callable参数关联了Queue对象，
# 将Queue对象在网络中暴露
Queuemanager.register('get_task_queue', callable=lambda: task_queue)
Queuemanager.register('get_result_queue', callable=lambda: result_queue)
# 实现第三步：绑定端口8001，设置验证口令‘qiye’。这个相当于对象的初始化
manager = Queuemanager(address=('', 8001), authkey=b'admin#123')
# 实现第四步：启动管理，监听信息通道
manager.start()
# 实现第五步：通过管理实例的方法获得通过网络访问的Queue对象
task = manager.get_task_queue()
result = manager.get_result_queue()
# 实现第六步：添加任务
for url in ["ImageUrl_" + str(i) for i in range(10)]:
    print('put task %s ...' % url)
    task.put(url)
# 获取返回结果
print('try get result...')
for i in range(10):
    print('result is %s' % result.get(timeout=10))
# 关闭管理
manager.shutdown()
```

`taskManager_Windows版.py`

```python
# -*- coding: utf-8 -*-
import queue as Queue
from multiprocessing.managers import BaseManager
from multiprocessing import freeze_support

# 任务个数
task_number = 10
# 定义收发队列
task_queue = Queue.Queue(task_number)
result_queue = Queue.Queue(task_number)


def get_task():
    return task_queue


def get_result():
    return result_queue


# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass


def win_run():
    # windows下绑定调用接口不能使用lambda，所以只能先定义函数再绑定
    QueueManager.register('get_task_queue', callable=get_task)
    QueueManager.register('get_result_queue', callable=get_result)
    # 绑定端口并设置验证口令，windows下需要填写ip地址，linux下不填默认为本地
    manager = QueueManager(address=('127.0.0.1', 8001), authkey=b'admin#123')
    # 启动
    manager.start()
    try:
        # 通过网络获取任务队列和结果队列
        task = manager.get_task_queue()
        result = manager.get_result_queue()
        # 添加任务
        for url in ["ImageUrl_" + str(i) for i in range(10)]:
            print('put task %s ...' % url)
            task.put(url)
        print('try get result...')
        for i in range(10):
            print('result is %s' % result.get(timeout=10))
    except:
        print('Manager error')
    finally:
        # 一定要关闭，否则会爆管道未关闭的错误
        manager.shutdown()


if __name__ == '__main__':
    # windows下多进程可能会有问题，添加这句可以缓解
    freeze_support()
    win_run()
```

`task_worker.py`

```python
# -*- coding: utf-8 -*-
import time
from multiprocessing.managers import BaseManager


# 创建类似的QueueManager:
class QueueManager(BaseManager):
    pass


# 实现第一步：使用QueueManager注册获取Queue的方法名称
QueueManager.register('get_task_queue')
QueueManager.register('get_result_queue')
# 实现第二步：连接到服务器:
server_addr = '127.0.0.1'
print('Connect to server %s...' % server_addr)
# 端口和验证口令注意保持与服务进程设置的完全一致:
m = QueueManager(address=(server_addr, 8001), authkey=b'admin#123')
# 从网络连接:
m.connect()
# 实现第三步：获取Queue的对象:
task = m.get_task_queue()
result = m.get_result_queue()
# 实现第四步：从task队列取任务,并把结果写入result队列:
while (not task.empty()):
    image_url = task.get(True, timeout=5)
    print('run task download %s...' % image_url)
    time.sleep(1)
    result.put('%s--->success' % image_url)
# 处理结束:
print('worker exit.')
```

执行 taskManager_Windows 版.py 的输出如下：

```sh
put task ImageUrl_0 ...
put task ImageUrl_1 ...
put task ImageUrl_2 ...
put task ImageUrl_3 ...
put task ImageUrl_4 ...
put task ImageUrl_5 ...
put task ImageUrl_6 ...
put task ImageUrl_7 ...
put task ImageUrl_8 ...
put task ImageUrl_9 ...
try get result...
```

执行 task_worker.py 的输出如下：

```sh
Connect to server 127.0.0.1...
run task download ImageUrl_0...
run task download ImageUrl_1...
run task download ImageUrl_2...
run task download ImageUrl_3...
run task download ImageUrl_4...
run task download ImageUrl_5...
# .......
```

taskManager_Windows 版.py 的输出

```sh
result is ImageUrl_0--->success
result is ImageUrl_1--->success
result is ImageUrl_2--->success
result is ImageUrl_3--->success
result is ImageUrl_4--->success
result is ImageUrl_5--->success
# .....
```

## 3. 多线程

线程线程（Thread，有时被称为轻量级进程）跟进程有些相似，不同的是所有线程运行在同一个进程中，共享运行环境。

线程有开始、顺序执行和结束 3 部分，有一个自己的指令指针，记录运行到什么地方。

线程的运行可能被抢占（中断）或暂时被挂起（睡眠），从而让其他线程运行，这叫作让步。

一个进程中的各个线程之间共享同一块数据空间，所以线程之间可以比进程之间更方便地共享数据和相互通信。

线程一般是并发执行的。正是由于这种并行和数据共享的机制，使得多个任务的合作变得可能。

实际上，在单 CPU 系统中，真正的并发并不可能，每个线程会被安排成每次只运行一小会儿，然后就把 CPU 让出来，让其他线程运行。

在进程的整个运行过程中，每个线程都只做自己的事，需要时再跟其他线程共享运行结果。多个线程共同访问同一块数据不是完全没有危险的，由于访问数据的顺序不一样，因此有可能导致数据结果不一致的问题，这叫作竞态条件。大多数线程库都带有一系列同步原语，用于控制线程的执行和数据的访问

Python 的标准库提供了两个模块： \_thread 和 threading ， \_thread 是低级模块， threading 是高级模块，对 \_thread 进行了封装。

绝大多数情况下，我们只需要使用 threading 这个高级模块。启动一个线程就是把一个函数传入并创建 Thread 实例，然后调用 start() 开始执行：

### 调用 Thread 类来创建多线程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import threading
import time


# 线程体函数
def thread_bady():
    # 当前线程对象
    t = threading.current_thread()

    for n in range(5):
        # 当前线程名
        print("第{}次执行线程:{}".format(n, t.name))
        # 线程休眠，如果不休眠，线程对象t1结束后才会执行线程对象t2线程将
        time.sleep(1)
    print("线程:{}执行完成！".format(t.name))


# 主函数
def main():
    # 创建线程对象t1
    t1 = threading.Thread(target=thread_bady, name="hu_thread")
    # 启动线程t1
    t1.start()

    # 创建线程对象t2
    t2 = threading.Thread(target=thread_bady, name="xiaojian_thread")
    # 启动线程t2
    t2.start()


if __name__ == '__main__':
    main()
```

### 继承 Thread 类创建多线程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import threading
import time

import threading
import time


class MyThread(threading.Thread):
    def __init__(self, name=None):
        super(MyThread, self).__init__(name=name)

    # 线程体函数
    def run(self):
        # 当前线程对象
        t = threading.current_thread()
        for n in range(5):
            # 当前线程名
            print("第{}次执行线程:{}".format(n, t.name))
            # 线程休眠
            time.sleep(1)
        print("线程{}执行完毕！".format(t.name))


def main():
    # 创建线程对象t1
    t1 = MyThread(name="t1-thread")
    # 启动线程t1
    t1.start()

    # 创建线程对象t2
    t2 = MyThread(name="t2-thread")
    # 启动线程t2
    t2.start()


if __name__ == '__main__':
    main()
```

执行结果

```sh
第0次执行线程:t1-thread
第0次执行线程:t2-thread
第1次执行线程:t1-thread
第1次执行线程:t2-thread
第2次执行线程:t1-thread
第2次执行线程:t2-thread
第3次执行线程:t1-thread
第3次执行线程:t2-thread
第4次执行线程:t1-thread
第4次执行线程:t2-thread
线程t1-thread执行完毕！
线程t2-thread执行完毕！
```

- 演示 demo 属性的作用 后台线程

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2019/12/21 18:27
# filename: 04.demo属性使用.py
import threading
import time


class myThread(threading.Thread):
    def __init__(self, mynum):
        super(myThread, self).__init__()
        self.mynum = mynum

    def run(self):
        time.sleep(1)
        for i in range(self.mynum, self.mynum + 5):
            print(str(i * i) + ";")


def main():
    """
    main()主函数运行结束时，ma和mb在后台运行，无法输出运行结果
    :return:
    """
    print("start............")
    ma = myThread(1)
    mb = myThread(16)
    ma.daemon = True
    mb.daemon = True
    ma.start()
    mb.start()
    print("end...........")


if __name__ == '__main__':
    main()

"""
start............
end...........
"""
```

- 等待线程结束

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2019/6/23 14:37
# filename: 等待线程结束.py
"""
join()方法，当前线程t1调用join()方法时，会阻塞当前线程，等到t1线程结束，如果t1线程结束
或者等待超时，则当前线程回到活动状态继续执行。
join(timeout=None)
参数timeout 设置超时时间，单位是秒。如果没有设置timeout时间，则可以一直等待

使用join()方法的场景是：一个线程依赖另一个线程的运行结果，所以调用另一个线程的join()方法等待它的运行完成
"""
import threading
import time

# 共享变量0
value = 0


# 线程体函数
def thread_body():
    global value
    # 当前线程对象
    print("ThreadA 开始.....")
    for n in range(2):
        print("ThreadA 执行.......")
        value += 1
        # 线程休眠
        time.sleep(1)
        print("ThreadA 结束.......")


def main():
    print("主线程 开始........")
    t1 = threading.Thread(target=thread_body, name="ThreadA")
    # 启动线程
    t1.start()
    # 主线程被阻塞，等待t1线程结束
    t1.join()
    print("value = {0}".format(value))
    print("主线程  结束.....")


if __name__ == '__main__':
    main()
```

输出信息:

```sh
主线程 开始........
ThreadA 开始.....
ThreadA 执行.......
ThreadA 结束.......
ThreadA 执行.......
ThreadA 结束.......
value = 2
主线程  结束.....
```

### 互斥锁

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# threading.Lock()
# 使用互斥锁可以防止多个线程同时读取内存的某一个区域,互斥锁保证了每个线程同一时间只有一个在使用内存资源

"""
从系统的角度来看。锁的作用其实是将多线程变回到单线程，这是以牺牲性能，来换取程序的准确性。

在代码设计中，应该最大化地避免使用锁。即使加了锁，也要让被保护的区域尽量地少，在满足准确性的同时实现性能最大化。
在代码中，有“加锁”操作，就一定要有与之对应的“解锁”操作，否则代码失去多线程的优势。

在Python中，使用threading.RLock类来创建锁。threading.RLock类有两个方法--acquire与release

* acquire负责开始对代码进行保护，在acquire之后的代码，都将只允许一个线程进行执行。

* release方法用于停止保护（即释放锁资源）。在release之后的代码又恢复到原来的样子，可以被多线程交叉执行。
"""

from threading import Thread, Lock
import time

'''
# 互斥锁的使用

#创建锁
mutex = threading.Lock()

#锁定
mutex.acquire([blocking])

#释放锁
mutex.release()

'''
# 计数器，总票数
num = 20


def task(arg):
    global num       # 使用全局变量
    mutex.acquire()  # 锁定线程，只有1个线程可以抢用
    time.sleep(0.5)
    num -= 1
    print("{}号用户【线程】，购买成功，剩余{}张电影票".format(arg, num))
    mutex.release()  # 释放，其他线程可以进行操作


if __name__ == '__main__':
    mutex = Lock()  # 创建锁
    t_l = []
    for i in range(10):
        t = Thread(target=task, args=(i,))
        t_l.append(t)
        t.start()

    for t in t_l:
        t.join()

print("main thread end..!")

# 0号用户【线程】，购买成功，剩余19张电影票
# 1号用户【线程】，购买成功，剩余18张电影票
# 2号用户【线程】，购买成功，剩余17张电影票
# 3号用户【线程】，购买成功，剩余16张电影票
# 4号用户【线程】，购买成功，剩余15张电影票
# 5号用户【线程】，购买成功，剩余14张电影票
# 6号用户【线程】，购买成功，剩余13张电影票
# 7号用户【线程】，购买成功，剩余12张电影票
# 8号用户【线程】，购买成功，剩余11张电影票
# 9号用户【线程】，购买成功，剩余10张电影票
# main thread end..!
```

### 使用线程池提升运行效率

在 python 中使用线程池有两种方式，一种是基于第三方库 threadpool，另一种是基于 python3 新引入的库 `concurrent.futures.ThreadPoolExecutor`，这里我们介绍一下后一种。

concurrent.futures.ThreadPoolExecutor，在提交任务的时候有两种方式，一种是`submit()`函数，另一种是`map()`函数，两者的主要区别在于：

1.map 可以保证输出的顺序, submit 输出的顺序是乱的。

2.如果你要提交的任务的函数是一样的，就可以简化成 map。但是假如提交的任务函数是不一样的，或者执行的过程之可能出现异常（使用 map 执行过程中发现问题会直接抛出错误）就要用到 submit（）。

3.submit 和 map 的参数是不同的，submit 每次都需要提交一个目标函数和对应的参数，map 只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# filename: 08.使用线程池提升运行效率.py
"""

在需要频繁创建线程的系统中， 一般都会使用线程池技术。原因有两点：
    1.每一个线程的创建都是需要占用系统资源的， 是一件相对耗时的事情。同样在销毁线程时还需要回收线程资源。
    线程池技术， 可以省去创建与回收过程中所浪费的系统开销。

    2.在某些系统中需要为每个子任务来创建对应的线程(例如爬虫系统中的子链接)。
    这种情况会导致线程数量失控性暴涨， 直到程序崩溃。线程池技术可以很好地固定线程的数量保持程序稳定。


实现线程池

Python中，使用conncurrent.futures 模块下的ThreadPoolExecutor 类来实现线程池。在实例化时， 会将需要的线程个数传入。
系统就会为该线程池初始化相应个数的线程。线程池的使用有两种方式。

    * 抢占式： 线程池中的线程执行顺序不固定。该方式使用ThreadPooIExecutor 的submit方法实现。

    * 非抢占式： 线程将按照调用的顺序执行。此方式使用ThreadPoolExecutor 的map方法来实现。

从使用角度来看： 抢占式更灵活； 非抢占式更严格。


· 抢占式， 允许池中线程的处理函数不一样。如执行过程中某个线程出现异常， 也不影响其他线程。
· 非抢占式， 要求线程池中的线程必须执行同样的处理函数。而且一旦某个线程出现异常,其他线程也会停止。
"""
from concurrent.futures import ThreadPoolExecutor
import time


def printperson(p):
    '''
    定义线程池处理函数
    :param p:
    :return:
    '''
    print(p)
    time.sleep(2)


person = ["hujianli1", "hujianli2", "hujianli3"]

start_time = time.time()
for p in person:
    printperson(p)

end_time = time.time()
printperson("all spend time :{}".format(end_time - start_time))

"""
hujianli1
hujianli2
hujianli3
all spend time :6.00168251991272
"""
```

#### 实现抢占线程池

```python
start2 = time.time()
with ThreadPoolExecutor(3) as executor:
    for p in person:
        executor.submit(printperson, p)
end2 = time.time()
printperson("all spend time :{}".format(end2 - start2))

"""
hujianli1
hujianli2
hujianli3
all spend time :2.0018222332000732
"""
```

#### 实现非抢占线程池

```python
start3 = time.time()
with ThreadPoolExecutor(3) as executorl:
    executorl.map(printperson, person)
end3 = time.time()
printperson("all spend time :{}".format(end3 - start3))
"""
hujianli1
hujianli2
hujianli3
all spend time :2.001864433288574
"""
```

代码示例

```python
from concurrent.futures import ThreadPoolExecutor
from threading import Thread, currentThread
from time import time


def task(i):
    print("{} 在执行任务{}".format(currentThread().name, i))
    time.sleep(1)


if __name__ == '__main__':
    pool = ThreadPoolExecutor(4)  # 进程池里有4个进程
    for i in range(20):  # 20个任务
        pool.submit(task, i)  # 进程池里当前执行的任务i，池子里的4个进程一次一次执行任务
```

使用 map()方法启动线程，并收集线程任务的返回值。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/1/14 22:19
# filename: 09-1.线程池02.py
from concurrent.futures import ThreadPoolExecutor
import threading
import time


def action(max):
    my_sum = 0
    for i in range(max):
        print(threading.current_thread().name + " " + str(i))
        my_sum += 1
    return my_sum


# 创建一个包含4个线程的线程池
with ThreadPoolExecutor(max_workers=4) as pool:
    # 使用线程执行map计算
    # 后面的元祖有3个元素，因此程序启动了3个线程来执行action函数
    results = pool.map(action, (50, 100, 150))
    print("---------------------------------------------")
    for i in results:
        print(i)
```

#### 线程池实现并发执行命令

```python
def mulit_run(func, max_workers, args):
    """
    多线程执行命令
    :param func:  执行函数
    :param max_workers: 最多线程数
    :param args: 可迭代对象
    :return:
    """
    from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETED
    executor = ThreadPoolExecutor(max_workers=max_workers)
    all_task = [executor.submit(func, i) for i in args]
    wait(all_task, return_when=ALL_COMPLETED)
```

调用方法

```sh
mulit_run(run_cmd, len(all_images), ['docker pull %s' % i for i in set(all_images)])
```

示例：

```python
import concurrent.futures
import subprocess

def pull_docker_image(image_name):
    command = f"docker pull {image_name}"
    try:
        # 执行命令
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        # 输出命令执行结果
        print(f"Successfully pulled image: {image_name}")
        print("Output:", result.stdout)
    except subprocess.CalledProcessError as e:
        # 输出错误信息
        print(f"Failed to pull image: {image_name}")
        print("Error:", e.stderr)

def multi_pull_docker_images(image_names, num_threads):
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
        # 使用map方法并行执行函数
        executor.map(pull_docker_image, image_names)

# 假设这是你的所有镜像列表
all_images = ['ubuntu', 'nginx', 'redis', 'mysql', 'mongo']

# 指定线程数目
num_threads = 3

# 批量拉取 Docker 镜像
multi_pull_docker_images(all_images, num_threads)
```

一个扫描 IP 地址的批量 ping 工具

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
import ipaddress
import argparse
import sys
import subprocess

"""
python ip_address-ping.py -dn 192.168.0.0/24
"""


def parse_args():
    parser = argparse.ArgumentParser(description="a simple ip_address transfer mask expand client")
    parser.add_argument("-v", "--version", action="version", version='%(prog)s 0.1')
    parser.add_argument('-dn', '--target_net', help='目标网段', type=str)
    if not len(sys.argv) > 1:
        parser.print_help()
        sys.exit(1)
    args = parser.parse_args()
    return args


def get_hosts_from_network(network_str):
    try:
        network = ipaddress.ip_network(str(network_str), strict=False)
        hosts = list(network.hosts())
        return [str(host) for host in hosts]
    except ValueError as e:
        print("Error: {}".format(e))
        return []


def ping_host(ip):
    try:
        if subprocess.call(['ping', '-i', '0.2', '-c', '3', '-W', '1', ip], stdout=subprocess.DEVNULL) == 0:
            return ip
        else:
            return None
    except Exception as e:
        print(f"Error while pinging {ip}: {e}")
        return None


def mulit_run(func, max_workers, args, result_list):
    from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED
    executor = ThreadPoolExecutor(max_workers=max_workers)
    all_task = [executor.submit(func, arg) for arg in args]
    wait(all_task, return_when=ALL_COMPLETED)

    for future in all_task:
        result = future.result()
        if result is not None:
            result_list.append(result)


def main():
    args = parse_args()
    d_net = args.target_net

    hosts = get_hosts_from_network(d_net)

    connect_ip = list()
    max_workers = 10  # 设置最大线程数

    # 使用mulit_run函数执行ping命令，并将能ping通的结果存储到connect_ip列表中
    mulit_run(ping_host, max_workers, hosts, connect_ip)

    print("Connectable IPs:", connect_ip)


if __name__ == '__main__':
    main()
```

### threading.Timer()定时器

这个类表示一个动作应该在一个特定的时间之后运行 — 也就是一个计时器。Timer 是 Thread 的子类， 因此也可以使用函数创建自定义线程

Timers 通过调用它们的 start()方法作为线程启动。timer 可以通过调用 cancel()方法（在它的动作开始之前）停止。timer 在执行它的动作之前等待的时间间隔可能与用户指定的时间间隔不完全相同。

#### 简单定时器 示例

```python
import threading

def hello():
    print("hello, world")

t = threading.Timer(5.0, hello)
t.start()  # after 5 seconds, "hello, world" will be printed
```

上面的例子中，hello 函数将在 5 秒后被调用。

#### 封装一个 run_cmd() 函数

用于执行命令，并返回执行结果、标准输出和标准错误。

```python
# Prints to stderr
def printerr(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

# Prints to stdout
def printout(*args, **kwargs):
    print(*args, **kwargs)

# Exits process with a message and non-0 exit code
def fail(msg):
    sys.exit(msg)

# Runs a given command with optional timeout.
# Returns (returncode, stdout, stderr) tuple. If timeout
# occurred, returncode will be negative (-9 on macOS).
def run(cmd, timeout=None, stdin=None, show_stderr=True):
    stdout, stderr, returncode, timer = None, None, None, None
    try:
        proc = subprocess.Popen(shlex.split(cmd),
                                stdout=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                stderr=subprocess.PIPE)
        if timeout is not None:
            timer = threading.Timer(timeout, proc.kill)
            timer.start()
        stdout, stderr = proc.communicate(input=stdin)
        if len(stderr) > 0 and show_stderr:
            printerr(stderr)
        returncode = proc.returncode
    except OSError as e:
        stdout = e.strerror
        returncode = e.errno
    except Exception as e:
        stdout = str(e)
        returncode = 1
    finally:
        if timer is not None:
            timer.cancel()
        return returncode, stdout, stderr
```

#### 实现 一个 monitor_service 服务

确保安装了 psutil 和 mysql-connector-python 库。

```sh
pip install psutil mysql-connector-python
```

配置 MySQL 数据库
首先，创建一个数据库和表来存储监控数据。假设数据库名为 system_monitor，表名为 system_monitor。

```sql

CREATE DATABASE system_monitor;
USE system_monitor;
CREATE TABLE system_monitor (
    id INT AUTO_INCREMENT PRIMARY KEY,
    cpu_usage FLOAT,
    memory_usage FLOAT,
    disk_usage FLOAT,
    net_sent BIGINT,
    net_recv BIGINT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

首先，创建一个配置文件 config.ini 来存储 MySQL 等信息：

```ini
[mysql]
host = localhost
user = root
password = root
database = monitor

[service]
interval = 5
```

monitor_service.py

```python
import sys
import threading
import time
import psutil
import mysql.connector
import logging
import configparser

# 配置日志记录
logging.basicConfig(filename='/var/log/monitor_service.log', level=logging.INFO, format='%(asctime)s - %(message)s')

# 读取配置文件
config = configparser.ConfigParser()
config.read('/path/to/config.ini')

class MonitorService:
    def __init__(self):
        self.interval = config.getint('service', 'interval')
        self.timer = None
        self.running = False

    def _monitor(self):
        if not self.running:
            return

        try:
            # 获取系统信息
            cpu_usage = psutil.cpu_percent()
            memory_info = psutil.virtual_memory()
            disk_info = psutil.disk_usage('/')
            net_info = psutil.net_io_counters()

            # 记录日志
            logging.info(f"CPU Usage: {cpu_usage}%")
            logging.info(f"Memory Usage: {memory_info.percent}%")
            logging.info(f"Disk Usage: {disk_info.percent}%")
            logging.info(f"Net Sent: {net_info.bytes_sent}, Net Received: {net_info.bytes_recv}")

            # 写入 MySQL 数据库
            self._write_to_db(cpu_usage, memory_info.percent, disk_info.percent, net_info.bytes_sent, net_info.bytes_recv)
        except Exception as e:
            logging.error(f"Monitoring error: {e}")

        # 重新启动定时器
        self.timer = threading.Timer(self.interval, self._monitor)
        self.timer.start()

    def _write_to_db(self, cpu, memory, disk, net_sent, net_recv):
        try:
            with mysql.connector.connect(
                host=config.get('mysql', 'host'),
                user=config.get('mysql', 'user'),
                password=config.get('mysql', 'password'),
                database=config.get('mysql', 'database')
            ) as connection:
                with connection.cursor() as cursor:
                    cursor.execute(
                        "INSERT INTO system_monitor (cpu, memory, disk, net_sent, net_recv) VALUES (%s, %s, %s, %s, %s)",
                        (cpu, memory, disk, net_sent, net_recv)
                    )
                    connection.commit()
        except mysql.connector.Error as err:
            logging.error(f"Database error: {err}")

    def start(self):
        if not self.running:
            self.running = True
            self.timer = threading.Timer(self.interval, self._monitor)
            self.timer.start()
            logging.info("Service started")

    def stop(self):
        if self.running:
            self.timer.cancel()
            self.running = False
            logging.info("Service stopped")

    def restart(self):
        self.stop()
        time.sleep(1)
        self.start()
        logging.info("Service restarted")

if __name__ == '__main__':
    service = MonitorService()

    if len(sys.argv) != 2:
        print("Usage: python monitor_service.py [start|stop|restart]")
        sys.exit(1)

    action = sys.argv[1].lower()

    if action == 'start':
        service.start()
    elif action == 'stop':
        service.stop()
    elif action == 'restart':
        service.restart()
    else:
        print("Unknown command")
        sys.exit(1)
```

使用方法：

- 启动服务：python monitor_service.py start
- 停止服务：python monitor_service.py stop
- 重启服务：python monitor_service.py restart

最后，创建 systemd 服务配置文件 monitor_service.service：

```ini
[Unit]
Description=System Monitoring Service
After=network.target

[Service]
ExecStart=/usr/bin/python3 /path/to/monitor_service.py start
ExecStop=/usr/bin/python3 /path/to/monitor_service.py stop
ExecReload=/usr/bin/python3 /path/to/monitor_service.py restart
Restart=always
User=nobody

[Install]
WantedBy=multi-user.target
```

将服务配置文件放置在 /etc/systemd/system/ 目录下，然后执行以下命令来管理服务：

```sh
sudo systemctl daemon-reload
sudo systemctl start monitor_service
sudo systemctl enable monitor_service
sudo systemctl status monitor_service
```

这样，就可以使用 systemctl 来管理这个监控服务了

#### 实现一个 docker 清理镜像 服务

```python
# -*- encoding:utf-8 -*-
import sys
import threading
import subprocess
import logging

# 配置日志记录
logging.basicConfig(filename='/var/log/docker_cleanup.log', level=logging.INFO, format='%(asctime)s - %(message)s')

# 定义清理间隔时间（秒）
sleep_time = 3600.0  # 每小时清理一次

def run_cmd(cmd):
    try:
        result = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
        return result.decode('utf-8').strip()
    except subprocess.CalledProcessError as e:
        logging.error(f"Command '{cmd}' failed with error: {e.output.decode('utf-8').strip()}")
        return None

def cleanup_docker():
    try:
        # 清理无用镜像
        logging.info("Starting cleanup of unused Docker images...")
        run_cmd('docker image prune -f')
        logging.info("Unused Docker images cleaned up.")

        # 清理无用卷
        logging.info("Starting cleanup of unused Docker volumes...")
        run_cmd('docker volume prune -f')
        logging.info("Unused Docker volumes cleaned up.")

    except Exception as e:
        logging.error(f"Cleanup error: {e}")
    finally:
        start_watcher()

def start_watcher():
    thread = threading.Timer(sleep_time, cleanup_docker)
    thread.start()

if __name__ == "__main__":
    start_watcher()
```

这个脚本会每隔一小时（3600 秒）清理一次无用的 Docker 镜像和卷，并将日志记录到 /var/log/docker_cleanup.log 文件中。你可以根据需要调整 sleep_time 的值来改变清理的频率。

将这个脚本保存为 docker_cleanup.py，然后创建一个 systemd 服务配置文件 docker_cleanup.service：

```ini
[Unit]
Description=Docker Cleanup Service
After=network.target

[Service]
ExecStart=/usr/bin/python3 /path/to/docker_cleanup.py
Restart=always
User=nobody

[Install]
WantedBy=multi-user.target
```

将服务配置文件放置在 /etc/systemd/system/ 目录下，然后执行以下命令来管理服务：

```sh
sudo systemctl daemon-reload
sudo systemctl start docker_cleanup
sudo systemctl enable docker_cleanup
sudo systemctl status docker_cleanup
```

#### 一个循环定时服务

balancer.py

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# coding=utf-8
import logging
import sys
import threading
from time import sleep


class RepeatedTimer(object):
    def __init__(self, interval, function, *args, **kwargs):
        self._timer = None
        self.interval = interval
        self.function = function
        self.args = args
        self.kwargs = kwargs
        self.is_running = False
        self.start()

    def _run(self):
        self.is_running = False
        self.start()
        self.function(*self.args, **self.kwargs)

    def start(self):
        if not self.is_running:
            self._timer = threading.Timer(self.interval, self._run)
            self._timer.daemon = True
            self._timer.start()
            self.is_running = True

    def stop(self):
        self._timer.cancel()
        self.is_running = False


def hello(name):
    print "Hello %s!" % name


if __name__ == '__main__':

    print "starting..."
    rt = RepeatedTimer(1, hello, "World")  # it auto-starts, no need of rt.start()
    try:
        while (True):
            sleep(15)  # your long-running job goes here...
    finally:
        rt.stop()  # better in a try/finally block to make sure the program ends!
```

### 参考文献

python 线程池 ThreadPoolExecutor

- https://www.cnblogs.com/goldsunshine/p/16878089.html

Python 多线程-线程池 ThreadPoolExecutor

- https://www.cnblogs.com/yeyuzhuanjia/p/18286041

## 4.协程库

### 什么是协程？

协程是在 1963 年被提出的概念，但在最近几年才在某些语言中得到广泛的应用，比如 Lua、Python 等。

我们可以认为一个线程是调用某个函数方法，协程可以控制函数方法的执行过程，转向执行其他函数方法，并在适当的时候切换到原来的函数方法中继续执行。

Python 中常见的协程模块有 yield、yield from、async/await、asyncio、Gevent 等，只有 Gevent 是第三方模块，其他都是 Python 的内置模块。

协程实现的几种方式?

- python2.X:利用生成器通过 yield+send 实现协程
- python3.4:利用 asyncio+yield from 实现协程
- python3.5:asyncio+async/await(比较熟悉)
- python3.7:引入了 asyncio.create_task 和 asyncio.run 两个高级接口

### yield 与 yield from

```python
def consumer():
    """任务1:接收数据，处理数据"""
    print('开始接收数据')
    while True:
        print('等待中……')
        x = yield
        print('处理数据：', x)

def producer():
    """任务2:生产数据"""
    c = consumer()
    # 找到函数consumer()的yield位置
    next(c)
    for i in range(2):
        print('发送数据：', i)
        # 给函数consumer()的yield传值，然后循环给下一个yield传值
        # 使用yield的send()方法切换另一个任务
        c.send(i)

if __name__ == '__main__':
    # 基于yield保存状态，实现两个任务直接来回切换，即并发的效果
    producer()
```

yield from 重要的作用是提供了一个数据传输管道。下面通过一个简单的例子加以说明：

```python
def consumer():
    print('开始接收数据')
    while True:
        print('等待中……')
        x = yield
        print('处理数据：', x)

def wraps(c):
    while True:
        print('running')
        yield from c

def producer(wrap):
    next(wrap)
    for i in range(2):
        print('发送数据：', i)
        wrap.send(i)

if __name__ == '__main__':
    c = consumer()
    wrap = wraps(c)
    producer(wrap)
```

### 异步并发 asyncio 与 async/await

asyncio 是 Python 3.4 新加入的标准库，用来做异步 I/O。

```python
import asyncio

@asyncio.coroutine
def say_hello():
    print("Hello Python!")
    # 调用asyncio.sleep(1)，设置延时
    # yield from是协程切换执行的函数
    yield from asyncio.sleep(1)
    print("Hello Django!")

# 创建EventLoop对象
loop = asyncio.get_event_loop()
# 执行异步函数say_hello()
loop.run_until_complete(say_hello())

# 执行多个异步函数
# loop.run_until_complete(asyncio.wait([say_hello(), say_hello()]))

# 销毁EventLoop对象
loop.close()
```

不推荐使用装饰器 asyncio.coroutine，建议改用关键字 async。

关键字 async/await 是从 Python 3.5 开始引入的新语法，它主要优化 asyncio.coroutine 的语法，说明如下：

- 1.将@asyncio.coroutine 替换为 async。
- 2.将 yield from 替换为 await。

对上述例子进行改写，代码如下：

```python
import asyncio
import time

async def get_times():
  """
  程序在执行异步函数get_times()的时候，当出现延时asyncio.sleep(1)时，程序自动切换并执行另一个异步任务。
  """
    for i in range(2):
        print('进入等待')
        # 设置延时
        await asyncio.sleep(1)
        print('Hello times is ', i)

async def say_hello():
  """
  在异步函数say_hello()中，使用关键字await调用异步函数get_times()
  """
    print("Hello Python!")
    # 调用异步函数get_times()
    await get_times()
    print("Hello Django!")


start = time.time()
# 创建EventLoop对象
loop = asyncio.get_event_loop()
# 执行异步函数say_hello()
# loop.run_until_complete(say_hello())
# 执行多个异步函数
loop.run_until_complete(asyncio.wait([say_hello(), say_hello()]))
# 销毁EventLoop对象
loop.close()
end = time.time()
print('一共耗时：', end-start)
```

### 什么是 gevent?

`gevent` 是 Python 的一个并发框架，基于 `greenlet` 实现，使用了 epoll 事件监听机制以及诸多其他优化而变得高效。

其基本思想就是一个 `greenlet` 就是一个协程，当 `greenlet` 遇到 IO 操作时，比如访问网络，就会自动切换到其他的 `greenlet`，等待 IO 完成再切换回来继续执行。

`gevent` 可以帮我们自动实现这个协程切换的过程。

### 协程的例子

创建一个新的虚拟环境，并在该环境中安装必要的库

```sh
python3 -m venv venv
source venv/bin/activate
pip install gevent
```

代码

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
import gevent, time


def f1():
    for i in range(5):
        print('function:@@@f1 | NUM: @@@', i)

        # 此处阻塞，gevent会帮我们切换到其他协程去↓
        gevent.sleep(0)


def f2():
    for i in range(5):
        print('function:@@@f2 | NUM: @@@', i)

        # 此处阻塞，gevent会帮我们切换到其他协程去↑
        gevent.sleep(0)


# 创建两个协程对象，分别去执行两个函数
xc1 = gevent.spawn(f1)
xc2 = gevent.spawn(f2)

# 将协程们交给gevent去执行
gevent.joinall([xc1, xc2])
```

执行结果：

```sh
function:@@@f1 | NUM: @@@ 0
function:@@@f2 | NUM: @@@ 0
function:@@@f1 | NUM: @@@ 1
function:@@@f2 | NUM: @@@ 1
function:@@@f1 | NUM: @@@ 2
function:@@@f2 | NUM: @@@ 2
function:@@@f1 | NUM: @@@ 3
function:@@@f2 | NUM: @@@ 3
function:@@@f1 | NUM: @@@ 4
function:@@@f2 | NUM: @@@ 4
```

如上，当 `gevent` 帮我们执行两个协程的时候，首先 xc1 执行到 `gevent.sleep(0)`时发生阻塞，
此时，`gevent` 帮我们将切换到 xc2，xc2 执行到 `gevent.sleep(0)`时又发生了阻塞，此时，gevent 又帮我们将切换到 xc1 去执行。

### Q&A

#### Q：gevent 无法捕获的耗时

代码:

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# Copyright: (c)  : @Time 2024/9/18 17:27  @Author  : hjl

import gevent, time


def f1():
    for i in range(5):
        print('function:@@@f1 | NUM: @@@', i)

        # 注意这里
        time.sleep(0.1)


def f2():
    for i in range(5):
        print('function:@@@f2 | NUM: @@@', i)

        # 注意这里
        time.sleep(0.1)


# 创建两个协程对象，分别去执行两个函数
xc1 = gevent.spawn(f1)
xc2 = gevent.spawn(f2)

# 将协程们交给gevent去执行
gevent.joinall([xc1, xc2])
```

输出

```sh
function:@@@f1 | NUM: @@@ 0
function:@@@f1 | NUM: @@@ 1
function:@@@f1 | NUM: @@@ 2
function:@@@f1 | NUM: @@@ 3
function:@@@f1 | NUM: @@@ 4
function:@@@f2 | NUM: @@@ 0
function:@@@f2 | NUM: @@@ 1
function:@@@f2 | NUM: @@@ 2
function:@@@f2 | NUM: @@@ 3
function:@@@f2 | NUM: @@@ 4
```

如上，你会发现，`time.sleep(0.1)`耗费的时间，`gevent` 无法捕捉，导致代码是串行的，虽然我们创建了协程，但是并没有起到异步的作用。

怎么办呢？请看下面的解决方案。

#### A：猴子补丁

对于无法捕获的耗时，gevent 为我们提供了猴子补丁，当我们为我们的程序打了猴子补丁，

那么当我们的程序遇到任何耗时的操作，`gevent` 都会帮我们去自动切换协程，从而实现异步高并发。

代码：

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
import gevent,time
from gevent import monkey;monkey.patch_all()

def f1():
    for i in range(5):
        print('function:@@@f1 | NUM: @@@',i)

        # 注意这里
        time.sleep(0.1)

def f2():
    for i in range(5):
        print('function:@@@f2 | NUM: @@@',i)

        # 注意这里
        time.sleep(0.1)

# 创建两个协程对象，分别去执行两个函数
xc1=gevent.spawn(f1)
xc2=gevent.spawn(f2)

# 将协程们交给gevent去执行
gevent.joinall([xc1,xc2])
```

执行结果：

```sh
function:@@@f1 | NUM: @@@ 0
function:@@@f2 | NUM: @@@ 0
function:@@@f1 | NUM: @@@ 1
function:@@@f2 | NUM: @@@ 1
function:@@@f1 | NUM: @@@ 2
function:@@@f2 | NUM: @@@ 2
function:@@@f1 | NUM: @@@ 3
function:@@@f2 | NUM: @@@ 3
function:@@@f1 | NUM: @@@ 4
function:@@@f2 | NUM: @@@ 4
```

如上，你会发现协程的切换已经实现，问题完美解决。

### 实践

#### 异步 requests 请求

在 HTTP 的异步开发中，猴子补丁要在导入 gevent 之前打，否则会出现异常。

```sh
pip install requests
```

代码：

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# Copyright: (c)  : @Time 2024/9/18 17:32  @Author  : hjl

from gevent import monkey;monkey.patch_all()
import gevent, time, requests
from urllib3 import disable_warnings

disable_warnings()


def req(url):
    res = requests.get(url, verify=False)
    if res:
        print('URL:{} | CODE:{}!'.format(url, res.status_code))
    else:
        print('URL:{} FAILED!')


xc1 = gevent.spawn(req, 'https://www.baidu.com')
xc2 = gevent.spawn(req, 'https://www.gitee.com')
xc3 = gevent.spawn(req, 'https://www.huaweicloud.com')

gevent.joinall([xc1, xc2, xc3])
```

执行结果：

```sh
URL:https://www.baidu.com | CODE:200!
URL:https://www.huaweicloud.com | CODE:200!
URL:https://www.gitee.com | CODE:200!
```

### 锁

#### gevent 锁的使用

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# Copyright: (c)  : @Time 2024/9/18 17:48  @Author  : hjl
# @Author  : hujl
# 同时允许多个协程操作对象的锁,通过互斥访问,保证资源只在程序上下文被单次使用
from gevent import sleep
from gevent.pool import Pool
from gevent.lock import BoundedSemaphore

sem = BoundedSemaphore(2)  # 超过2就会阻塞等待


def worker1(n):
    # 获取锁
    sem.acquire()
    print('Worker %i acquired semaphore' % n)
    sleep(0)
    # 释放锁
    sem.release()
    print('Worker %i released semaphore' % n)


def worker2(n):
    with sem:
        print('Worker %i acquired semaphore' % n)
        sleep(0)
    print('Worker %i released semaphore' % n)


pool = Pool()
pool.map(worker1, range(0, 2))
pool.map(worker2, range(3, 6))
```

执行结果：

```sh
Worker 0 acquired semaphore
Worker 1 acquired semaphore
Worker 0 released semaphore
Worker 1 released semaphore
Worker 3 acquired semaphore
Worker 4 acquired semaphore
Worker 3 released semaphore
Worker 4 released semaphore
Worker 5 acquired semaphore
Worker 5 released semaphore
```

#### gevent 信号量实现的锁

代码：

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
from gevent import monkey;monkey.patch_all()
from gevent.lock import Semaphore
import gevent,time

# 信号量设置为1
s1=Semaphore(1)

def f1():
    for i in range(5):
        # 信号量-1，即拿到锁
        s1.acquire()
        print('function:@@@f1 | NUM: @@@',i)

        # 信号量+1，即释放锁
        s1.release()

        # 猴子补丁帮忙识别阻塞
        time.sleep(0.1)

def f2():
    for i in range(5):
        # 信号量-1，即拿到锁
        s1.acquire()
        print('function:@@@f2 | NUM: @@@',i)

        # 信号量+1，即释放锁
        s1.release()

        # 猴子补丁帮忙识别阻塞
        time.sleep(0.3)

# 创建两个协程对象，分别去执行两个函数
xc1=gevent.spawn(f1)
xc2=gevent.spawn(f2)

# 将协程们交给gevent去执行
gevent.joinall([xc1,xc2])
```

执行结果：

```sh
function:@@@f1 | NUM: @@@ 0
function:@@@f2 | NUM: @@@ 0
function:@@@f1 | NUM: @@@ 1
function:@@@f1 | NUM: @@@ 2
function:@@@f2 | NUM: @@@ 1
function:@@@f1 | NUM: @@@ 3
function:@@@f1 | NUM: @@@ 4
function:@@@f2 | NUM: @@@ 2
function:@@@f2 | NUM: @@@ 3
function:@@@f2 | NUM: @@@ 4
```

如上，可以看到，gevent 可以自动处理锁和阻塞。

按阻塞规律，f1 和 f2 会交替执行，但是加上阻塞时间，

因为 f2 的阻塞时间是 f1 的 3 倍，所以前 6 条打印中，f1 执行的次数是 f2 的三倍，即 `gevent` 可以自动判断和处理阻塞和锁同时存在的情况。

### 事件

#### Event 阻塞事件

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# Event 阻塞事件
import gevent
from gevent.event import Event

evt = Event()


def setter():
    '''After 3 seconds, wake all threads waiting on the value of evt'''
    print('A: Hey wait for me, I have to do something')
    gevent.sleep(3)
    print("Ok, I'm done")
    evt.set()  # 表示事件完成


def waiter():
    '''After 3 seconds the get call will unblock'''
    print("I'll wait for you")
    evt.wait()  # 阻塞等待事件完成
    print("It's about time")


gevent.joinall([
    gevent.spawn(setter),
    gevent.spawn(waiter),
    gevent.spawn(waiter),
    gevent.spawn(waiter),
    gevent.spawn(waiter),
    gevent.spawn(waiter)
])
```

执行结果:

```sh
A: Hey wait for me, I have to do something
I'll wait for you
I'll wait for you
I'll wait for you
I'll wait for you
I'll wait for you
Ok, I'm done
It's about time
It's about time
It's about time
It's about time
It's about time
```

#### AsyncResult 可传值的事件

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# AsyncResult 可传值的事件
import gevent
from gevent.event import AsyncResult

a = AsyncResult()


def setter():
    gevent.sleep(3)
    a.set('Hello!')  # 事件传值


def waiter():
    """
    After 3 seconds the get call will unblock after the setter
    puts a value into the AsyncResult.
    """
    print(a.get())  # 获取时间值


gevent.joinall([
    gevent.spawn(setter),
    gevent.spawn(waiter),
])
```

执行结果:

```sh
Hello!
```

### 队列

```python
#!/usr/bin/env python3
# -*- coding:utf8 -*-
# Copyright: (c)  : @Time 2024/9/18 18:18  @Author  : hjl
# @Author  : hujl
# @File    : demo8.py
# @Software: PyCharm
# @Desc    :
# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)
# /usr/local/python
# encoding:utf8
import gevent
from gevent.pool import Pool
from gevent.queue import Queue, Empty
import os

tasks = Queue(maxsize=30)  # 队列 超过30引发 gevent.hub.LoopExit
tasks1 = Queue()


def boss():
    print('放队列任务')
    for i in range(1, 25):
        tasks.put(i)


def worker1(n):
    print(len(pool))

    while not tasks.empty():  # 判断队列是否为空
        task = tasks.get()  # 获取队列内容
        tasks1.put(os.popen('id').read())
        print('Worker %s got task %s' % (n, task))
        gevent.sleep(0)  # 放弃当前任务


def worker2(name):
    try:
        while True:
            task = tasks1.get(timeout=2)
            print('获取后释放:%s' % task)

            gevent.sleep(0)
    except Empty:  # 等待超时报错完成
        print('Quitting time!')


gevent.spawn(boss).join()  # 执行单次协程任务

pool = Pool(5)  # 协程池大小
pool.map(worker1, range(0, 20))  # 通过map方法把多个任务分发给池中的5个协程

gevent.joinall([  # 同时执行多个协程任务
    gevent.spawn(worker2, 'steve'),
    gevent.spawn(worker2, 'john'),
    gevent.spawn(worker2, 'nancy'),
])
```

### 参考文献

- https://www.u1s1.vip/archives/gevent

## 5.异步 IO 库

Aio-libs 是一个协作维护组织，托管一组高质量构建的基于异步的库，包括 aiohttp。

高质量构建的一组基于异步的库

- https://github.com/aio-libs
