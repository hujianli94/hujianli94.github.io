# 21.Python 多线程和多进程

## 1.多线程与多进程

> 进程和线程的概念。

进程（Process）实际上表示的就是计算机正在进行的一个任务，比如，打开一个浏览器便是启动一个浏览器进程，打开一个记事本便是启动一个记事本进程。

但是，一个进程未必只能进行一件事，就像一个 Word 进程，在打字的同时还会有拼写检查，这些在进程内部同时进行的多个“子任务”，就称为线程（Thread）。

进程和线程的主要差别在于它们是不同的操作系统资源管理方式。

进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。

线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。

在以往的单核 CPU 上，系统执行多进程的方式是通过不断的在多个进程中切换——例如任务 1 执行 0.01 秒，切到任务 2 执行 0.01 秒再切到任务 3……以此类推，而在多核 CPU 出现后，真正的并行执行多任务才真正的得以实现，但绕是如此，一台计算机同时进行的进程是非常之多的，远远大于 CPU 的核心数量，因此，操作系统依然会将这些任务轮流调度到每个核心上运行。

如果我们要同时进行多个任务，我们有以下三种方案：

1. 写多个程序，然后同时运行
2. 在一个程序中运行多个线程
3. 多进程+多线程

### 1.1 多进程

要让 Python 程序实现多进程（multiprocessing），我们先了解操作系统的相关知识。

Unix/Linux 操作系统提供了一个 fork()系统调用，它非常特殊。普通的函数调用，调用一次，返回一次，但是 fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

子进程永远返回 0，而父进程返回子进程的 ID。这样做的理由是，一个父进程可以 fork 出很多子进程，所以，父进程要记下每个子进程的 ID，而子进程只需要调用 getppid()就可以拿到父进程的 ID。

Python 的 os 模块封装了常见的系统调用，其中就包括 fork，可以在 Python 程序中轻松创建子进程：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time

print('Process (%s) start...' % os.getpid())
# Only works on Unix/Linux/Mac:
pid = os.fork()
if pid == 0:
    print "我是子进程%s, 我的父进程是%s 我开始工作了..." % (os.getpid(), os.getppid())
    time.sleep(3)
else:
    print('我的进程id是 (%s) 我创建了一个子进程，子进程id是 (%s).' % (os.getpid(), pid))
    time.sleep(4)
```

运行结果如下：

```sh
$ python 01.os-fork-sample.py
Process (1695) start...
我的进程id是 (1695) 我创建了一个子进程，子进程id是 (1696).
我是子进程1696, 我的父进程是1695 我开始工作了...
```

由于 Windows 没有 fork 调用，上面的代码在 Windows 上无法运行。由于 Mac 系统是基于 BSD（Unix 的一种）内核，所以，在 Mac 下运行是没有问题的，推荐大家用 Mac！

有了 fork 调用，一个进程在接到新任务时就可以复制出一个子进程来处理新任务，常见的 Apache 服务器就是由父进程监听端口，每当有新的 http 请求时，就 fork 出子进程来处理新的 http 请求。

#### multiprocessing

如果你打算编写多进程的服务程序，Unix/Linux 无疑是正确的选择。由于 Windows 没有 fork 调用，难道在 Windows 上无法用 Python 编写多进程的程序？

由于 Python 是跨平台的，自然也应该提供一个跨平台的多进程支持。multiprocessing 模块就是跨平台版本的多进程模块。

##### Process

multiprocessing 模块提供了一个 Process 类来代表一个进程对象，下面的例子演示了启动一个子进程并等待其结束：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
from multiprocessing import Process
import os


# 子进程要执行的代码
def run_proc(name):
    print('Run child process %s (%s)...' % (name, os.getpid()))


if __name__ == '__main__':
    print('Parent process %s.' % os.getpid())
    p = Process(target=run_proc, args=('test',))
    print('Child process will start.')
    p.start()
    p.join()
    print('Child process end.')
```

执行结果如下：

```sh
$ /usr/bin/python 02.Process-sample.py
Parent process 1712.
Child process will start.
Run child process test (1723)...
Child process end.
```

Process 创建子进程时，只需要传入一个执行函数和函数的参数，创建一个 Process 实例，用 start()方法启动，这样创建进程比 fork()还要简单。
join()方法可以等待子进程结束后再继续往下运行，通常用于进程间的同步。

##### Process 子类创建进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time
from multiprocessing import Process


class Download(Process):
    def __init__(self, interval):
        Process.__init__(self)
        self.interval = interval

    # 重写Process类中的run()方法
    def run(self):
        # 开启这个进程所需执行的代码
        t_start = time.time()
        # time.sleep(3)     # 模拟阻塞的一个实现方式
        print("开启进程：%s进行下载操作" % os.getpid())
        print("子进程（%s）开始执行，父进程为（%s）" % (os.getpid(), os.getppid()))
        time.sleep(self.interval)
        t_stop = time.time()
        print("子进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))


if __name__ == '__main__':
    t_start = time.time()
    print("当前进程（%s）" % os.getpid())
    p = Download(2)
    p.start()
    # p.join(10)        # join 父进程等待子进程执行完毕后立刻执行
    time.sleep(10)      # 模拟阻塞，保证子进程完毕后父进程在执行
    t_stop = time.time()
    print("主进程（%s）执行完毕，耗时(%f)秒" % (os.getpid(), (t_stop - t_start)))
```

##### 使用进程池 Pool 创建进程

如果要启动大量的子进程，可以用进程池的方式批量创建子进程

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import multiprocessing as mp
import os
import time


def task(name):
    print("子进程 ({})执行的任务是 ({})".format(os.getpid(), name))
    time.sleep(1)


if __name__ == '__main__':
    print("父进程 ({})开始执行".format(os.getpid()))
    p = mp.Pool(4)
    for i in range(10):
        p.apply_async(task, args=(i,))

    p.close()
    p.join()
    print("所有子进程结束.....")
```

请注意输出的结果，

```sh
$ /usr/bin/python 04.Process-pool-sample.p
父进程 (2033)开始执行
子进程 (2047)执行的任务是 (0)
子进程 (2044)执行的任务是 (1)
子进程 (2045)执行的任务是 (2)
子进程 (2046)执行的任务是 (3)
子进程 (2047)执行的任务是 (4)
子进程 (2044)执行的任务是 (5)
子进程 (2045)执行的任务是 (6)
子进程 (2046)执行的任务是 (7)
子进程 (2047)执行的任务是 (8)
子进程 (2044)执行的任务是 (9)
所有子进程结束.....
```

说明：

1. 进程池 Pool 被创建出来后， `p.apply_async(task)` 语句不停地循环执行，相当于向进程池中提交了 10 个请求，它们会被放到一个队列中。
2. p = mp.Pool(4) 执行完毕后创建了 4 条进程，但尚未给它们分配各自的任务；也就意味着，无论有多少任务，实际的进程数只有 4 条，每次最多 4 条进程并行。
3. 当 Pool 中有进程任务执行完毕后，这条进程资源会被释放，Pool 会按先进先出的原则取出一个新的请求给空闲的进程继续执行。
4. 当 Pool 所有的进程任务完成后，会产生 4 个僵尸进程，如果主进程/主线程不结束，系统不会自动回收资源，需要调用 join 函数负责回收。
5. 在创建 Pool 进程池时，若不指定进程的最大数量，默认创建的进程数为系统的内核数量
6. 如果采用 `p.apply(task)`阻塞方式添加任务，其每次只能向进程池中添加一条任务，然后 for 循环会被阻塞等待，直到添加的任务被执行完毕，进程池中的 4 个进程交替执行新来的任务，此时相当于单进程。

进程池扫描主机端口实例

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# @auther:   18793
# @Date：    2020/6/22 11:21
# @filename: Process_Pool.py
# @Email:    1879324764@qq.com
# @Software: PyCharm

"""
进程池扫描主机端口实例
代码4-4利用单进程扫描主机端口，如果要扫描的端口范围比较大，则需要耗费比较长的时间。
利用多个进程同时扫描不同的端口范围，可以缩短程序运行时间。
进程池技术可以一次创建多个子进程，适合于子进程数量事先预知的情况。
代码利用进程池一次创建16个进程，然后利用这些进程扫描主机所有端口（0～65535），
每个进程扫描4096个端口。
"""
from multiprocessing import Pool
import os
import socket


def scan_port(ports):
    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    s.settimeout(1)
    for port in range(ports, ports + 4096):
        result = s.connect_ex((ip, port))
        if result == 0:
            print("I am process %d,port %d is openned!" % (os.getpid(), port))
    s.close()


ip = "192.168.0.109"
p = Pool(16)

for k in range(16):
    p.apply_async(scan_port, args=(k * 4096,))
p.close()
p.join()
print("All subprocesses had finished!")
```

##### 进程池

concurrent.futures - 启动并行任务

版本 3.2 中的新功能。

ProcessPoolExecutor 示例

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-


import concurrent.futures
import math

PRIMES = [
    112272535095293,
    112582705942171,
    112272535095293,
    115280095190773,
    115797848077099,
    1099726899285419]

def is_prime(n):
    if n % 2 == 0:
        return False

    sqrt_n = int(math.floor(math.sqrt(n)))
    for i in range(3, sqrt_n + 1, 2):
        if n % i == 0:
            return False
    return True

def main():
    with concurrent.futures.ProcessPoolExecutor() as executor:
        for number, prime in zip(PRIMES, executor.map(is_prime, PRIMES)):
            print('%d is prime: %s' % (number, prime))

if __name__ == '__main__':
    main()
```

#### 进程间通信

- Python 提供了多种进程间通信的方式，例如 Queue、Pipe、Value+Array 等

Queue 和 Pipe 的区别在于

- Pipe 常用来在两个进程间通信
- Queue 用来在多个进程间实现通信。

##### Queue 多进程队列的使用

Queue 模块可以用来进行线程间的通信，让各个线程之间共享数据。

Python 的 Queue 模块提供了同步、线程安全的队列类， 包括 `FIFO（先入先出）队列` `Queue、LIFO（后入先出）队列` `LifoQueue` 和`优先级队列 PriorityQueue`。

这些队列都实现了锁原语，能够在多线程中直接使用。可以使用队列实现线程间的同步。

| 方法名             | 描述                                        |
| ------------------ | ------------------------------------------- |
| Queue()            | 创建一个空的队列                            |
| q.put(item)        | 将 item 放入队列                            |
| q.get()            | 从队列中移除并返回一个 item                 |
| q.empty()          | 如果队列为空，返回 True；否则返回 False     |
| q.full()           | 如果队列已满，返回 True；否则返回 False     |
| q.qsize()          | 返回队列中的 item 数量                      |
| q.put_nowait(item) | 将 item 放入队列，不等待队列可用            |
| q.get_nowait()     | 从队列中移除并返回一个 item，不等待队列可用 |
| q.task_done()      | 表示之前入队的任务已经完成                  |
| q.join()           | 阻塞直到队列中所有的 item 都被处理完毕      |

代码示例 1

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
from multiprocessing import Process, Queue
import time

'''
2个子进程在队列中进行写入和读取数据，实现进程之间的通信
'''


def write(q):
    if not q.full():
        for i in range(5):
            message = "消息" + str(i)
            q.put(message)
            print("写入:{}".format(message))


def read(q):
    time.sleep(1)
    while not q.empty():
        print("读取:{}".format(q.get(True, 2)))


if __name__ == '__main__':
    print("主进程开始".center(100, "*"))
    q = Queue()
    pw = Process(target=write, args=(q,))
    pr = Process(target=read, args=(q,))
    pw.start()
    pr.start()
    pw.join()
    pr.join()
    print("主进程结束".center(100, "*"))

```

代码示例 2

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 10:55
# filename: 进程间通信01.py
from multiprocessing import Process, Queue
import os, time, random


# 写数据进程执行的代码
def proc_write(q, urls):
    print('Process(%s) is writing...' % os.getpid())
    for url in urls:
        q.put(url)
        print('Put %s to queue...' % url)
        time.sleep(random.random())


# 读进程执行的代码
def proc_read(q):
    print('Process(%s) is reading...' % os.getpid())
    while True:
        url = q.get(True)
        print('Get %s from queue.' % url)


if __name__ == '__main__':
    # 父进程创建Queue，并传给各个子进程
    q = Queue()
    proc_writer1 = Process(target=proc_write, args=(q, ['url1', 'url2', 'url3']))
    proc_writer2 = Process(target=proc_write, args=(q, ['url4', 'url5', 'url6']))
    proc_reader = Process(target=proc_read, args=(q,))
    # 启动子进程proc_writeer 写入
    proc_writer1.start()
    proc_writer2.start()
    # 启动子进程proc_reader,读取
    proc_reader.start()
    # 等待子进程proc_writer结束
    proc_writer1.join()
    proc_writer2.join()
    # proc_reader进程里是死循环，无法等待其结束，要强行终止
    proc_reader.terminate()

"""
Process(10608) is writing...
Put url1 to queue...
Process(7808) is writing...
Put url4 to queue...
Process(13840) is reading...
Get url1 from queue.
Get url4 from queue.
Put url5 to queue...
Get url5 from queue.
Put url2 to queue...
Get url2 from queue.
Put url6 to queue...
Get url6 from queue.
Put url3 to queue...
Get url3 from queue.
"""
```

##### Pipe 常用来在两个进程间进行通信

两个进程分别位于管道的两端。 Pipe 方法返回（conn1，conn2）代表一个管道的两个端。

Pipe 方法有 duplex 参数，如果 duplex 参数为 True（默认值），那么这个管道是全双工模式，也就是说 conn1 和 conn2 均可收发。

若 duplex 为 False，conn1 只负责接收消息，conn2 只负责发送消息。 send 和 recv 方法分别是发送和接收消息的方法。 例如，在全双工模式下，可以调用 conn1.send 发送消息，conn1.recv 接收消息。

如果没有消息可接收，recv 方法会一直阻塞。如果管道已经被关闭，那么 recv 方法会抛出 EOFError。

```python
#!/usr/bin/env python
# -*- coding:utf8 -*-
# auther; 18793
# Date：2020/2/26 11:08
# filename: sample01.py
import multiprocessing
import random
import time, os


def proc_send(pipe, urls):
    for url in urls:
        print("Process(%s) send :%s" % (os.getpid(), url))
        pipe.send(url)
        time.sleep(random.random())


def proc_recv(pipe):
    while True:
        print("Process(%s) rev:%s" % (os.getpid(), pipe.recv()))
        time.sleep(random.random())


if __name__ == '__main__':
    pipe = multiprocessing.Pipe()
    p1 = multiprocessing.Process(target=proc_send, args=(pipe[0], ["url_" + str(i) for i in range(10)]))
    p2 = multiprocessing.Process(target=proc_recv, args=(pipe[1],))
    p1.start()
    p2.start()
    p1.join()
    p1.join()
    p2.terminate()

"""
Process(17008) send :url_0
Process(13264) rev:url_0
Process(17008) send :url_1
Process(17008) send :url_2
Process(13264) rev:url_1
Process(17008) send :url_3
Process(17008) send :url_4
Process(13264) rev:url_2
Process(17008) send :url_5
Process(13264) rev:url_3
Process(17008) send :url_6
Process(17008) send :url_7
Process(13264) rev:url_4
Process(13264) rev:url_5
Process(17008) send :url_8
Process(17008) send :url_9
Process(13264) rev:url_6
"""
```
