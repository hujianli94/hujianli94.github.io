# 中间件容器化

通过前面的学习，我们基本掌握了 Kubernetes 的核心概念，相信读者已经能够部署一些比较简单的服务，同时也可以将公司的应用迁移至
Kubernetes。

但是在使用 Kubernetes 时，只部署一些简单的单体应用并非我们的最终目标。

在很多情况下，我们还需要将一些复杂的中间件部署到 Kubernetes，比如 Redis、RabbitMQ、Kafka、Zookeeper 集群等。

由于这些应用比较复杂，并且搭建时涉及的资源文件比较多，因此使用传统的管理方式显得很麻烦，我
们需要一种简单的方式就可以部署、管理 Kubernetes 上的比较复杂的应用，在 Kubernetes 中，可以使用包管理工具 Operator 和 Helm 来管理比较复杂的中
间件，本章的内容就是使用 Kubernetes 包管理工具将比较复杂的中间件部署至 Kubernetes。

## 1. 传统架构如何管理中间件集群

使用 Kubernetes 部署中间件集群之前，我们先来回顾一下，在传统架构中是如何管理中间件集群的。

在传统架构中，管理一个中间件集群的流程大致如图

1）假设现在需要中间件集群，比如需要一个三主三从的 Redis 集群。

2）如果公司环境目前没有服务器，需要新申请 3 ～ 6（有可能是复用之前的服务器搭建集群）台服务器用来部署 Redis 集群。

3）从 Redis 官方网站下载某个版本的 Redis 安装包，并且安装到每一台服务器上，同时可能每台服务器也需要做相关的优化配置，比如修改提升性能
的内核等。

4）进行每台服务器的 Redis 配置，如果是一台服务器部署两个 Redis 实例，可能还需要配置不同的端口号等。

传统架构管理中间件
![](../../assets/static/kubernetes/5/chuantong-01.png){: .zoom}

5）启动每台服务器上的 Redis 实例。

6）通过相关命令建立集群，当然有的中间件是没有此步骤的，可以直接通过相关配置文件直接建立集群。

7）如果集群想要添加节点，可能就需要重复 2）～ 6）的步骤。

综上可知，在传统架构中想要搭建一个类似于 Redis 的中间件集群，其实是一件挺烦琐且非常具有重复性的事，并且由于服务器的环境不同，在安装
过程中可能还会遇到各种各样的问题，浪费很多时间和精力。

但是在 Kubernetes 中，可以利用 Kubernetes 的特性非常方便且迅速地启动一个对应的集群。接下来讲解如何在 Kubernetes 中部署相关的中间件。

## 2. Kubernetes 如何管理中间件集群

在 Kubernetes 中，可以使用 Kubernetes 的包管理工具非常方便地搭建一个中间件集群，最为常用的包管理工具还是以 Operator 和 Helm 为主，首先我
们来简单看一下 Operator 和 Helm 的区别。

### 2.1 Operator 和 Helm 的区别

简而言之，包管理工具就是把相关文件的定义统一管理，然后可以很方便地通过这些工具管理比较复杂的应用，比如 MyS?L 集群、Redis 集群等，实
现一键创建集群、扩容、备份等。当然，公司内开发的应用程序也可以通过 Kubernetes 的包管理工具进行管理，目前常用的两种包管理工具是 Operator
和 Helm，类似的还有 Kustomize、CNAB 等。

虽然 Operator 和 Helm 实现的功能类似，但是两者还是有很大的区别的。

==Operator 更倾向于管理复杂的有状态服务，比如 MySQL 集群、Redis 集群、TiDB 集群等，而 Helm 更倾向于管理无状态应用的部署，比如公司的服务、某
些不需要持久化数据的中间件、不需要实现额外功能的服务等。==

如果读者之前使用 Operator 和 Helm 部署过一些服务，就会知道很多中间件的部署都有 Helm 和 Operator 两种方式可以选择，比如 RabbitMQ 可以通过 Helm 部署，也可以通过 Operator 部署。读者可能会有一些疑惑，既然功能类似，为何还要大费周章地重复“造轮子”呢？

前文提到过，Helm 适合部署不需要额外功能的服务，这里提到的额外功能有备份、回滚以及更高级的用法。

如果想要实现这些逻辑，使用 Helm 不容易实现，或者无法实现，而 Operator 可以通过代码来实现相关的逻辑，这也是为什么**Operator 更倾向于管理更为复杂的服务。**

另外，还有一个很大的区别，就是两种工具的实现难度是不一样的。

比如自己写一个 Helm 的 Chart（Helm 包的名称叫 Chart），只需要将相关的模板文件放在 Chart 的 templates 目录， 然后抽离相关的配置， 放置于
values.yaml，模板文件通过 go template 语法即可生成相关的资源文件并加载到 Kubernetes 中。

而 Operator 需要自己动手编写一个控制器，这个控制器可以解析自己定义的 CRD（Custom Resource Define，自定义资源类型），然
后生成相关的资源文件并加载到 Kubernetes 中。想要实现上述所讲的备份、回滚等功能，也需要自己编写相应的代码实现。

所以 Operator 需要一定的开发能力，可以用 Golang、Java 等语言。

在实际使用时，两者没有最佳选择，只有更合适的选择。

比如想要搭建某个流行的中间件集群（比如 Redis 集群），我们不需要从头开始写一个 Redis 集群的 Operator 或者 Helm 的 Chart。

因为像 Redis 这样比较流行的中间件，官方或者第三方平台（比如 Bitnami）已经为我们提供了一些非常好用且
功能齐全的 Operator 或者 Helm Chart，直接使用即可。

在企业内部编写 Operator 或者 Helm Chart 大部分都是基于公司自己的业务系统展开的（当然，一些公司也会为开源项目做出自己的贡献，也会为中间件编写相应的
Operator，比如 UCloud 提供的 Redis Cluster Operator 也是非常好用的），比如公司的某个产品，可以一键式部署或者迁移等。

**而像这类一键式搭建业务系统的需求，编写 Helm 要比 Operator 简单很多，但是如果想要实现更多、更复杂的逻辑，Operator 可能更为合适。**

### 2.2 Kubernetes 管理中间件集群的流程

之前我们已经介绍了传统架构管理中间件集群的流程，接下来介绍 Kubernetes 是如何利用包管理工具来管理中间件集群的。

无论是选择 Helm 还是 Operator 来管理中间件，都是非常简单的。Helm 和 Operator 管理中间件的流程如图

![](../../assets/static/kubernetes/5/k8s-managezhongjj.png){: .zoom}

比如使用 Helm 管理中间件集群的流程如下：

1）首先集群中需要有一个 Helm 的客户端，用来执行增删改查命令，和 Kubectl 类似。

2）之后需要找到对应的 Chart（可以在中间件的官网或者 GitHub 中查找），比如安装 Redis、Zookeeper 集群的包。这个包和镜像类似，都是放在
一个仓库中，镜像放在镜像仓库，Chart 放在 Chart 仓库。

3）如果是新建集群，只需要执行 helm install 命令即可一键式创建该集群。如果想要更新配置，直接使用 helm upgrade 即可。

使用 Operator 管理中间件的流程如下：

1）相对于 Helm 需要安装单独的客户端工具，Operator 不需要单独的客户端工具，使用 kubectl 即可。所以第一步就是找到对应的 Operator（和 Helm 类
似，可以在 GitHub 或者其官方网站上查找）。

2）创建对应的控制器，用来解析一些自定义资源逻辑的程序。

3）之后创建自定义资源即可，对应的控制器会解析自定义资源实现相关功能，比如创建集群、扩容、备份等。

由此看来，Helm 和 Operator 可以很方便地实现中间件集群的管理和维护。接下来我们通过几个简单的例子实践一下 Helm 和 Operator 的使用。

## 3. Operator 的使用

### 3.1 使用 Operator 安装 Redis 集群

本小节演示的 Operator 是由 UCloud 开源的一个项目。

!!! info "项目地址"

    https://github.com/ucloud/redis-cluster-operator，

接下来根据前文提到的流程创建一个具有 6 个实例的 Redis 集群。

首先需要下载该项目至服务器：

```shell
$ git clone https://github.com/ucloud/redis-cluster-operator.git
$ cd redis-cluster-operator
```

之后创建对应的 CRD，然后就可以通过这些 CRD 声明一个创建 Redis 集群的 YAML 文件：

```shell
$ kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml
$ kubectl create -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml
```

假设需要将 Redis 集群部署至 redis-cluster 命名空间，需要先创建 redis-cluster 命名空间和对应的一些权限，并且将 Operator 控制器安装至该
命名空间：

```shell
$ kubectl create ns redis-cluster
$ kubectl create -f deploy/service_account.yaml -n redis-cluster
$ kubectl create -f deploy/namespace/role.yaml -n redis-cluster
$ kubectl create -f deploy/namespace/role_binding.yaml -n redis-cluster
$ kubectl create -f deploy/namespace/operator.yaml -n redis-cluster
```

查看 Redis Operator 的 Pod 状态：

```shell
$ kubectl get pod -n redis-cluster
NAME                                      READY   STATUS    RESTARTS   AGE
redis-cluster-operator-675ccbc697-c6m2t   1/1     Running   0          37s
```

接下来通过一个自定义类型的 YAML 文件一键式启动一个三主三从的节点，在启动之前先来看一下这个文件的内容：

```shell
$ cat deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml
apiVersion: redis.kun/v1alpha1
kind: DistributedRedisCluster
metadata:
  annotations:
    # if your operator run as cluster-scoped, add this annotations
    #redis.kun/scope: cluster-scoped
  name: example-distributedrediscluster
spec:
  # Add fields here
  masterSize: 3
  clusterReplicas: 1
  image: redis:5.0.4-alpine
```

注意看该文件的 kind 和 apiVersion，在标准的 Kubernetes 中，是没有 apiVersion 为 redis.kun/v1alpha1、kind 为 DistributedRedisCluster 这种类
型的资源的。

我们常用的是类似于 Deployment、StatefulSet 的资源，这种不属于原生资源的资源称为自定义资源类型（Custom Resource Definition，
CRD )。

上述是通过命令` kubectl create -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml`创建了“扩展资源类型”。

继续看该文件的 spec 定义，也是和原生的资源定义有所不同，所以标准 Kubernetes 解析不了该类型的资源，该类型的资源创建后，由 Redis
Operator 负责解析。接下来看一下这段 YAML 代码具体的含义：

- masterSize：新建的 Redis 集群有几个主节点。
- clusterReplicas：每个主节点有几个从节点。
- image：该集群使用哪个镜像。

!!! Warning "注意"

    由于我们创建的是命名空间基本的Operator，因此需要将annotations的redis.kun/scope:cluster-scoped注释掉。

在明白其配置含义后，通过一条命令即可创建一个 Redis 集群：

```shell
$ kubectl create -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml -n redis-cluster
distributedrediscluster.redis.kun/example-distributedrediscluster created
```

创建完成后，可以通过查看自定义 distributedrediscluster 资源的状态来判断 Redis 集群是否完成了初始化：

```sh
$ kubectl get distributedrediscluster -n redis-cluster
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Scaling   42s
```

此时也可以查看 Pod 的状态，虽然都是正常状态，但是由于没有完成槽的分配，因此当前集群还是不可用的状态：

```sh
$ kubectl get pod -n redis-cluster -w
NAME                                      READY   STATUS    RESTARTS   AGE
drc-example-distributedrediscluster-0-0   1/1     Running   0          7m3s
drc-example-distributedrediscluster-0-1   1/1     Running   0          6m3s
drc-example-distributedrediscluster-1-0   1/1     Running   0          7m3s
drc-example-distributedrediscluster-1-1   1/1     Running   0          6m23s
drc-example-distributedrediscluster-2-0   1/1     Running   0          7m3s
drc-example-distributedrediscluster-2-1   1/1     Running   0          6m23s
redis-cluster-operator-675ccbc697-c6m2t   1/1     Running   0          14m
```

当状态由 Scaling 变成了 Healthy（此过程可能需要 30 分钟左右的时间），即表示 Redis 集群已经创建完成，并完成了槽的配置，此时已经可以正
常使用：

```sh
$ kubectl get distributedrediscluster -n redis-cluster
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   3            Healthy   7m7s
```

在创建 Redis 集群后，会创建如下几个 Service：

```sh
$ kubectl get svc -n redis-cluster
NAME                                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)              AGE
example-distributedrediscluster     ClusterIP   10.104.160.70   <none>        6379/TCP,16379/TCP   7m52s
example-distributedrediscluster-0   ClusterIP   None            <none>        6379/TCP,16379/TCP   7m52s
example-distributedrediscluster-1   ClusterIP   None            <none>        6379/TCP,16379/TCP   7m52s
example-distributedrediscluster-2   ClusterIP   None            <none>        6379/TCP,16379/TCP   7m52s
redis-cluster-operator-metrics      ClusterIP   10.101.25.137   <none>        8383/TCP,8686/TCP    14m
```

其中 example-distributedrediscluster 为程序端用来操作 Redis 集群的 Service 名称，可以通过 redis://example-distributedrediscluster.redis-cluster:6379 链接至该集群。

接下来我们可以使用任意具有 Redis 客户端工具的 Pod 进行连接测试，比如使用 drc-example-distributedrediscluster-0-0 这个 Pod 进行测试，首先
登录至该 Pod（由于 Redis 客户端和 Redis 集群是同一个 Namespace，因此.redis-cluster 可以省略）：

```shell
$ kubectl exec -it drc-example-distributedrediscluster-0-0 -n redis-cluster -- sh
/data # redis-cli -h example-distributedrediscluster
example-distributedrediscluster:6379> info
.....

# Replication
role:slave
master_host:10.0.14.130
master_port:6379
master_link_status:up
master_last_io_seconds_ago:10
master_sync_in_progress:0
slave_repl_offset:728
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:a303475b201463f9c7476f9356f73fb5511b8e57
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:728
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:728
```

使用 redis-cli 连接到 Redis 集群后，可以执行 Redis 的相关命令查看一些信息，在此不再演示，有兴趣的读者可以自行查找学习。接下来我们进行数
据的写入。

请注意框内的提示，我们在执行 set 命令时，Redis 抛出一个异常 MOVED，这就是 Redis 集群的机制，了解过 Redis 集群架构的读者
可能有所了解。

Redis 集群采用分片机制，每个数据插入时会选择一个对应的槽存放数据，如果该槽位不在这个已经连接的实例上，就需要进行“下一
跳”，也就是会连接到具有该槽位的节点上，再写入数据，当然如果该槽位正好位于正在连接的这个实例上，就会直接写入数据。

```sh
/data # redis-cli -h example-distributedrediscluster
example-distributedrediscluster:6379> set a 1
(error) MOVED 15495 10.0.20.225:6379
```

接下来连接到 10.0.20.225:6379 这个实例上，尝试写入数据

```sh
/data # redis-cli -h 10.0.20.225
10.0.20.225:6379> set a 1
OK
10.0.20.225:6379> get a
"1"
10.0.20.225:6379> quit
/data #
```

可以看到当连接至 172.27.14.202 节点时，就可以设置 key 为 a 的值，当查看 a 的值的时候，也能直接返回结果，当然如果连接的不是 10.0.20.225 这
个实例，也是返回 MOVED 错误。

由于 Redis 集群的这种机制，因此在 Kubernetes 集群外部的客户端无法正常使用 Kubernetes 中的 Redis 集群，因为在它需要 MOVED 时，会返回一个 Pod 的
IP，这个 IP 为 Kubernetes 内部 IP，标准的 Kubernetes 不会让外部网络和其直接通信，如果需要集群外部连接至 Kubernetes 内部的 Redis 集群，可以采用
Redis 集群的代理工具进行解决，有兴趣的读者可以自行尝试。

但是生产环境的 Redis 集群一般不会直接暴露在外，这也是一种安全的手段，非生产环境采用单示例的 Redis 即可满足需要，单示例的 Redis 是没有此类问题的，可以直接通过 NodePort 类型的 Service 进行连接。

> 注意
> 以上演示的集群安装并没有进行 Redis 集群数据的持久化，如果想要持久化数 据 ， 需要有 StorageClass 的支持，提供动态存储。
>
> 之后使用 deploy/example/persistent.yaml 文件创建具有持久化数据的 Redis 集群，只需要修改 class 字段为集群内块存储的 StorageClass 名字即可，size 按需修改。当然，不要忘记注释 redis.kun/scope: cluster-scoped 字段。

### 3.2 Redis 集群一键扩容

上一小节通过 Operator 很方便、迅速地搭建了一个 Redis 集群，接下来介绍如何通过 Operator 完成一键式集群扩容。

前面提到过自定义资源的配置，有一个参数为 masterSize: 3，该参数决定了该集群一共有多少个 Master 节点，clusterReplicas 参数决定了每个
Master 节点有几个从节点，如果想要扩容该集群，只需要更改这两个参数即可。

比如将集群扩容为具有 4 个 Master 节点的集群：

```sh
$ grep "master" deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml
  masterSize: 4

$ kubectl replace -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml -n redis-cluster
distributedrediscluster.redis.kun/example-distributedrediscluster replaced
```

再次查看 Pod，会发现创建了两个新的 Pod：

```sh
$ kubectl get pod -n redis-cluster
NAME                                      READY   STATUS    RESTARTS   AGE
drc-example-distributedrediscluster-0-0   1/1     Running   0          54m
drc-example-distributedrediscluster-0-1   1/1     Running   0          53m
drc-example-distributedrediscluster-1-0   1/1     Running   0          54m
drc-example-distributedrediscluster-1-1   1/1     Running   0          53m
drc-example-distributedrediscluster-2-0   1/1     Running   0          54m
drc-example-distributedrediscluster-2-1   1/1     Running   0          53m
drc-example-distributedrediscluster-3-0   1/1     Running   0          2m37s
drc-example-distributedrediscluster-3-1   1/1     Running   0          97s
redis-cluster-operator-675ccbc697-c6m2t   1/1     Running   0          61m
```

此时集群的状态又会变成 Scaling 然后变成 Healthy 状态：

```sh
$ kubectl get distributedredisclusters -n redis-cluster
NAME                              MASTERSIZE   STATUS    AGE
example-distributedrediscluster   4            Healthy   55m
```

---

!!! Warning "注意"

    > 在扩容和缩容的过程中，Redis集群是不可用状态，所以最好在非业务时间段进行扩容。Redis集群的可用性测试和创建小节一致，
    > 此处不再演示。

### 3.3 集群清理

如果只是用来测试和学习，在操作完成后可以清理该集群，清理步骤较传统架构简单，只需要将安装的 create 改成 delete 即可：

```sh
$ kubectl delete -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml -n redis-cluster
$ kubectl delete -f deploy/cluster/operator.yaml -n redis-cluster
$ kubectl delete -f deploy/cluster/cluster_role_binding.yaml -n redis-cluster
$ kubectl delete -f deploy/cluster/cluster_role -n redis-cluster
$ kubectl delete -f deploy/service_account.yaml -n redis-cluster
$ kubectl delete -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml -n redis-cluster
$ kubectl delete -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml -n redis-cluster
$ kubectl delete ns redis-cluster
```

如果以后不需要再创建 Redis 集群，可以把 CRD 也删除：

```sh
$ kubectl delete -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml
$ kubectl delete -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml
```

## 4. Helm 的使用

上一小节演示了如何使用 Operator 快速创建一个 Redis 集群，同时也实现了一键式扩容。

本节将会继续学习 Kubernetes 包管理工具 Helm 的使用，实现一键式创建 Zookeeper、Kafka 集群等。

### 4.1 安装 Helm 客户端

使用 Helm 和 Operator 不同的是，Helm 的操作需要使用单独的 Helm 客户端工具。对于 Helm V3 和 V2 的安装是有所区别的，Helm V3 只需要将 Helm 客户端
的二进制文件放在/usr/local/bin/下即可，V2 除了二进制工具外，还需要安装一个 Tiller 服务端，由于现在主要以 V3 为主，因此不再演示 V2 的安装和使
用。

首先在 Helm 官方下载 Helm 的二进制包（此处演示的为 Linux 的安装步骤，其他步骤可以参考：https://helm.sh/docs/intro/install/）：

软件包下载地址为：https://github.com/helm/helm/releases

```sh
# 本例安装的为3.7.2，其他版本下载地址https://github.com/helm/helm/releases
$ wget https://mirrors.huaweicloud.com/helm/v3.7.2/helm-v3.7.2-linux-amd64.tar.gz
$ tar -zxvf helm-v3.7.2-linux-amd64.tar.gz
$ cd linux-amd64/
$ cp helm /usr/local/bin/
$ chmod a+x  /usr/local/bin/helm

#查看版本
$ helm version
version.BuildInfo{Version:"v3.7.2", GitCommit:"663a896f4a815053445eec4153677ddc24a0a361", GitTreeState:"clean", GoVersion:"go1.16.10"}
```

```shell
# 脚本安装
$ curl -fsSL -o get_helm.sh \
  https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh



# 二进制安装
wget https://get.helm.sh/helm-v3.11.1-linux-amd64.tar.gz
tar xvf helm-v3.11.1-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin
rm -rf linux-amd64 helm-v3.7.0-linux-amd64.tar.gz



# 补全
helm completion bash > /etc/bash_completion.d/helm
```

### 4.2 Helm 客户端命令入门

在学习 Helm 命令之前，先来认识一下 Helm。Helm 的机制和 Docker 有些类似，Docker 有镜像的概念，用来放置相关程序，并且镜像是放在镜像仓库里
面进行管理的。

而 Helm 相对于 Docker 镜像有一个名词叫 Chart，是 Helm 放置模板和配置的地方，同时 Chart 也可以放在仓库中，只不过放置镜像的叫镜像仓
库，放置 Chart 的叫 Chart 仓库，比较主流的镜像仓库 Harbor 不仅支持镜像的存储，也支持 Chart 的存储。

有了上述理解，结合之前讲过的管理中间件的流程，可以知道我们在安装相关中间件的时候，需要先找到对应的包（就是前面讲的 Chart），而这个
包有一个存放的仓库，因此需要先在当前环境添加这个仓库，从这个仓库中下载该包，然后进行安装（不下载也可以直接安装）即可。

安装完 Helm 后，可以执行 helm repo list 查看已经添加的仓库有哪些：

```sh
$ helm repo list
NAME                            URL
kubernetes-dashboard            https://kubernetes.github.io/dashboard/
ingress-nginx                   https://kubernetes.github.io/ingress-nginx
nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/
mycharts                        https://hujianli94.github.io/mycharts/
```

默认情况下，有一个名字为 stable 的仓库，仓库地址是https://charts.helm.sh/stable。
之后可以通过 helm repo add 命令添加一个 BitNami（BitNami 是一个开源项目，具有很多解决方案，我们常用的很多服务在 BitNami 仓库都能找到）的仓库：

```sh
$ helm repo add bitnami https://charts.bitnami.com/bitnami
$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "nfs-subdir-external-provisioner" chart repository
...Successfully got an update from the "mycharts" chart repository
...Successfully got an update from the "ingress-nginx" chart repository
```

添加完成后再次查看仓库列表：

```sh
$ helm repo list
NAME                            URL
.....
bitnami                         https://charts.bitnami.com/bitnami
```

接下来可以通过 helm search 命令查看 BitNami 仓库有哪些可用的 Chart：

```sh
$ helm search repo bitnami
NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/airflow                                 14.0.12         2.5.1           Apache Airflow is a tool to express and execute...
bitnami/apache                                  9.2.16          2.4.55          Apache HTTP Server is an open-source HTTP serve...
bitnami/appsmith                                0.1.13          1.9.8           Appsmith is an open source platform for buildin...
bitnami/argo-cd                                 4.4.10          2.6.2           Argo CD is a continuous delivery tool for Kuber...
bitnami/argo-workflows                          5.1.9           3.4.5           Argo Workflows is meant to orchestrate Kubernet...
bitnami/aspnet-core                             4.0.6           7.0.3           ASP.NET Core is an open-source framework for we...
bitnami/cassandra                               10.0.3          4.1.0           Apache Cassandra is an open source distributed ...
bitnami/cert-manager                            0.9.1           1.11.0          cert-manager is a Kubernetes add-on to automate...
bitnami/clickhouse                              3.0.2           23.1.3          ClickHouse is an open-source column-oriented OL...
bitnami/common                                  2.2.3           2.2.3           A Library Helm Chart for grouping common logic ...
.....
```

- NAME：Chart 的名字。
- CHART VERSION：Chart 的版本。
- APP VERSION：Chart 内应用的名称。
- DESCRIPTION：Chart 的描述文件。

也可以直接搜索软件的名字，比如搜索 Kafka 相关的 Chart：

```sh
$ helm search repo kafka
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/kafka                   21.0.1          3.4.0           Apache Kafka is a distributed streaming platfor...
bitnami/dataplatform-bp2        12.0.5          1.0.1           DEPRECATED This Helm chart can be used for the ...
bitnami/schema-registry         8.0.6           7.3.1           Confluent Schema Registry provides a RESTful in...
```

接下来可以使用 helm pull 将某个 Chart 下载至本地，比如将上述检索的 Kafka 下载至本地：

```sh
[root@k8s-master01 helm-devops]# mkdir helm-study
[root@k8s-master01 helm-devops]# cd helm-study/
[root@k8s-master01 helm-study]# helm pull bitnami/kafka
[root@k8s-master01 helm-study]# ls
kafka-21.0.1.tgz
```

此时下载的为新版的 Chart，读者的版本可能会不一致，我们可以通过-l 参数查看某个 Chart 的历史版本：

```sh
$ helm search repo kafka -l
NAME                            CHART VERSION   APP VERSION     DESCRIPTION
bitnami/kafka                   21.0.1          3.4.0           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   21.0.0          3.4.0           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.1.1          3.4.0           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.1.0          3.4.0           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.6          3.3.2           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.5          3.3.2           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.4          3.3.1           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.3          3.3.1           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.2          3.3.1           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.1          3.3.1           Apache Kafka is a distributed streaming platfor...
bitnami/kafka                   20.0.0          3.3.1           Apache Kafka is a distributed streaming platfor...
.....
```

然后通过指定--version 参数下载指定版本：

```sh
$ helm pull bitnami/kafka --version=17.2.3
$ ls
kafka-17.2.3.tgz  kafka-21.0.1.tgz
```

其他关于 Helm 的增删改查，在安装中间件时再为读者介绍。

### 4.3 部署 Zookeeper

接下来通过 Helm 安装一个 Zookeeper 集群至 Kubernetes。使用 Helm 安装 Chart 时，需要注意 Chart 包可以下载至本地，也可以不用下载至本地。

本小节演示的是下载至本地进行安装，下一小节将演示无须下载至本地进行安装。

首先将 Zookeeper 的 Chart 包下载至本地：

```sh
$ helm pull bitnami/zookeeper
$ ls
kafka-17.2.3.tgz  kafka-21.0.1.tgz  zookeeper-11.1.3.tgz
```

之后将该包解压，然后修改 values.yaml 的配置即可：

```sh
$ tar xf zookeeper-11.1.3.tgz
$ ls
kafka-17.2.3.tgz  kafka-21.0.1.tgz  zookeeper  zookeeper-11.1.3.tgz

$ cd zookeeper/
$ ll
total 108
-rw-r--r-- 1 root root   219 Feb 18 05:13 Chart.lock
drwxr-xr-x 3 root root    20 Feb 24 10:21 charts
-rw-r--r-- 1 root root   740 Feb 18 05:13 Chart.yaml
-rw-r--r-- 1 root root 60053 Feb 18 05:13 README.md
drwxr-xr-x 2 root root  4096 Feb 24 10:21 templates
-rw-r--r-- 1 root root 36146 Feb 18 05:13 values.yaml
```

和 Operator 的区别是 ， Operator 通过 CRD 管理配置 ， Helm 通过 values.yaml 管理配置，所以我们只需要更改 values.yaml 即可实现一些定制
化配置。打开 values.yaml 修改一些常用配置：

```sh
$ vim values.yaml

replicaCount: 3
# 演示环境关闭认证，生产环境需要按需开启
auth:
  client:
    enabled: false
persistence:
  # 关闭数据持久化，测试环境
  enabled: false
  existingClaim: ""
  # 生产环境需要配置storageClass实现数据持久化
  storageClass: ""
  accessModes:
    - ReadWriteOnce
  size: 8Gi
  annotations: {}
```

修改完毕后，一条命令即可创建 Zookeeper 集群，假设部署至 public-service 命名空间：

```sh
$ helm install zookeeper . --namespace public-service --create-namespace
NAME: zookeeper
LAST DEPLOYED: Fri Feb 24 10:32:01 2023
NAMESPACE: public-service
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: zookeeper
CHART VERSION: 11.1.3
APP VERSION: 3.8.1

** Please be patient while the chart is being deployed **

ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:

    zookeeper.public-service.svc.cluster.local

To connect to your ZooKeeper server run the following commands:

    export POD_NAME=$(kubectl get pods --namespace public-service -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
    kubectl exec -it $POD_NAME -- zkCli.sh

To connect to your ZooKeeper server from outside the cluster execute the following commands:

    kubectl port-forward --namespace public-service svc/zookeeper 2181:2181 &
    zkCli.sh 127.0.0.1:2181
```

---

注意

由于我们是将 Chart 包下载至本地，更改 values.yaml 文件后才执行的安装，因此在使用`helm install -n public-service zookeeper .`命令时，不
要忘记命令最后有一个点，并且需要在 values.yaml 所在目录下执行。

另外,本次演示的内容没有进行数据持久化，生产环境可以使用 StorageClass 进行数据持久化，下一小节演示的 Kafka 同样如此。

---

查看 Pod 的状态：

```sh
$ kubectl get pod -n public-service
NAME          READY   STATUS    RESTARTS   AGE
zookeeper-0   1/1     Running   0          4m55s
zookeeper-1   1/1     Running   0          4m55s
zookeeper-2   1/1     Running   0          4m55s
```

查看创建的 Service：

```sh
$ kubectl get svc -n public-service
NAME                 TYPE        CLUSTER-IP        EXTERNAL-IP   PORT(S)                      AGE
zookeeper            ClusterIP   192.168.144.220   <none>        2181/TCP,2888/TCP,3888/TCP   5m11s
zookeeper-headless   ClusterIP   None              <none>        2181/TCP,2888/TCP,3888/TCP   5m11s
```

其中 zookeeper-headless 是 Zookeeper 实例内部通信使用的，Zookeeper 是外部程序调用的 Service。

之后可以通过 Zookeeper 的客户端工具查看 Zookeeper 集群的状态：

```sh
$ kubectl exec -it pod/zookeeper-0 -n public-service /opt/bitnami/zookeeper/bin/zkServer.sh status
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
/opt/bitnami/java/bin/java
ZooKeeper JMX enabled by default
Using config: /opt/bitnami/zookeeper/bin/../conf/zoo.cfg
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
```

其他节点可自行测试。

### 4.4 部署 Kafka

上一小节使用 Helm 安装 Zookeeper 集群至 Kubernetes，采用的是先下载后安装。

本小节演示不下载进行 Chart 的安装，相同的是，Kafka 的 Chart 也是使用 Bitnami 仓库中的 Chart。

新版的 Kafka 集群的创建已经可以不依赖 Zookeeper，但是目前生产上采用的并不都是全新的版本，所以本次演示的是之前版本的 Kafka。由于上一小
节已经安装了 Zookeeper 集群，因此本次安装依赖的 Zookeeper 可以使用上一小节安装的，通过一条命令即可安装 Kafka 集群：

```sh
$ helm repo add bitnami https://charts.bitnami.com/bitnami
## 部署3节点kafka
helm install kafka bitnami/kafka \
  --set zookeeper.enabled=false \
--set containerPorts.client=9092 \
--set service.ports.client=9092
  --set replicaCount=3 \
# 替换ZOOKEEPER-SERVICE-NAME为 zookeeper.default.svc
  --set externalZookeeper.servers=ZOOKEEPER-SERVICE-NAME
```

可以看到本次安装并没有下载对应的 Chart 包，也没有修改 values.yaml 的操作，而是使用--set 指定一些配置，--set 的功能相当于修改 values.yaml 中的配置，用来渲染模板。

比如--set persistence.enabled=false 就是和之前修改 values.yaml 的配置功能一样，需要注意--set 的优先级高于 values.yaml 中的配置，
也就是说如果是下载下来的 Chart 包，然后修改了 values.yaml 的内容，但是依旧可以使用--set 覆盖里面的配置。

### 4.5 Kafka 集群扩容

由于 Kafka 是直接通过--set 安装的，因此更新时需要找到之前的命令进行更新，如果忘记可以通过 helm get values 查看：

```sh
$ helm upgrade kafka bitnami/kafka \
  --set zookeeper.enabled=false \
  --set replicaCount=7 \
  --set externalZookeeper.servers=ZOOKEEPER-SERVICE-NAME
```

### 4.6 删除 Helm Release

如果前面创建的中间件不再使用，可以使用 Helm delete 删除，删除 Zookeeper 和命名空间：

```shell
$ helm delete kafka -n public-service
release "kafka" uninstalled

#之后删除Zookeeper和命名空间：
$ helm delete Zookeeper -n public-service
release "zookeeper" uninstalled

$ kubectl delete ns public-service
namespace "public-service" deleted
```

### 4.7 部署 Kubenetes-Dashboard

#### 1. 目标

1. 使用 Helm3 快速完成 Dashboard 部署
2. 调整 session 的默认失效时间
3. 本次部署的 dashboard 版本 2.0.4

#### 2. Helm 部署

**chart 地址**

> https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard/2.8.1

**参数示例**

```yaml
extraArgs:
  - --token-ttl=0
  - --system-banner="Welcome to 3IN Kubernetes Cluster. 操作需谨慎！！！"
ingress:
  enabled: true
  annotations:
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
  paths:
    - /
  hosts:
    - kube-dashboard.3incloud.cn
  tls:
    - secretName: tls-3incloudcn
      hosts:
        - kube-dashboard.3incloud.cn
metricsScraper:
  enabled: true
```

> \- token-ttl=0 配置用来表示登陆后 session 不失效

**安装**

```sh
# add repo
$ helm repo add k8s-dashboard https://kubernetes.github.io/dashboard/

# install
$ helm install kube-dashboard k8s-dashboard/kubernetes-dashboard -n kube-dashboard -f value.yaml
```

**错误修正**

修改 Deployment kube-dashboard-metrics-server 镜像为`registry.cn-hangzhou.aliyuncs.com/vcors/metrics-server-amd64:v0.3.6`，gcr 仓库镜像国内访问不到。

#### 3. 创建账户

创建账户

```shell
$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test
  namespace: kube-dashboard
EOF
```

绑定角色

```shell
$ cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: test
  namespace: kube-dashboard
EOF
```

获取 Token

```shell
$ kubectl -n kube-dashboard describe secret $(kubectl -n monitor get secret | grep wuyuexin | awk '{print $1}')
```

大功告成

!!! example "参考文献"

    https://github.com/kubernetes/dashboard

### 4.8 Helm 部署 DevOps 工具

!!! example "参考文献"

    > https://www.yuque.com/wuyuexin/bu3d95/aqrvn9
    >
    > **在Kubernetes上使用Jenkins**
    >
    > https://www.yuque.com/wuyuexin/bu3d95/eikxqa

## 5. Helm V3 入门到放弃

https://blog.51cto.com/devwanghui/2523543

Helm 安装各种服务-持续更新（nfs-client-provisioner、ingress-nginx、Jumpserver、Harbor、Jenkins、Gitlab、PrometheusAlert、Kube-Promethues）

https://www.putianhui.cn/posts/60dba0725b3e/

## 6. Helm 更多使用

!!! info "参考文献"

    [Helm Chart语法概要](https://www.cnblogs.com/ssgeek/p/15511387.html)


    [helm仓库详解](https://zhangzhuo.ltd/articles/2022/02/12/1644647638198.html)


    https://docs.youdianzhishi.com/k8s/helm/overview/


    [Helm Charts 开发完整示例](https://zhuanlan.zhihu.com/p/483177900)


    [Helm3 Chart多依赖微服务构建案例](https://blog.csdn.net/qq_39680564/article/details/107516510)


    [Kubernetes 集群管理-Helm部署及使用](https://www.cnblogs.com/yanjieli/p/13176133.html)


    [helm 源码剖析](https://hustclf.github.io/posts/413a7d5a/)

## 6. 小结

本章演示了 Operator 和 Helm 的简单使用，可以看到，基本上常用的一些 服务都能找到对应的 Operator 或者 Helm 进行部署，一些常用的开源项目一般 不需要我们自行编写 Operator 控制器或者 Helm 的包， 只需要在官方网站或者 GitHub 找到相关资料即可。

通过上述演示也能看出来，无论是什么中间件， 部署步骤都是一致的，基本没有太大的区别。

Operator 就是创建 CRD 和控制 器，然后创建自定义资源，Helm 就是找到对应的包，然后通过 install 安装。

将一些中间件服务部署到 Kubernetes 中，可以很大程度地降低应用部署 的复杂度，同时可以提升部署的效率。如果企业内部有完善的存储平台供 Kubernetes 使用，基本上可以将任何中间件部署至 Kubernetes 集群，也就是 实现“一切皆容器”的思想。

通常情况下，在 Kubernetes 集群外部会有一个 Ceph 这样的分布式存储平台供 Kubernetes 使用，在 Kubernetes 上部署对应的 CSI 进行存储编排，之后即可非常方便地实现中间件数据的持久化，这也是常用的一种方式和架构。
