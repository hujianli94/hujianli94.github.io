# Kubernetes调度基础


对于Kubernetes的应用部署，我们之前提到过Pod就是用来配置容器的，容器就是用来运行应用的。

也提到过，在实际使用时，不会创建单独的Pod运行应用，而是使用更高级的资源进行调度，比如Deployment等。

本章的内容就是讲解Kubernetes原生的调度资源， 在讲解高级资源Deployment、StatefulSet之前，首先会介绍什么是 Replication Controller和ReplicaSet。






## 1.Replication Controller和ReplicaSet

Replication Controller（复制控制器，RC）和ReplicaSet（复制集，RS）是两种简单部署Pod的方式。

因为在生产环境中主要使用更高级的Deployment等方式进行Pod的管理和部署，所以本节只对Replication Controller和Replica Set的部署方式进行简单介绍。





### 1.1 Replication Controller

Replication Controller可以确保Pod副本数达到期望值，也就是RC定义的数量。换句话说，Replication Controller可以确保一个Pod或一组同类Pod总是可用的。如果存在的Pod大于设定的值，则Replication Controller将终止额外的Pod。

如果太小，Replication Controller将启动更多的Pod用于保证达到期望值。与手动创建Pod不同的是，用Replication Controller维护的Pod在失败、删除或终止时会自动替换。

因此，即使应用程序只需要一个Pod，也应该使用Replication Controller或其他方式管理。Replication Controller类似于进程管理程序，但是Replication Controller不是监视单个节点上的各个进程，而是监视多个节点上的多个Pod。



定义一个Replication Controller的示例如下：

```yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx
spec:
  replicas: 3
  selector:
    app: nginx
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
```



### 1.2 ReplicaSet

ReplicaSet是支持基于集合的标签选择器的下一代Replication Controller，**==它主要用作Deployment协调创建、删除和更新Pod，和Replication Controller唯一的区别是，ReplicaSet支持标签选择器。==**

**==在实际应用中，虽然ReplicaSet可以单独使用，但是一般建议使用Deployment来自动管理ReplicaSet，除非自定义的Pod不需要更新或有其他编排等。==**





定义一个ReplicaSet的示例如下：

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
    matchExpressions:
      - {key: tier, operator: In, values: [frontend]}
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below.
          # value: env
        ports:
        - containerPort: 80
```

Replication Controller和ReplicaSet的创建、删除和Pod并无太大区别，

**Replication Controller目前几乎已经不在生产环境中使用，ReplicaSet也很少单独被使用，都是使用更高级的资源Deployment、DaemonSet、StatefulSet管理Pod。**




## 2.无状态应用管理Deployment

前文提到的ReplicaSet可以确保在任何给定时间运行的Pod副本达到指定的数量，但是Deployment是一个更高级的概念，它管理ReplicaSet并为Pod和ReplicaSet提供声明性更新以及许多其他有用的功能，
所以建议在生产环境下使用Deployment代替ReplicaSet。



Deployment一般用于部署公司的无状态服务，这个也是最常用的控制器，因为企业内部现在都是以微服务为主，而微服务实现无状态化也是最佳实践，
可以利用Deployment的高级功能做到无缝迁移、自动扩容缩容、自动灾难恢复、一键回滚等功能。



### 2.1 创建Deployment

创建一个Deployment也很简单，将前面章节创建Pod的YAML文件稍微更改一下即可变成Deployment的资源文件，比如创建一个有3个Nginx副本(实例)的Deployment，可以看到只是apiVersion和Kind有所变化，如下所示：

dp-nginx.yml

```yaml
# 从Kubernetes 1.16版本开始，彻底废弃了其他的APIVersion，只能使用apps/v1，1.16以上的版本可以使用extension等
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.7.9
          ports:
            - containerPort: 80

```

示例解析：

- nginx-deployment：Deployment的名称。
- replicas：创建Pod的副本数。
- selector：定义Deployment如何找到要管理的Pod，与template的

label（标签）对应，apiVersion为apps/v1必须指定该字段。
- template字段包含以下字段：
  
  ◆ app: nginx使用label（标签）标记Pod。
  
  ◆ spec：表示Pod运行一个名字为nginx的容器。
  
  ◆ image：运行此Pod使用的镜像。
  
  ◆ Port：容器用于发送和接收流量的端口。



使用kubectl create创建此Deployment：

```sh
$ kubectl apply -f  dp-nginx.yml
deployment.apps/nginx-deployment created
```

使用kubectl get或者kubectl describe查看此Deployment的状态：

```sh
$ kubectl get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           2m45s
```

可以使用rollout命令查看整个Deployment创建的状态：

```sh
$ kubectl rollout status deployment/nginx-deployment
deployment "nginx-deployment" successfully rolled out
```

当rollout结束时，再次查看此Deployment，可以看到AVAILABLE的数量和YAML文件中定义的replicas相同：

```sh
$ kubectl get deployment
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           2m45s
```

之前讲过Deployment通过ReplicaSet管理Pod，可以查看此Deployment当前对应的ReplicaSet：

```sh
$ kubectl get rs -l app=nginx
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-5d59d67564   3         3         3       4m44s


$ kubectl get pod -l app=nginx
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-5d59d67564-qns4q   1/1     Running   0          6m2s
nginx-deployment-5d59d67564-vqqb9   1/1     Running   0          6m2s
nginx-deployment-5d59d67564-xsssg   1/1     Running   0          6m2s
```

!!! warning 注意
  
    ReplicaSet的命名格式为[DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]POD-TEMPLATE-HASH-VALUE，是自动生成的，不用手动指定。


如果Deployment有过更新，对应的ReplicaSet可能不止一个，可以通过`-o yaml`获取当前对应的ReplicaSet是哪个，其余的ReplicaSet为保留的历史版本，用于回滚等操作。



查看此Deployment创建的Pod，可以看到Pod的hash值5d59d67564和上述Deployment对应的ReplicaSet的hash值一致：

```shell
$ kubectl get pods --show-labels
NAME                                READY   STATUS    RESTARTS   AGE   LABELS
nginx-deployment-5d59d67564-qns4q   1/1     Running   0          14m   app=nginx,pod-template-hash=5d59d67564
nginx-deployment-5d59d67564-vqqb9   1/1     Running   0          14m   app=nginx,pod-template-hash=5d59d67564
nginx-deployment-5d59d67564-xsssg   1/1     Running   0          14m   app=nginx,pod-template-hash=5d59d67564
```



!!! info "建议生产环境为资源添加limit和liveness"


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: test
  name: grafana-alert-deploy
  labels:
    name: grafana-alert-deploy
spec:
  replicas: 2
  selector:
    matchLabels:
      name: grafanaAlert
  template:
    metadata:
      labels:
        name: grafanaAlert
    spec:
      containers:
      - name: grafana-alert
        image: grafana_alert:cm_v2
        imagePullPolicy: IfNotPresent
        command: ["python3.8","-u","-m","flask","run","-h","0.0.0.0","-p","9999"]
        ports:
        - containerPort: 9999
          protocol: TCP
        volumeMounts:
        - name: grafana-alert-log
          mountPath: /opt/grafanaAlert/log
        readinessProbe:
          tcpSocket:
            port: 9999
        livenessProbe:
          tcpSocket:
            port: 9999
        resources:
          limits:
            cpu: 1
            memory: 100Mi
          requests:
            cpu: 100m
            memory: 10Mi
      volumes:
      - name: grafana-alert-log
        hostPath:
          path: /var/log/grafana-alert
          type: Directory
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: role
                    operator: In
                    values:
                      - removable
      dnsPolicy: ClusterFirst
      restartPolicy: Always
```


还可以使用 `$ kubectl explain deployment` 命令详细查看Deployment控制器中资源支持的所有字段的详细说明。


如果想了解一个正在运行的Pod的配置，可以通过以下命令获取。
```sh
$ kubectl get deployment {deployment名称} -o yaml
示例
$ kubectl get deployment exampledeployment -o yaml
```


创建deploymen的几种方式


1. 直接使用命令方式：

`--record` 参数用来记录版本，也可以忽略，建议带上

```sh
kubectl create deploy  my-dep3 --image=centos-nginx:1.23.0  --record 
```
 

2. 使用yaml文件方式

使用参数`--dry-run`空运行，导出yaml文件，可对yaml文件增减参数修改后发布

```sh
kubectl create deploy  my-dep3 --image=centos-nginx:1.23.0 --dry-run -o yaml > my-dep3.yaml
```

创建deployment

```sh
kubectl apply -f my-dep3.yaml --record 
```





### 2.2 更新Deployment

通过Deployment部署应用后，经常会有Deployment文件的配置更改或者镜像版本迭代的需求，更改配置后该Deployment会创建新的ReplicaSet，之后会对管理的Pod进行滚动升级。


!!! warning 注意

    当且仅当Deployment的Pod模板（即.spec.template）更改时，才会触发Deployment更新，例如更改内存、CPU配置或者容器的image。


假如更新Nginx Pod的image使用nginx:1.9.1，并使用--record记录当前更改的参数，后期回滚时可以查看到对应的信息：

```shell
$ kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record
deployment.extensions/nginx-deployment image updated
```

当然，也可以使用edit命令直接编辑Deployment，效果相同：

```shell
$ kubectl edit deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment edited
```

同样，可以使用kubectl rollout status查看更新过程：

```sh
$ kubectl rollout status deployment/nginx-deployment
Waiting for deployment "nginx-deployment" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "nginx-deployment" rollout to finish: 1 old replicas are pending termination...
Waiting for deployment "nginx-deployment" rollout to finish: 1 old replicas are pending termination...
deployment "nginx-deployment" successfully rolled out
```

可以看出更新过程为新旧交替更新，首先新建一个Pod，当Pod状态为Running时，删除一个旧的Pod，同时创建一个新的Pod。

当触发一个更新后，会有新的ReplicaSet产生，旧的ReplicaSet会被保存，查看此时的ReplicaSet，可以从AGE或READY看出新旧ReplicaSet：

```sh
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-5d59d67564   0         0         0       19m
nginx-deployment-69c44dfb78   3         3         3       3m25s
```

通过describe查看Deployment的详细信息

```sh
$ kubectl describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Wed, 14 Sep 2022 16:35:56 +0800
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 2
                        kubernetes.io/change-cause: kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
....
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  24m    deployment-controller  Scaled up replica set nginx-deployment-5d59d67564 to 3
  Normal  ScalingReplicaSet  8m22s  deployment-controller  Scaled up replica set nginx-deployment-69c44dfb78 to 1
  Normal  ScalingReplicaSet  7m47s  deployment-controller  Scaled down replica set nginx-deployment-5d59d67564 to 2
  Normal  ScalingReplicaSet  7m46s  deployment-controller  Scaled up replica set nginx-deployment-69c44dfb78 to 2
  Normal  ScalingReplicaSet  7m14s  deployment-controller  Scaled down replica set nginx-deployment-5d59d67564 to 1
  Normal  ScalingReplicaSet  7m14s  deployment-controller  Scaled up replica set nginx-deployment-69c44dfb78 to 3
  Normal  ScalingReplicaSet  6m49s  deployment-controller  Scaled down replica set nginx-deployment-5d59d67564 to 0
```

在describe中可以看出，第一次创建时，它创建了一个名为nginx-deployment-5d59d67564，并直接将其扩展为3个副本。
更新部署时 ，它创建了一个新的ReplicaSet，命名为nginx-deployment-6987cdb55b，并将其副本数扩展为1，然后将旧的ReplicaSet缩小为2，这样至少可以有两个Pod可用，最多创建了4个Pod。


以此类推，使用相同的滚动更新策略向上和向下扩展新旧ReplicaSet，最终新的ReplicaSet可以拥有3个副本，并将旧的ReplicaSet缩小为0。




更新部署时，它创建了一个新的ReplicaSet，命名为nginx-deployment-69c44dfb78，并将其副本数扩展为1，然后将旧的ReplicaSet缩小为2，这样至少可以有两个Pod可用，最多创建了4个Pod。

以此类推，使用相同的滚动更新策略向上和向下扩展新旧ReplicaSet，最终新的ReplicaSet可以拥有3个副本，并将旧的ReplicaSet缩小为0。





### 2.3 回滚Deployment

当更新的版本不稳定或配置不合理时，可以对其进行回滚操作，假设我们又进行了几次更新(此处以更新镜像版本触发更新，更改配置效果类)：

```shell
$ kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v1 --record
$ kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v2 --record
```

使用kubectl rollout history查看更新历史：

```shell
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record=true
3         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v1 --record=true
4         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v2 --record=true
```

查看Deployment某次更新的详细信息，使用--revision指定某次更新的版本号：

```sh
$ kubectl rollout history deployment/nginx-deployment --revision=3
deployment.apps/nginx-deployment with revision #3
Pod Template:
  Labels:       app=nginx
        pod-template-hash=666f99dd56
  Annotations:  kubernetes.io/change-cause: kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v1 --record=true
  Containers:
   nginx:
    Image:      dotbalo/canary:v1
    Port:       80/TCP
    Host Port:  0/TCP
    Environment:        <none>
    Mounts:     <none>
  Volumes:      <none>
```

如果只需要回滚到上一个稳定版本，使用kubectl rollout undo即可：

```sh
$ kubectl rollout undo deployment/nginx-deployment
deployment.apps/nginx-deployment rolled back

# 再次查看更新历史，发现REVISION5回到了canary:v1：
$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record=true
4         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v2 --record=true
5         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v1 --record=true
```

如果要回滚到指定版本，使用--to-revision参数：

```sh
$ kubectl rollout undo deployment/nginx-deployment --to-revision=2
deployment.extensions/nginx-deployment

$ kubectl rollout history deployment/nginx-deployment
deployment.apps/nginx-deployment
REVISION  CHANGE-CAUSE
1         <none>
4         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v2 --record=true
5         kubectl set image deployment nginx-deployment nginx=dotbalo/canary:v1 --record=true
6         kubectl set image deployment nginx-deployment nginx=nginx:1.9.1 --record=true
```



### 2.4 扩容Deployment

当公司访问量变大，或者有预期内的活动时，3个Pod可能已无法支撑业务时，可以提前对其进行扩展。

使用kubectl scale动态调整Pod的副本数，比如增加Pod为5个：

```sh
$ kubectl scale deployment.v1.apps/nginx-deployment --replicas=5
deployment.apps/nginx-deployment scaled

$ kubectl rollout status deployment/nginx-deployment
Waiting for deployment "nginx-deployment" rollout to finish: 3 of 5 updated replicas are available...
Waiting for deployment "nginx-deployment" rollout to finish: 4 of 5 updated replicas are available...
deployment "nginx-deployment" successfully rolled out
```

查看Pod，此时Pod已经变成了5个：

```sh
$ kubectl get pod
NAME                                READY   STATUS    RESTARTS   AGE
nginx-deployment-69c44dfb78-cqdd7   1/1     Running   0          50s
nginx-deployment-69c44dfb78-lnx2s   1/1     Running   0          2m44s
nginx-deployment-69c44dfb78-qz6gw   1/1     Running   0          2m40s
nginx-deployment-69c44dfb78-tltjh   1/1     Running   0          2m42s
nginx-deployment-69c44dfb78-xgwk6   1/1     Running   0          50s
```



### 2.5 暂停和恢复Deployment更新

上述演示的均为更改某一处的配置，更改后立即触发更新，大多数情况下可能需要针对一个资源文件更改多处地方，而并不需要多次触发更新，
此时可以使用Deployment暂停功能，临时禁用更新操作，对Deployment进行多次修改后再进行更新。



使用kubectl rollout pause命令即可暂停Deployment更新：

```sh
$ kubectl rollout pause deployment/nginx-deployment
deployment.apps/nginx-deployment paused
```

然后对Deployment进行相关更新操作，比如先更新镜像，然后对其资源进行限制（如果使用的是kubectl edit命令，可以直接进行多次修改，无须暂停更新，kubectl set命令一般会集成在CICD流水线中）：

```sh
$ kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1

$ kubectl set resources deployment.v1.apps/nginx-deployment -c=nginx --limits=cpu=200m,memory=512Mi
deployment.apps/nginx-deployment resource requirements updated
```

进行完最后一处配置更改后，使用kubectl rollout resume恢复Deployment更新：

```sh
$ kubectl rollout resume deployment.v1.apps/nginx-deployment
deployment.apps/nginx-deployment resumed
```

可以查看到恢复更新的Deployment创建了一个新的ReplicaSet：

```sh
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-5d59d67564   0         0         0       73m
nginx-deployment-666f99dd56   0         0         0       36m
nginx-deployment-69c44dfb78   0         0         0       58m
nginx-deployment-79d4d8c7d5   0         0         0       35m
nginx-deployment-f7fdb4ccd    5         5         4       113s
```

可以查看到Deployment的image（镜像）已经变为nginx:1.9.1：

```sh
$ kubectl describe deploy nginx-deployment
Name:                   nginx-deployment
Namespace:              default
......
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:      nginx:1.9.1
```



### 2.6 更新Deployment的注意事项

历史版本清理策略：

在默认情况下，revision保留10个旧的ReplicaSet，其余的将在后台进行垃圾回收，可以在.spec.revisionHistoryLimit设置保留ReplicaSet的个数。当设置为0时，不保留历史记录。

更新策略：

- .spec.strategy.type==Recreate：表示重建，先删掉旧的Pod，再创建新的Pod。
- .spec.strategy.type==RollingUpdate：表示滚动更新，可以指定maxUnavailable和maxSurge来控制滚动更新过程。
  


    - .spec.strategy.rollingUpdate.maxUnavailable：指定在回滚更新时最大不可用的Pod数量，可选字段，默认为25%，可以设置为数字或百分比，如果maxSurge为0，则该值不能为0。
    
    - .spec.strategy.rollingUpdate.maxSurge：可以超过期望值的最大Pod数，可选字段，默认为25%，可以设置成数字或百分比，如果maxUnavailable为0，则该值不能为0。



Ready策略：

- .spec.minReadySeconds是可选参数，指定新创建的Pod应该在没有任何容器崩溃的情况下视为Ready（就绪）状态的最小秒数，默认为0，即一旦被创建就视为可用，通常和容器探针连用。






[Deployment控制器的伸缩](https://my-docker-k8s-blog.readthedocs.io/en/latest/05.Kubernetes%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/5.%E6%8E%A7%E5%88%B6%E5%99%A8/1.Deployment%E6%8E%A7%E5%88%B6%E5%99%A8.html#id3)


[Deployment控制器的更新](https://my-docker-k8s-blog.readthedocs.io/en/latest/05.Kubernetes%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/5.%E6%8E%A7%E5%88%B6%E5%99%A8/1.Deployment%E6%8E%A7%E5%88%B6%E5%99%A8.html#id3)







## 3.有状态应用管理StatefulSet



StatefulSet（有状态集，缩写为sts）常用于部署有状态的且需要有序启动的应用程序，比如在进行Spring Cloud项目容器化时，
Eureka的部署是比较适合用StatefulSet部署方式的，可以给每个Eureka实例创建一个唯一且固定的标识符，并且每个Eureka实例无须配置多余的Service，
其余Spring Boot应用可以直接通过Eureka的Headless Service进行注册。





### 3.1 StatefulSet的基本概念

StatefulSet主要用于管理有状态应用程序的工作负载API对象。

比如在生产环境中，可以部署ElasticSearch集群、MongoDB集群或者需要持久化的RabbitMQ集群、Redis集群、Kafka集群和ZooKeeper集群等。



和Deployment类似，一个StatefulSet也同样管理着基于相同容器规范的Pod。不同的是，StatefulSet为每个Pod维护了一个黏性标识。

这些Pod是根据相同的规范创建的，但是不可互换，每个Pod都有一个持久的标识符，在重新调度时也会保留，一般格式为StatefulSetName-Number。
比如定义一个名字是Redis-Sentinel的StatefulSet，指定创建3个Pod，那么创建出来的Pod名字就为Redis-Sentinel-0、Redis-Sentinel-1、Redis-Sentinel-2。
而StatefulSet创建的Pod一般使用Headless Service（无头服务）进行通信，和普通的Service的区别在于Headless Service没有ClusterIP，它使用Endpoint进行互相通信，Headless一般的格式为：


```sh
statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local
```



配置说明：

- serviceName为Headless Service的名字，创建StatefulSet时必须指定Headless Service名称。
- 0..N‒1为Pod所在的序号，从0开始到N‒1。
- statefulSetName为StatefulSet的名字。
- namespace为服务所在的命名空间。
- .cluster.local为Cluster Domain（集群域）。



假如公司某个项目需要在Kubernetes中部署一个主从模式的Redis，此时使用StatefulSet部署就极为合适，因为StatefulSet启动时，只有当前一个容器完全启动时，后一个容器才会被调度，并且每个容器的标识符是固定的，
那么就可以通过标识符来断定当前Pod的角色。

比如用一个名为redis-ms的StatefulSet部署主从架构的Redis，第一个容器启动时，它的标识符为redis-ms-0，并且Pod内主机名也为redis-ms-0，此时就可以根据主机名来判断，
当主机名为redis-ms-0的容器作为Redis的主节点，其余为从节点，那么Slave连接Master主机配置就可以使用不会更改的Master的Headless Service，此时Redis从节点（Slave）配置文件如下：

```
 port 6379
 slaveof redis-ms-0.redis-ms.public-service.svc.cluster.local 6379
 tcp-backlog 511
 timeout 0
 tcp-keepalive 0
 ...
```

其中redis-ms-0.redis-ms.public-service.svc.cluster.local是Redis Master的Headless Service，在同一命名空间下只需要写redis-ms-0.redis-ms即可，后面的public-service.svc.cluster.local可以省略。



### 3.2 StatefulSet的注意事项

一般StatefulSet用于有以下一个或者多个需求的应用程序：

- 需要稳定的独一无二的网络标识符。
- 需要持久化数据。
- 需要有序的、优雅的部署和扩展。
- 需要有序的自动滚动更新。



如果应用程序不需要任何稳定的标识符或者有序的部署、删除或者扩展，应该使用无状态的控制器部署应用程序，比如Deployment或者ReplicaSet。StatefulSet是Kubernetes 1.9版本之前的beta资源，在1.5版本之前的任何Kubernetes版本都没有。

Pod所用的存储必须由PersistentVolume Provisioner（持久化卷配置器）根据请求配置StorageClass，或者由管理员预先配置，当然也可以不配置存储。





为了确保数据安全，删除和缩放StatefulSet不会删除与StatefulSet关联的卷，可以手动选择性地删除PVC和PV。



StatefulSet目前使用Headless Service（无头服务）负责Pod的网络身份和通信，需要提前创建此服务。



### 3.3 定义一个StatefulSet资源文件

定义一个简单的StatefulSet的示例如下：

sts-web.yml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
          name: web
```



!!! warning 
  
      此示例没有添加存储配置，后面的章节会单独讲解存储相关的知识点。


其中：

- kind: Service定义了一个名字为Nginx的Headless Service，创建的Service格式为nginx-0.nginx.default.svc.cluster.local，其他的类似，因为没有指定Namespace（命名空间），所以默认部署在default。
- kind: StatefulSet定义了一个名字为web的StatefulSet，replicas表示部署Pod的副本数，本实例为2。



在StatefulSet中必须设置Pod选择器（.spec.selector）用来匹配其标签（.spec.template.metadata.labels）。

在1.8版本之前，如果未配置该字段（.spec.selector），将被设置为默认值，在1.8版本之后，如果未指定匹配Pod Selector，则会导致StatefulSet创建错误。



当StatefulSet控制器创建Pod时，它会添加一个标签statefulset.kubernetes.io/pod-name，该标签的值为Pod的名称，用于匹配Service。



### 3.4 创建StatefulSet


创建StatefulSet：

```sh
$ kubectl create -f sts-web.yml
service/nginx created
statefulset.apps/web created

$ kubectl get sts
NAME   READY   AGE
web    2/2     41s

$ kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   142d
nginx        ClusterIP   None         <none>        80/TCP    67s

$ kubectl get pod -l app=nginx
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          6m57s
web-1   1/1     Running   0          6m36s
```

此时使用StatefulSet部署了两个Pod，分别为web-0、web-1，同时也创建一个CLUSTER-IP为None的Headless Service，

在同一个命名空间内使用web-0.nginx和web-1.nginx即可访问这两个Pod，

跨命名空间可以使用web-0.nginx.default访问（跨命名空间访问资源的情况很少，应当尽量规避）。



向Headless Service对象请求服务

```sh
$ kubectl run cirros-$RANDOM --rm -it --image=cirros -- sh
If you don't see a command prompt, try pressing enter.
/ # / # curl web-0.nginx
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
......
</body>
</html>

/ # curl web-1.nginx
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
......
</body>
</html>
```



### 3.5 StatefulSet创建Pod的流程

StatefulSet管理的Pod部署和扩展规则如下：

- 对于具有N个副本的StatefulSet，将按顺序从0到N‒1开始创建Pod。
- 当删除Pod时，将按照N‒1到0的反顺序终止。
- 在缩放Pod之前，必须保证当前的Pod是Running（运行中）或者Ready（就绪）。
- 在终止Pod之前，它所有的继任者必须是完全关闭状态。



StatefulSet的pod.Spec.TerminationGracePeriodSeconds（终止Pod的等待时间）不应该指定为0，
设置为0对StatefulSet的Pod是极其不安全的做法，优雅地删除StatefulSet的Pod是非常有必要的，而且是安全的，因为它可以确保在Kubelet从APIServer删除之前，让Pod正常关闭。





当创建上面的Nginx实例时，Pod将按web-0、web-1、web-2的顺序部署3个Pod。

在web-0处于Running或者Ready之前，web-1不会被部署，相同的，web-2在web-1未处于Running和Ready之前也不会被部署。

如果在web-1处于Running和Ready状态时，web-0变成Failed（失败）状态，那么web-2将不会被启动，直到web-0恢复为Running和Ready状态。



如果用户将StatefulSet的replicas设置为1，那么web-2将首先被终止，在完全关闭并删除web-2之前，不会删除web-1。

如果web-2终止并且完全关闭后，web-0突然失败，那么在web-0未恢复成Running或者Ready时，web-1不会被删除。



### 3.6 StatefulSet扩容和缩容

和Deployment类似，可以通过更新replicas字段扩容/缩容StatefulSet，也可以使用kubectl scale、kubectl edit和kubectl patch来扩容/缩容一个StatefulSet。



**（1）扩容**

将上述创建的StatefulSet副本增加到5个（如果StatefulSet挂载的为静态PV，需要在扩容之前先创建PV）：


```sh
$ kubectl scale sts web --replicas=5
statefulset.apps/web scaled
```

查看扩容后Pod的状态：

```sh
$ kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
web-0             1/1     Running   0          34m
web-1             1/1     Running   0          34m
web-2             1/1     Running   0          2m37s
web-3             1/1     Running   0          2m12s
web-4             1/1     Running   0          109s
```


也可使用以下命令动态查看：

```sh
$ kubectl get pod -l app=nginx -w
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          34m
web-1   1/1     Running   0          33m
web-2   1/1     Running   0          2m12s
web-3   1/1     Running   0          107s
web-4   1/1     Running   0          84s
```



**（2）缩容**

首先打开另一个终端动态查看缩容的流程：

```sh
$ kubectl get pod -l app=nginx -w
```

在另一个终端将副本数改为3（此处演示的为patch命令，patch比edit和scale稍复杂一些）：

```sh
$ kubectl patch sts web -p '{"spec":{"replicas":3}}'
statefulset.apps/web patched
```

此时可以看到第一个终端显示web-4和web-3的Pod正在被有序地删除（或终止）：

```sh
$ kubectl get pod -l app=nginx -w
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          34m
web-1   1/1     Running   0          33m
web-2   1/1     Running   0          2m12s
web-3   1/1     Running   0          107s
web-4   1/1     Running   0          84s
web-4   1/1     Terminating   0          3m17s
web-4   0/1     Terminating   0          3m18s
web-4   0/1     Terminating   0          3m19s
web-4   0/1     Terminating   0          3m19s
web-3   1/1     Terminating   0          3m42s
web-3   0/1     Terminating   0          3m43s
web-3   0/1     Terminating   0          3m47s
web-3   0/1     Terminating   0          3m47s
```

### 3.7 StatefulSet更新策略


和Deployment控制器一样，StatefulSet控制器也可以实现动态伸缩，只需要修改配置模板中的replicas属性然后执行应用即可。

但与Deployment控制器不同的地方在于，Pod是有序伸缩的，就像创建StatefulSet控制器时依次创建Pod一样。

在扩容时，后续新增的Pod会从前往后依次创建，创建完成后才开始下一个 Pod 的创建；

在缩容时，会先从编号最大的 Pod开始，从后往前依次删除，完全删除后才开始下一个Pod的删除。


StatefulSet控制器有两种更新策略，可以在模板中通过.spec.updateStrategy属性进行设置。

- (1) OnDelete策略

OnDelete更新策略实现了传统（1.7版本之前）的行为，它也是默认的更新策略。
当我们选择这个更新策略并修改StatefulSet的.spec.template字段时，StatefulSet控制器不会自动更新Pod，必须手动删除Pod才能使控制器创建新的Pod。


- (2) RollingUpdate策略

RollingUpdate（滚动更新）策略会自动更新一个StatefulSet中所有的Pod，采用与序号索引相反的顺序进行滚动更新。



**StatefulSet 和 deployment 的滚动更新差异**

StatefulSet控制器和Deployment控制器的滚动更新，有一些细节上的差异。

- 因为StatefulSet控制器是有序的，所以它会从编号最大的Pod到最小的Pod依次更新，而且在更新前不会立即删除旧的Pod，而是 等新的Pod已完全创建完毕且处于Running状态时，才会替换并删除旧的Pod。

- StatefulSet控制器拥有独有的更新属性 `.spec.updateStrategy.rollingUpdate.partition`。这种方式类似于金丝雀部署，如果将partition设置为4，只有编号大于或等于4的Pod才会进行更新，编号小于partition的Pod将不会更新。如果已经更新的Pod通过验证，则再将partition改为0，更新其余Pod即可。



比如更改一个名称为web的StatefulSet使用RollingUpdate方式更新：

```sh
$ kubectl patch sts web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate"}}}'
statefulset.apps/web patched (no change)
```

查看更改后的StatefulSet：

```sh
$ kubectl get sts web -o yaml|grep -A 1 "updateStrategy"
  updateStrategy:
    rollingUpdate:
```

然后改变容器的镜像触发滚动更新（此处使用jsonPath的方式更改的资源配置，可以使用set或edit减少复杂度）：

```sh
$ kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"gcr.io/google_containers/nginx-slim:0.8"}]'
```

如上所述，StatefulSet中的Pod采用和序号相反的顺序更新。

在更新下一个Pod前，StatefulSet控制器会终止每一个Pod并等待它们变成Running和Ready状态。
在当前顺序变成Running和Ready状态之前，StatefulSet控制器不会更新下一个Pod，但它仍然会重建任何在更新过程中发生故障的Pod，使用它们当前的版本。
已经接收到请求的Pod将会被恢复为更新的版本，没有收到请求的Pod则会被恢复为之前的版本。





在更新过程中可以使用`kubectl rollout status sts/<name>`来查看滚动更新的状态：

```sh
$ kubectl rollout status sts/web
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 1 pods at revision web-56b5798f76...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
waiting for statefulset rolling update to complete 2 pods at revision web-56b5798f76...
Waiting for 1 pods to be ready...
Waiting for 1 pods to be ready...
statefulset rolling update complete 3 pods at revision web-56b5798f76...
```

查看更新后的镜像：

```sh
$ for p in 0 1 2; do kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
```



**（3）分段更新**

StatefulSet可以使用RollingUpdate更新策略的partition参数来分段更新一个StatefulSet。

分段更新将会使StatefulSet中其余的所有Pod（序号小于分区）保持当前版本，只更新序号大于等于分区的Pod，利用此特性可以简单实现金丝雀发布（灰度发布）或者分阶段推出新功能等。



!!! info "提示"


     金丝雀发布是指在黑与白之间能够平滑过渡的一种发布方式。



比如我们定义一个分区"partition":3，可以使用patch或edit直接对StatefulSet进行设置：

```shell
$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'
```

然后再次使用patch改变容器的镜像：

```shell
$ kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"registry.k8s.io/nginx-slim:0.7"}]'
statefulset.apps/web patched
```

删除 StatefulSet 中的 Pod 触发更新：

```shell
$ kubectl delete pod web-2
pod "web-2" deleted
```

此时，因为Pod web-2的序号小于分区3，所以Pod不会被更新，还是会使用以前的容器恢复Pod。

等待Pod变成Running和Ready。

```shell
$ kubectl get pod -l app=nginx -w
NAME      READY     STATUS              RESTARTS   AGE
web-0     1/1       Running             0          4m
web-1     1/1       Running             0          4m
web-2     0/1       ContainerCreating   0          11s
web-2     1/1       Running   0         18s
```

将分区改为2，此时会自动更新web-2（因为之前更改了更新策略），但是不会更新web-0和web-1：

```shell
$ kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":2}}}}'
```

按照上述方式可以实现分阶段更新，类似于灰度/金丝雀发布。查看最终的结果如下：

```shell
$ for p in 0 1 2; do kubectl get pod "web-$p" --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'; echo; done
registry.k8s.io/nginx-slim:0.8
registry.k8s.io/nginx-slim:0.8
registry.k8s.io/nginx-slim:0.8
```

!!! info "请注意"

    虽然更新策略是RollingUpdate，StatefulSet 还是会使用原始的容器恢复Pod。 这是因为Pod的序号比updateStrategy指定的partition更小。




### 3.8 删除StatefulSet

删除StatefulSet有两种方式，即级联删除和非级联删除。使用非级联方式删除StatefulSet时，StatefulSet的Pod不会被删除；

使用级联方式删除StatefulSet时，StatefulSet和它的Pod都会被删除。



**（1）非级联删除**

使用kubectl delete sts xxx删除StatefulSet时，只需提供--cascade=false参数，就会采用非级联删除，此时删除StatefulSet不会删除它的Pod：

```sh
$ kubectl delete statefulset web --cascade=false
warning: --cascade=false is deprecated (boolean value) and can be replaced with --cascade=orphan.
statefulset.apps "web" deleted


$ kubectl get sts
No resources found in default namespace.

$ kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
web-0             1/1     Running   0          10m
web-1             1/1     Running   0          10m
web-2             1/1     Running   0          7m28s
```

由于此时删除了StatefulSet，它管理的Pod变成了“孤儿”Pod，因此单独删除Pod时，该Pod不会被重建：

```sh
$ kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
web-0             1/1     Running   0          3m11s
web-1             1/1     Running   0          2m51s
web-2             1/1     Running   0          2m34s

$ kubectl delete po web-0
pod "web-0" deleted

$ kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
web-1             1/1     Running   0          3m53s
web-2             1/1     Running   0          3m36s
```

当再次创建此StatefulSet时，web-0会被重新创建，由于web-1已经存在，因此不会被再次创建，因为最初此StatefulSet的replicas是2，所以web-2会被删除，
代码如下（忽略AlreadyExists错误，因为只删除了StatefulSet并没删除Service，所以在此创建Service时会提示已经存在该Service）：

```shell
$ kubectl create -f sts-web.yml
statefulset.apps/web created
Error from server (AlreadyExists): error when creating "sts-web.yml": services "nginx" already exists

$  kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
web-0             1/1     Running   0          79s
web-1             1/1     Running   0          4m28s
```





**（2）级联删除**

省略--cascade=false参数即为级联删除：

```shell
$ kubectl delete sts web
statefulset.apps "web" deleted

$ kubectl get pod
```

也可以使用-f指定创建StatefulSet和Service的YAML文件，直接删除StatefulSet和Service（此文件将StatefulSet和Service写在了一起）：

```shell
$ kubectl delete -f sts-web.yml
service "nginx" deleted
Error from server (NotFound): error when deleting "sts-web.yml": statefulsets.apps "web" not found
```


!!! example "参考文献"


    > https://kubernetes.io/zh-cn/docs/tutorials/stateful-application/basic-stateful-set/




参考文献：

[StatefulSet控制器](https://my-docker-k8s-blog.readthedocs.io/en/latest/05.Kubernetes%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/5.%E6%8E%A7%E5%88%B6%E5%99%A8/4.StatefulSet%E6%8E%A7%E5%88%B6%E5%99%A8.html#service)



## 4.守护进程集DaemonSet

DaemonSet（守护进程集，缩写为ds）和守护进程类似，它在符合匹配条 件的节点上均部署一个Pod。

### 4.1 什么是DaemonSet

有时候我们需要在每个Kubernetes节点或符合条件的节点上都部署某个应用，那么就可以使用Kubernetes的DaemonSet调度Pod。
DaemonSet确保全部（或者某些符合条件）节点上运行一个Pod副本。

当有新节点加入集群时，也会为它们新增一个Pod，当节点从集群中移除时，这些Pod也会被回收，删除DaemonSet将会删除它创建的所有Pod。



使用DaemonSet的一些典型用法：

- 运行集群存储daemon（守护进程），例如在每个节点上运行Glusterd、Ceph等。
- 在每个节点上运行日志收集daemon，例如Fluentd、Logstash。
- 在每个节点上运行监控daemon，比如Prometheus Node Exporter、Collectd、Datadog代理、New Relic代理或Ganglia gmond。



### 4.2 定义一个DaemonSet

创建一个DaemonSet和Deployment类似，比如创建一个fluentd的DaemonSet：

fluentd-ds.yml

```yaml
apiVersion: apps/v1
kind: DaemonSet # kind为DaemonSet
metadata:
  name: fluentd-es-v2.0.4
  namespace: logging
  labels:
    k8s-app: fluentd-es
    version: v2.0.4
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-es
      version: v2.0.4
  template:
    metadata:
      labels:
        k8s-app: fluentd-es
        kubernetes.io/cluster-service: "true"
        version: v2.0.4
      # This annotation ensures that fluentd does not get evicted if the node
      # supports critical pod annotation based priority scheme.
      # Note that this does not guarantee admission on the nodes (#40573).
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      serviceAccountName: fluentd-es
      containers:
      - name: fluentd-es
        image: k8s.gcr.io/fluentd-elasticsearch:v2.0.4
        env:
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      nodeSelector:
        beta.kubernetes.io/fluentd-ds-ready: "true"
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-es-config-v0.1.4
```

**（1）必需字段**

和其他所有Kubernetes配置一样，DaemonSet需要apiVersion、kind和metadata字段，同时也需要一个.spec配置段。



**（2）Pod模板**

.spec唯一需要的字段是.spec.template。.spec.template是一个Pod模板，它与Pod具有相同的配置方式，但它不具有apiVersion和kind字段。除了Pod必需的字段外，在DaemonSet中的Pod模板必须指定合理的标签。

**==在DaemonSet中的Pod模板必须具有一个RestartPolicy，默认为Always。==**



**（3）Pod Selector**

.spec.selector字段表示Pod Selector，它与其他资源的.spec.selector的作用相同。

.spec.selector表示一个对象，它由如下两个字段组成：

- matchLabels：与ReplicationController的.spec.selector的作用相同，用于匹配符合条件的Pod。
- matchExpressions：允许构建更加复杂的Selector，可以通过指定key、value列表以及与key和value列表相关的操作符。



如果上述两个字段都指定，结果表示的是AND关系（逻辑与的关系）。

.spec.selector必须与.spec.template.metadata.labels相匹配。

如果没有指定（APIVersion为apps/v1必须指定该字段），默认是等价的，如果它们的配置不匹配，则会被API拒绝。



**（4）指定节点部署Pod**

如果指定了.spec.template.spec.nodeSelector，DaemonSet Controller将在与Node Selector(节点选择器)匹配的节点上创建Pod，比如部署在磁盘类型为ssd的节点上(需要提前给节点定义标签Label)：

```
      nodeSelector:
        disktype: ssd
```



> **提示**
> 
> Node Selector同样适用于其他Controller。



### 4.3 创建DaemonSet

在生产环境中，公司业务的应用程序一般无须使用DaemonSet部署，一般情况下只有像Fluentd（日志收集）、Ingress（集群服务入口）、Calico（集群网络组件）、
Node-Exporter（监控数据采集）等才需要使用DaemonSet部署到每个节点，本节只演示DaemonSet的使用。


比如创建一个Nginx Ingress（可以在对应的章节找到该文件）




```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      hostNetwork: true
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.22.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1

---
```


```shell
$ pwd
$ kubectl create -f nginx-ds.yaml
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole
created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-
binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-
clusterrole-nisa-bindi
ng created
daemonset.extensions/nginx-ingress-controller created
```
此时会在每个节点上创建一个Pod：
```shell
$ kubectl get pod -n ingress-nginx
NAME                                           READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-267hs                 1/1     Running   3          60d
ingress-nginx-controller-zlwmm                 1/1     Running   2          60d
.......
ingress-nginx-defaultbackend-b578d6c5b-svwsw   1/1     Running   19         177d


# 使用-o wide查看pod所在节点
$ kubectl get pod -n ingress-nginx -o wide
NAME                                           READY   STATUS    RESTARTS   AGE    IP             NODE           NOMINATED NODE   READINESS GATES
ingress-nginx-controller-267hs                 1/1     Running   3          60d    192.168.1.79   gitee-k8s-n2   <none>           <none>
ingress-nginx-controller-zlwmm                 1/1     Running   2          60d    192.168.1.32   gitee-k8s-n1   <none>           <none>
.......
ingress-nginx-defaultbackend-b578d6c5b-svwsw   1/1     Running   19         177d   10.0.35.131    gitee-k8s-n2   <none>           <none>
```
!!! warning "注意"

  
    注意
    因为笔者的Master节点删除了Taint（Taint和Toleration内容见本书9.4 节），所以也能部署Ingress或者其他Pod，在生产环境中，在Master节点除了系统组件外最好不要部署其他Pod。


### 4.4 更新和回滚DaemonSet

如果添加了新节点或修改了节点标签（Label），DaemonSet将立刻向新匹配上的节点添加Pod，同时删除不能匹配的节点上的Pod。



在Kubernetes 1.6以后的版本中，可以在DaemonSet上执行滚动更新，未来的Kubernetes版本将支持节点的可控更新。

DaemonSet滚动更新可参考：https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/。



DaemonSet更新策略和StatefulSet类似，也有OnDelete和RollingUpdate两种方式。


- RollingUpdate：当使用RollingUpdate方式时，在更新DaemonSet 控制器模板后，旧的DaemonSet Pod将被终止，并且将以受控方式自动创建新的DaemonSet Pod。

- OnDelete：这是向后兼容的默认更新方式。当使用OnDelete更新方式时，在更新DaemonSet控制器模板后，只有手动删除旧的DaemonSet控制器Pod后，才会创建新的DaemonSet控制器Pod。这与Kubernetes 1.5或更早版本中DaemonSet控制器的行为相同。



#### OnDelete

OnDelete更新方式。 为了修改之前的yml文件，将其从httpd 2.2版本升级到2.4版本， 首先，运行如下命令。

`exampleDaemonset.yml`

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: exampledaemonset
spec:
  selector:
    matchLabels:
      example: deploymentforhttpd
  template:
    metadata:
      labels:
        example: deploymentforhttpd
    spec:
      containers:
        - name: httpd
          image: httpd:2.4
          ports:
          - containerPort: 80
            hostPort: 8082
            protocol: TCP
  updateStrategy:
    type: OnDelete
```

接下来，运行以下命令，通过模板创建DaemonSet控制器。

```sh
$ kubectl get daemonset
NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
exampledaemonset   28        28        28      28           28          <none>          14s

$ kubectl apply -f exampleDaemonset.yml
daemonset.apps/exampledaemonset configured

$ kubectl get daemonset
NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
exampledaemonset   28        28        28      0            28          <none>          2m19s
```

此时通过 `$ kubectl get daemonset` 命令查看状态，可以发现 `UP-TO-DATE` 变为0，表示DaemonSet控制器现在都是旧版本，

接下来，执行 `$ kubectl get pods -o wide` 命令，现在所有的Pod都是旧版本

```sh
$ kubectl get pod  -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE            NOMINATED NODE   READINESS          GATES
exampledaemonset-4dghq   1/1     Running   0          99s   10.0.37.173   gitee-k8s-w09   <none>           <none>
exampledaemonset-4rcmq   1/1     Running   0          98s   10.0.18.230   gitee-k8s-w25   <none>           <none>
exampledaemonset-4s5cc   1/1     Running   0          99s   10.0.6.200    gitee-k8s-w27   <none>           <none>
exampledaemonset-6dlbh   1/1     Running   0          99s   10.0.32.85    gitee-k8s-w05   <none>           <none>
exampledaemonset-6r52w   1/1     Running   0          99s   10.0.25.143   gitee-k8s-w17   <none>           <none>
....
```

接下来，删除第一个Pod以触发更新，在本例中为exampledaemonset-4dghq，这个Pod所在的机器为gitee-k8s-w09。执行如下 命令。

```sh
$ kubectl delete pod exampledaemonset-4dghq
```

接下来，执行 `$ kubectl get pods -o wide` 命令，可以看到原来旧版本的 `exampledaemonset-4dghq` 已经被删除，取而代之的是一个新版本Pod。

最后，通过 `$ kubectl get daemonset` 命令查看状态，可以发现UP-TO-DATE变为1，这表示已经有1台机器上的DaemonSet控制器是 新版本了。

```sh
$ kubectl get daemonset
NAME               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
exampledaemonset   28        28        28      1            28          <none>          5m25s
```


#### RollingUpdate


对于RollingUpdate，与之前Deployment控制器不一样的地方在于，

DaemonSet控制器中的RollingUpdate只支持maxUnavailable属性。

因为DaemonSet控制器是在每个Node主机上启动的唯一Pod，不能启动多余的节点，所以无法使用maxSurge属性。

DaemonSet的更新和回滚与Deployment类似，此处不再演示。




#### 回滚版本

查看上一节创建的DaemonSet更新方式：
```shell
$ kubectl get ds nginx-ds -o go-template='{{.spec.updateStrategy.type}}{{"\n"}}'
RollingUpdate
```

命令式更新，和之前的Deployment、StatefulSet方式一致：
```shell
$ kubectl edit ds/<daemonset-name>
$ kubectl patch ds/<daemonset-name> -p=<strategic-merge-patch>
```

更新镜像：
```shell
$ kubectl set image ds/<daemonset-name><container-name>=<container-new-image> --record=true
```

查看更新状态:
```shell
$ kubectl rollout status ds/<daemonset-name>
```

列出所有修订版本：
```shell
$ kubectl rollout history daemonset <daemonset-name>
```

回滚到指定revision：
```shell
$ kubectl rollout undo daemonset <daemonset-name> --to-revision=<revision>
```



??? example "参考文献"


    https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/daemonset/

    https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/





## 5.CronJob



CronJob（计划任务，缩写为cj）用于以时间为基准周期性地执行任务，这些自动化任务和运行在Linux或UNIX系统上的CronJob一样。


CronJob对于创建定期和重复任务非常有用，例如执行备份任务、周期性调度程序接口、发送电子邮件等。



对于Kubernetes 1.8以前的版本，需要添加--runtime-config=batch/v2alpha1=true参数至APIServer中，然后重启APIServer和Controller Manager才可启用CronJob功能，

对于1.8以后的版本，无须修改任何参数，可以直接使用，本节的示例基于Kubernetes 1.8以上的版本。



### 5.1 创建CronJob

创建CronJob有两种方式，一种是直接使用kubectl创建，另一种是使用YAML文件创建。

使用kubectl创建CronJob的命令如下(新版本的run命令无法创建CronJob，需要直接使用YAML文件创建)：

```sh
$ kubectl create cronjob hello --image=busybox   --schedule="*/1 * * * *" --restart=OnFailure -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"
cronjob.batch/hello created
```

- hello：计划任务的名称。
- schedule：调度周期，分别为分、时、日、月、周，和Linux配置方式一致。
- restart：重启策略。·　image：执行计划任务的镜像，一般是包含依赖工具的镜像，比如curl、Xtrabackup。
- /bin/sh：计划任务的操作。

创建完成之后，可以使用kubectl get cj hello -o yaml查看创建的CronJob的详细信息，对应的YAML文件如下：

```yaml
$ kubectl get cj hello -o yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
  namespace: default
spec:
  concurrencyPolicy: Allow
  failedJobsHistoryLimit: 1
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: hello
    spec:
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
            image: busybox
            imagePullPolicy: Always
            name: hello
            resources: {}
            terminationMessagePath: /dev/termination-log
            terminationMessagePolicy: File
          dnsPolicy: ClusterFirst
          restartPolicy: OnFailure
          schedulerName: default-scheduler
          securityContext: {}
          terminationGracePeriodSeconds: 30
  schedule: '*/1 * * * *'
  successfulJobsHistoryLimit: 3
  suspend: false
status: {}
```

> **说明**
> 
> 本例创建一个每分钟执行一次、打印当前时间和Hello from the Kubernetes cluster的计划任务。



查看创建的CronJob：

```shell
$ kubectl get cj
NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   */1 * * * *   False     0        74s             106s
```



计划任务在执行时会启动一个名字为CRONJOB_NAME-xxxxx的Job去调度任务，等待1分钟可以查看执行的任务（Jobs）：

```shell
$ kubectl get jobs
NAME             COMPLETIONS   DURATION   AGE
hello-27720215   1/1           2s         98s
```

CronJob每次调用任务的时候会创建一个Job，Job会创建一个名为JOB_NAME-xxx的Pod执行命令，成功执行完任务后，Pod状态就会变成Completed，如下所示：

```shell
$ kubectl get pod
NAME                   READY   STATUS      RESTARTS   AGE
hello-27720215-zsk5n   0/1     Completed   0          2m22s
hello-27720216-frh4w   0/1     Completed   0          82s
k9s-shell-15624        1/1     Running     0          56d


```

此时可以通过logs查看Pod的执行日志：

```shell
$ kubectl logs pod/hello-27720215-zsk5n
Thu Sep 15 03:36:13 UTC 2022
Hello from the Kubernetes cluster
```



如果要删除CronJob，直接使用delete即可：

```shell
$ kubectl delete cronjob hello
cronjob.batch "hello" deleted
```



### 5.2 可用参数的配置



定义一个CronJob的YAML文件如下：

```yaml
apiVersion: batch/v1 # K8s小于1.21 batch/v1beta1
kind: CronJob
metadata:
labels:
  run: hello
name: hello
namespace: default
spec:
concurrencyPolicy: Allow
failedJobsHistoryLimit: 1
jobTemplate:
  metadata:
	creationTimestamp: null
  spec:
	template:
	  metadata:
		creationTimestamp: null
		labels:
		  run: hello
	  spec:
		containers:
		- args:
		  - /bin/sh
		  - -c
		  - date; echo Hello from the Kubernetes cluster
		  image: busybox
		  imagePullPolicy: Always
		  name: hello
		  resources: {}
		  terminationMessagePath: /dev/termination-log
		  terminationMessagePolicy: File
		dnsPolicy: ClusterFirst
		restartPolicy: OnFailure
		schedulerName: default-scheduler
		securityContext: {}
		terminationGracePeriodSeconds: 30
schedule: '*/1 * * * *'
successfulJobsHistoryLimit: 3
suspend: false
```

其中各参数的说明如下，可以按需修改：

- schedule：调度周期，和Linux一致，分别是分、时、日、月、周。
- restartPolicy：重启策略，和Pod一致。
- concurrencyPolicy：并发调度策略。可选参数如下：
  - Allow：允许同时运行多个任务。
  - Forbid：不允许并发运行，如果之前的任务尚未完成，新的任务不会被创建。
  - Replace：如果之前的任务尚未完成，新的任务会替换之前的任务。
    
  
- suspend：如果设置为true，则暂停后续的任务，默认为false。
- successfulJobsHistoryLimit：保留多少已完成的任务，按需配置。
- failedJobsHistoryLimit：保留多少失败的任务。



**相对于Linux上的计划任务，Kubernetes的CronJob更具有可配置性，并且对于执行计划任务的环境只需启动相对应的镜像即可。**

比如，如果需要Go或者PHP环境执行任务，就只需要更改任务的镜像为Go或者PHP即可，而对于Linux上的计划任务，则需要安装相对应的执行环境。此外，Kubernetes的CronJob是创建Pod来执行的，更加清晰明了，查看日志也比较方便。可见，Kubernetes的CronJob更加方便和简单。



!!! abstract "更多CronJob的内容可以参考Kubernetes的官方文档"

    https://kubernetes.io/docs/home/





## 6.小结

本章学习了Kubernetes开箱即用的调度资源，Deployment是实际使用时用得最多的一种类型。

Kubernetes原生的资源调度基本上可以满足大部分的需求，但是在实际使用时，可能还会有一些定制化的需求，比如计划任务需要在每台服务器都要执行等。

此类需求可以根据实际情况开发定制化的资源调度，类似的有阿里云开源的OpenKruise等，有兴趣的读者可以关注一下。