#  Kubeadm安装高可用集群

我们可以使用两种方式安装K8s集群：Kubeadm方式和二进制方式。

Kubeadm是一个简单易用的安装工具，可用于快速搭建Kubernetes集群，目前是比较方便和推荐的方式。

二进制安装方式调试Bug比较方便，有助于清楚了解K8s组件的原理，对于全面理解Kubernetes会有帮助。

本章首先介绍使用Kubeadm安装K8s集群的具体方式和步骤。本章的安装内容在Kubernetes 1.14以上版本均可使用，本书安装的版本是1.22（本书截稿前的最新版本）。

就Kubeadm各个版本的安装来看，基本上大同小异，安装步骤改动也不大，基本环境配置和基本组件安装类似，每个版本的区别可能只是集群初始化不同。

相信随着版本的迭代，安装步骤会变得更加简单、快捷。针对不同的版本，读者也可以参考官网的安装步骤：

[利用 kubeadm 创建高可用集群](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)



##  1. Haproxy+Keepalived搭建高可用 Kubernetes 集群（centos7）

###  1.1 基本环境配置

读者需自行准备

- 3台最低2核CPU和4GB以上内存的服务器，系统为CentOS 7.x,安装master节点
- 1台最低4核CPU和8GB以上内存的服务器，系统为CentOS 7.x,安装node节点

如果没有这么多的机器，可以只安装3台Master或者一台Master、1台Node。

集群采用的服务器IP规划如表1.1所示。

表1.1　高可用Kubernetes集群规划

| 主机名           | IP地址            | 说明            | 服务器配置                     |
| ---------------- | ----------------- | --------------- | ------------------------------ |
| k8s-master-01~03 | 192.168.1.190~192 | master节点*3    | 2c4g 系统盘：40G  数据盘：100G |
| VIP              | 192.168.1.193     | keeplived虚拟IP |                                |
| k8s-node01~n     | 192.168.1.200     | node节点*n      | 4c8g 100G                      |

Kubernetes一共涉及三个网段，一个是宿主机的网段，也就是上述的192.168.1.x，同时需要Pod和Service的网段，三者的网段不可交叉。

安装配置信息如表1.2所示。

| 配置信息    | 备注           |
| ----------- | -------------- |
| 系统版本    | Centos7.9      |
| Docker版本  | 20.10.x        |
| Pod网段     | 172.16.0.0/12  |
| Service网段 | 192.168.0.0/16 |

> 注意
>
> 宿主机网段、K8s Service网段、Pod网段不能重复。



所有节点配置主机名（其他节点名称可自行更改）：

```shell
# hostnamectl set-hostname k8s-master01 && eval -- bash
# hostnamectl set-hostname k8s-master02 && eval -- bash
# hostnamectl set-hostname k8s-master03 && eval -- bash
# hostnamectl set-hostname k8s-node01 && eval -- bash
```

所有节点配置hosts，修改/etc/hosts如下：

```shell
# cat >>/etc/hosts<<'EOF'
192.168.1.190 k8s-master01
192.168.1.191 k8s-master02
192.168.1.192 k8s-master03
192.168.1.200 k8s-node01
EOF
```

配置静态IP参考

```shell
# cat /etc/sysconfig/network-scripts/ifcfg-eth0
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="static"
IPADDR="192.168.1.200"
PREFIX="24"
GATEWAY="192.168.1.1"
DNS1="192.168.1.60"
DNS2="114.114.114.114"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="eth0"
DEVICE="eth0"
ONBOOT="yes"
```

所有节点配置Docker、Kubernetes和默认yum源：

```shell
# curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo

# yum install -y yum-utils \
  device-mapper-persistent-data \
  lvm2

# yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
    
# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo
```

所有节点安装一些常用的工具：

```shell
# yum install wget jq psmisc vim net-tools telnet git -y
```

所有节点关闭防火墙、SELinux、DNSmasq：

```shell
# systemctl disable --now firewalld
# systemctl disable --now dnsmasq
# systemctl disable --now NetworkManager

# setenforce 0
# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/sysconfig/selinux
# sed -i 's#SELINUX=enforcing#SELINUX=disabled#g' /etc/selinux/config
```

所有节点关闭Swap分区：

```shell
# swapoff -a && sysctl -w vm.swappiness=0
# sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab
```

所有节点安装ntpdate（如果公司的服务器已经配置了自动同步时间，关于时间的配置可以不用操作）：

```shell
# rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm
# yum install ntpdate -y
```

所有节点同步时间：

```shell
# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
# echo 'Asia/Shanghai' >/etc/timezone
# hwclock --systohc
# ntpdate time2.aliyun.com

# yum install chrony -y
# systemctl enable --now chronyd
# chronyc sources
210 Number of sources = 4
MS Name/IP address         Stratum Poll Reach LastRx Last sample
===============================================================================
^+ sv1.ggsrv.de                  2   6    17    32   -823us[-1128us] +/-   98ms
^- montreal.ca.logiplex.net      2   6    17    32    -17ms[  -17ms] +/-  179ms
^- ntp6.flashdance.cx            2   6    17    32    -32ms[  -32ms] +/-  161ms
^* 119.28.183.184                2   6    33    32   +661us[ +357us] +/-   38ms
# date
Tue Aug 31 14:36:14 CST 2021
```

所有节点配置limit：

```shell
# ulimit -SHn 65535
 
# cat >>/etc/security/limits.conf<<-'EOF'
# 末尾添加如下内容
* soft nofile 65536
* hard nofile 131072
* soft nproc 65535
* hard nproc 655350
* soft memlock unlimited
* hard memlock unlimited
EOF
```

安装过程中，生成配置文件和证书均在Master01上操作，所以Master01节点需要免密钥登录其他节点，之后将文件传送到其他节点。

集群管理也在Master01上操作（也可以是其他单独的节点）。

配置密钥（只在Master01或管理节点操作，以下步骤将管理节点和Master01统称为Master01）：

```shell
# ssh-keygen -t rsa
# for i in k8s-master01 k8s-master02 k8s-master03 k8s-node01;do ssh-copy-id -i .ssh/id_rsa.pub $i;done
```

Master01下载安装所有的源码文件：

```shell
# cd /root/ ; git clone https://github.com/dotbalo/k8s-ha-install.git

## github 拉取受网络限制，可以使用gitee仓库
# cd /root/; git clone https://gitee.com/hujianli94net/k8s-ha-install
```

所有节点升级系统：

```shell
# yum update -y
```



###  1.2 内核配置

为了集群的稳定性和兼容性，生产环境的内核最好升级到4.18版本以上，本示例将升级到4.19版本。

> 离线升级内核

Master01下载离线包：

```shell
# cd /root
# wget -c http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm
# wget -c http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm
```

将安装包从Master01节点传到其他节点：

```shell
# for i in k8s-master02 k8s-master03 k8s-node01;do scp kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm $i:/root/ ; done
```

所有节点安装内核：

```shell
# cd /root && yum localinstall -y kernel-ml*

## 所有节点更改内核启动顺序
# grub2-set-default  0 && grub2-mkconfig -o /etc/grub2.cfg

# grubby --args="user_namespace.enable=1" --update-kernel="$(grubby --default-kernel)"

# 检查默认内核是不是4.19
[root@k8s-master02 ~]# grubby --default-kernel
/boot/vmlinuz-4.19.12-1.el7.elrepo.x86_64

# 所有节点重启，然后检查内核是不是4.19

# reboot
# uname -a
Linux k8s-master02 4.19.12-1.el7.elrepo.x86_64 #1 SMP Fri Dec 21 11:06:36 EST 2018 x86_64 x86_64 x86_64 GNU/Linux
```



> 在线升级内核

```sh
# upgrade Linux kernel
# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
# yum install -y https://www.elrepo.org/elrepo-release-7.el7.elrepo.noarch.rpm

## 升级内核中途不要随意中断，防止升级失败
# yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y
# grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg
# reboot
```



所有节点安装ipvsadm和ipset：

```sh
# yum install ipvsadm ipset sysstat conntrack libseccomp -y
```

所有节点配置ipvs模块，在内核4.19+版本nf_conntrack_ipv4已经改为nf_conntrack，4.18以下版本使用nf_conntrack_ipv4即可：

```sh
# cat >> /etc/modules-load.d/ipvs.conf<<'EOF'
ip_vs
ip_vs_lc
ip_vs_wlc
ip_vs_rr
ip_vs_wrr
ip_vs_lblc
ip_vs_lblcr
ip_vs_dh
ip_vs_sh
ip_vs_fo
ip_vs_nq
ip_vs_sed
ip_vs_ftp
ip_vs_sh
nf_conntrack
ip_tables
ip_set
xt_set
ipt_set
ipt_rpfilter
ipt_REJECT
ipip
EOF
```

然后执行

```sh
# systemctl enable --now systemd-modules-load.service
```

即可。

开启一些K8s集群中必需的内核参数，所有节点配置K8s内核：

```shell
# cat <<EOF > /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
net.ipv4.conf.all.route_localnet = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963
net.netfilter.nf_conntrack_max=2310720

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_intvl =15
net.ipv4.tcp_max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_timestamps = 0
net.core.somaxconn = 16384
EOF
 
# sysctl --system
```

所有节点配置完内核后，重启服务器，保证重启后内核依旧加载：

```shell
# reboot
# lsmod | grep --color=auto -e ip_vs -e nf_conntrack
ip_vs_ftp              16384  0
nf_nat                 32768  1 ip_vs_ftp
ip_vs_sed              16384  0
ip_vs_nq               16384  0
ip_vs_fo               16384  0
ip_vs_sh               16384  0
ip_vs_dh               16384  0
ip_vs_lblcr            16384  0
ip_vs_lblc             16384  0
ip_vs_wrr              16384  0
ip_vs_rr               16384  0
ip_vs_wlc              16384  0
ip_vs_lc               16384  0
ip_vs                 151552  24 ip_vs_wlc,ip_vs_rr,ip_vs_dh,ip_vs_lblcr,ip_vs_sh,ip_vs_fo,ip_vs_nq,ip_vs_lblc,ip_vs_wrr,ip_vs_lc,ip_vs_sed,ip_vs_ftp
nf_conntrack          143360  2 nf_nat,ip_vs
nf_defrag_ipv6         20480  1 nf_conntrack
nf_defrag_ipv4         16384  1 nf_conntrack
libcrc32c              16384  3 nf_conntrack,nf_nat,ip_vs
```

###  1.3 K8s组件和Runtime安装

本节主要安装的是集群中用到的各种组件，比如docker-ce、containerd、Kubernetes组件等。

书中为读者准备了两种Runtime（运行时）的安装：

- Docker

- Containerd

> 注意
>
> 如果读者安装的Kubernetes版本高于1.24（含1.24），需要选择Containerd作为Runtime；如果版本低于1.24，Runtime选择Docker和Containerd均可。



docker和containerd的区别

https://docs.youdianzhishi.com/k8s/runtime/overview/





#### 1.3.1 Containerd作为Runtime

由于Kubernetes 1.24以上版本将不再直接支持Docker，因此需要将Kubernetes的Runtime改为Containerd。因为安装Docker时会自动安装Containerd，并且后面的课程也要使用到Docker，所以还是在每个节点安装Docker。

所有节点安装docker-ce-20.10：

```sh
# yum install docker-ce-20.10.* docker-ce-cli-20.10.* -y
```

由于并不是每个节点都需要Docker引擎，因此无须启动Docker，只需要配置和启动Containerd即可。

首先配置Containerd所需的模块（所有节点）：

```sh
# cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
```

所有节点加载模块：

```sh
# modprobe -- overlay
# modprobe -- br_netfilter
```

所有节点配置Containerd所需的内核：

```sh
# cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward               = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
```

所有节点加载内核：

```sh
# sysctl --system
```

所有节点配置Containerd的配置文件：

```sh
# mkdir -p /etc/containerd
# containerd config default | tee /etc/containerd/config.toml
```

所有节点将Containerd的Cgroup改为Systemd：

```sh
# vim /etc/containerd/config.toml
找到containerd.runtimes.runc.options，添加SystemdCgroup = true
所有节点将sandbox_image的Pause镜像改成符合自己版本的地址:registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6
```

所有节点启动Containerd，并配置开机自启动：

```sh
# systemctl daemon-reload
# systemctl enable --now containerd
```

所有节点配置crictl客户端连接的Runtime位置：

```sh
# cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
```

#### 1.3.2 Docker作为Runtime

如果读者选择Docker作为Runtime，安装步骤较Containerd更为简单，只需要安装并启动即可。

所有节点安装docker-ce 20.10：

```sh
# yum install docker-ce-20.10.* docker-ce-cli-20.10.* -y
```

由于新版Kubelet建议使用systemd，因此把Docker的CgroupDriver也改成systemd：

```sh
# mkdir -p /etc/docker  # 如果没有这个目录先创建，然后添加 daemon.json 文件

# vi /etc/docker/daemon.json
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "registry-mirrors" : [
    "https://ot2k4d59.mirror.aliyuncs.com/"
  ]
}
```

所有节点设置开机自启动Docker：

```sh
# systemctl daemon-reload && systemctl enable --now docker
```

#### 1.3.3 安装Kubernetes组件

接下来安装Kubernetes的系统组件。

所有节点执行

```sh
# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
        http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```



首先在Master01节点查看最新的Kubernetes版本是多少：

```sh
# yum list kubeadm.x86_64 --showduplicates | sort -r
```

假如读者看到的1.23.*，直接安装1.23版即可，如果是其他版本，则直接安装最新版。

如果是生产环境，建议安装第三个版本号大于5的版本，比如1.23.5。

如果没有1.23.5，只有1.23.3等，可以考虑使用1.22.5+等。当然，如果想要使用最新的功能，安装1.23.3也可以。

```sh
## # --disableexcludes 禁掉除了kubernetes之外的别的仓库
# yum install kubeadm-1.23.5 kubelet-1.23.5 kubectl-1.23.5 -y --disableexcludes=kubernetes
```

当然，如果想要使用最新的功能，安装1.23.3也可以。

所有节点安装1.25最新版本kubeadm、kubelet和kubectl：

```sh
# yum install kubeadm-1.25* kubelet-1.25* kubectl-1.25* -y --disableexcludes=kubernetes
```

更改Kubelet的配置使用Containerd作为Runtime：

```sh
# cat >/etc/sysconfig/kubelet<<EOF
KUBELET_KUBEADM_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock"
EOF
```

> 注意 如果读者不是采用Containerd作为Runtime，不需要执行上述命令。

所有节点设置Kubelet开机自启动（由于还未初始化，没有Kubelet的配置文件，此时Kubelet无法启动，无须管理）：

```sh
# systemctl daemon-reload
# systemctl enable --now kubelet
```

查看版本信息

```sh
# kubelet --version
Kubernetes v1.25.6

# kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.6", GitCommit:"ff2c119726cc1f8926fb0585c74b25921e866a28", GitTreeState:"clean", BuildDate:"2023-01-18T19:22:09Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
The connection to the server localhost:8080 was refused - did you specify the right host or port?

# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.6", GitCommit:"ff2c119726cc1f8926fb0585c74b25921e866a28", GitTreeState:"clean", BuildDate:"2023-01-18T19:20:37Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
```



###  1.4 高可用组件安装

本书采用KeepAlived和HAProxy实现高可用，所以需要安装KeepAlived和HAProxy。

KeepAlived和HAProxy的节点可以和Master在同一个节点，也可以在不同的节点。

如果读者是在公有云搭建高可用集群，可以采用公有云的负载均衡替代KeepAlived和HAProxy。

一般情况下我们创建2个负载均衡器的虚拟机，然后分配一个 VIP，然后使用 VIP 为负载均衡器提供服务，通过 VIP 将流量重定向到后端的某个 Kubernetes 控制器平面节点上。



> 提示
>
> 如果读者想要搭建只有一个Master节点的集群，可以不安装高可用组件。

#### 1. 安装Haproxy和Keepalived

所有Master节点通过yum安装HAProxy和KeepAlived：

```sh
# yum install keepalived haproxy -y
```

#### 2. Haproxy配置

所有Master节点配置HAProxy（详细配置可参考HAProxy官方文档，所有Master节点的HAProxy配置相同）：

```sh
# cat > /etc/haproxy/haproxy.cfg << EOF
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    log         127.0.0.1 local2    
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon 
       
    stats socket /var/lib/haproxy/stats
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000
#---------------------------------------------------------------------
# kubernetes apiserver frontend which proxys to the backends
#--------------------------------------------------------------------- 
frontend kubernetes-apiserver
    mode                 tcp
    bind                 *:16443
    option               tcplog
    default_backend      kubernetes-apiserver    
#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server      master01.k8s.io   192.168.1.190:6443 check
    server      master02.k8s.io   192.168.1.191:6443 check
    server      master03.k8s.io   192.168.1.192:6443 check
#---------------------------------------------------------------------
# collection haproxy statistics message
#---------------------------------------------------------------------
listen stats
    bind                 *:1080
    stats auth           admin:awesomePassword
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            /admin?stats
EOF
```

#### 3. Keepalived配置

所有Master节点配置KeepAlived，由于KeepAlived需要配置自身的IP地址和网卡名称，因此每个KeepAlived节点的配置不一样。

Master01节点的配置（注意每个节点的IP和网卡interface参数）：

```sh
Master01节点的配置：
# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
    router_id k8s
    script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    mcast_src_ip 192.168.1.190
    virtual_router_id 99
    priority 101
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass ceb1b3ec013d66163d6ab
    }
    virtual_ipaddress {
        192.168.1.193
    }
    track_script {
       chk_apiserver
    }
}
```

Master02节点的配置（注意更改加粗部分的配置）：

```sh
# vim /etc/keepalived/keepalived.conf 
! Configuration File for keepalived
global_defs {
    router_id k8s
    script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    mcast_src_ip 192.168.1.191
    virtual_router_id 99
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass ceb1b3ec013d66163d6ab
    }
    virtual_ipaddress {
       192.168.1.193
    }
    track_script {
       chk_apiserver
    }
}
```

Master03节点的配置（注意更改加粗部分的配置）：

```sh
# vim /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id k8s
    script_user root
    enable_script_security
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 5
    weight -5
    fall 2  
    rise 1
}
vrrp_instance VI_1 {
    state BACKUP
    interface eth0
    mcast_src_ip 192.168.1.192
    virtual_router_id 99
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass ceb1b3ec013d66163d6ab
    }
    virtual_ipaddress {
        192.168.1.193
    }
    track_script {
       chk_apiserver
    }
}
```

#### 4. 健康检查脚本

所有Master节点配置KeepAlived健康检查文件：

```shell
cat >/etc/keepalived/check_apiserver.sh<<EOF
#!/bin/bash

err=0
for k in $(seq 1 3)
do
    check_code=$(pgrep haproxy)
    if [[ $check_code == "" ]]; then
        err=$(expr $err + 1)
        sleep 1
        continue
    else
        err=0
        break
    fi
done

if [[ $err != "0" ]]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi
EOF

# chmod +x /etc/keepalived/check_apiserver.sh
```

#### 5. 启动

启动haproxy和keepalived：

```shell
# systemctl daemon-reload
# systemctl enable --now haproxy
# systemctl enable --now keepalived
```

如果是用HAProxy和KeepAlived实现的高可用，则需要测试VIP是否是正常的：

```sh
## 所有节点进行ping测试
# ping 192.168.1.193 -c 4
PING 192.168.1.193 (192.168.1.193) 56(84) bytes of data.
64 bytes from 192.168.1.193: icmp_seq=1 ttl=64 time=0.120 ms
64 bytes from 192.168.1.193: icmp_seq=2 ttl=64 time=0.061 ms
64 bytes from 192.168.1.193: icmp_seq=3 ttl=64 time=0.043 ms
64 bytes from 192.168.1.193: icmp_seq=4 ttl=64 time=0.047 ms
--- 10.0.0.236 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3106ms
rtt min/avg/max/mdev = 0.062/0.163/0.464/0.173 ms

## 所有节点进行telnet测试
# telnet 192.168.1.193 16443
Trying 10.0.0.236...
Connected to 10.0.0.236.
Escape character is '^]'.
Connection closed by foreign host.
```

如果ping不通且telnet没有出现，则认为VIP不可用。

如果VIP不可用，不可再继续往下执行，需要排查VIP的问题，比如防火墙和SELinux、HAProxy和Keepalived的状态，监听端口是否正常等。





### 1.5 集群初始化

使用Kubeadm安装集群时，需要一个Master节点初始化集群，然后加入其他节点即可。

初始化集群时，可以直接使用Kubeadm命令进行初始化，也可以使用一个配置文件进行初始化，由于使用命令行的形式可能需要配置的字段比较多，因此本示例采用配置文件进行初始化。

> 注意
>
> 首先创建的是kubeadm配置文件，宿主机网段、podSubnet网段、serviceSubnet网段不能重复；
>
> 其次，kubernetesVersion的值改为和读者环境Kubeadm版本一致，可以通过kubeadm version命令查询；
>
> 最后，如果不是高可用集群，controlPlaneEndpoint需要改为Master节点的IP和6443端口，certSANs也需要改为Master节点的IP。
>
> 注意criSocket更改为自己的Runtime。

Master01节点创建kubeadm-config.yaml配置文件如下（也可以使用如下命令自动生成`kubeadm config print init-defaults > kubeadm-config.yaml`）：

`kubeadm-config.yaml`

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.1.190
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: k8s-master01
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
    
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs  	# kube-proxy 模式
---
apiServer:
  certSANs:
  - 192.168.1.193
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: 192.168.1.193:6443
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.25.6	# 更改此处的版本号和kubeadm version一致
networking:
  dnsDomain: cluster.local
  podSubnet: 172.16.0.0/12
  serviceSubnet: 192.168.0.0/16
scheduler: {}
```

由于读者的版本和此示例可能不太一致，因此需要更新一下kubeadm配置文件（Master01节点操作）：

```sh
# kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml
```

将new.yaml文件复制到其他Master节点：

```sh
# for i in k8s-master02 k8s-master03; do scp new.yaml $i:/root/; done
```

之后所有Master节点提前下载镜像，可以节省初始化时间（其他节点不需要更改任何配置，包括IP地址也不需要更改）：

```sh
# kubeadm config images pull --config /root/new.yaml 
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.25.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.25.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.25.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.25.0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.8
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.6-0
[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.9.3
```

初始化Master01节点，初始化以后会在/etc/kubernetes目录下生成对应的证书和配置文件，之后其他Master节点加入Master01即可：

```sh
# kubeadm init --upload-certs --config /root/new.yaml
```

初始化成功以后，会产生Token值，用于其他节点加入时使用，因此要记录一下初始化成功生成的token值（令牌值）：

```sh
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.1.193:16443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4967912ebd27dc2541b108fcf80f8c0310e83cf24a79361f56c89aefdb0c2043 \
        --control-plane --certificate-key 00523d224310fbab1f4feb18e8a9376087ed4cec14634b733c5ff037fdacc4e0

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:
kubeadm join 192.168.1.193:16443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4967912ebd27dc2541b108fcf80f8c0310e83cf24a79361f56c89aefdb0c2043
        
        
# 检查网络是否生效
# kubectl cluster-info dump | grep -m 1 cluster-cidr
                            "--cluster-cidr=172.16.0.0/12"
                            
                            
# mkdir -p $HOME/.kube
# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
# sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

如果初始化失败，需要检查各项配置是否正确，之后再次初始化，清理命令如下：

```shell
# kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube
```

#### 1.5.1 配置kubectl

初始化成功后，Master01节点配置KUBECONFIG环境变量，之后Kubectl即可访问Kubernetes集群：

```shell
# 设置Kubectl 自动补全
yum install bash-completion -y
source <(kubectl completion bash) # 在 bash 中设置当前 shell 的自动补全，要先安装 bash-completion 包。
echo "source <(kubectl completion bash)" >> ~/.bashrc # 在你的 bash shell 中永久地添加自动补全
kubectl completion bash >/etc/bash_completion.d/kubectl

# cat <<EOF >> /root/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
echo 'alias k=kubectl'
echo 'complete -F __start_kubectl k'
EOF
source /root/.bashrc
```

查看节点状态

```sh
# kubectl get node
```

#### 1.5.2 配置helm

```sh
$ wget https://mirrors.huaweicloud.com/helm/v3.7.2/helm-v3.7.2-linux-amd64.tar.gz
$ tar -zxvf helm-v3.7.2-linux-amd64.tar.gz 
$ cp linux-amd64/helm /usr/local/bin/
$ chmod a+x  /usr/local/bin/helm

#查看版本
$ helm version
version.BuildInfo{Version:"v3.7.2", GitCommit:"663a896f4a815053445eec4153677ddc24a0a361", GitTreeState:"clean", GoVersion:"go1.16.10"}
```



###  1.6 Master实现高可用

其他Master节点和Node节点加入集群，方法是一致的，只不过Master节点加入集群时需要指定--control-plane和--certificate-key参数。

将Master02和Master03加入集群，两个节点分别执行：

```sh
  kubeadm join 192.168.1.193:16443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4967912ebd27dc2541b108fcf80f8c0310e83cf24a79361f56c89aefdb0c2043 \
        --control-plane --certificate-key 00523d224310fbab1f4feb18e8a9376087ed4cec14634b733c5ff037fdacc4e0
```

3个Master节点加入后，可以查看此时的节点状态

```sh
# kubectl get node
Unable to connect to the server: net/http: TLS handshake timeout
[root@k8s-master01 ~]# kubectl  get nodes
NAME           STATUS     ROLES           AGE     VERSION
k8s-master01   NotReady   control-plane   5m21s   v1.25.6
k8s-master02   NotReady   control-plane   20s     v1.25.6
k8s-master03   NotReady   control-plane   88s     v1.25.6
k8s-node01     NotReady   <none>          2m3s    v1.25.6
```

###  1.7 添加Worker节点

```sh
kubeadm join 192.168.1.193:16443 --token abcdef.0123456789abcdef \
        --discovery-token-ca-cert-hash sha256:4967912ebd27dc2541b108fcf80f8c0310e83cf24a79361f56c89aefdb0c2043
```

> 提示
>
> 所有节点加入后，节点的STATUS字段为NotReady，由于版本不同，显示的结果可能也不同，如果是NotReady，安装完CNI即可变成Ready状态。



###  1.8 Token过期处理

默认情况下，新建的集群Token有效期是24小时，如果过期的话，需要重新生成Token，此时可以使用如下命令生成：

```shell
# kubeadm token create --print-join-command
```

如果需要添加Master，certificate-key也需要重新生成：

```shell
# kubeadm init phase upload-certs  --upload-certs
```



###  1.9 Calico组件的安装

接下来安装CNI插件，CNI插件可以选择Calico或者Flannel等。

由于本书后期涉及网络策略（NetworkPolicy）的学习，需要CNI插件支持NetworkPolicy，因此本次安装的是Calico（Flannel暂不支持）。

在Master01节点安装Calico（注意其中1.23的版本号需要改成读者使用的版本号）：

```shell
# cd /root/k8s-ha-install && git checkout manual-installation-v1.25.x && cd calico/
```

修改Pod网段：

```shell
# POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= '{print $NF}'`

# sed -i "s#POD_CIDR#${POD_SUBNET}#g" calico.yaml

# kubectl apply -f calico.yaml
```

创建完成后，等待几分钟后查看状态：

```sh
# kubectl get pod -n kube-system
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-86d8c4fb68-4wk7q   1/1     Running   0          3m50s
calico-node-8cqhj                          1/1     Running   0          3m50s
calico-node-9dq92                          1/1     Running   0          3m50s
calico-node-dw6tq                          1/1     Running   0          3m50s
calico-node-g94pp                          1/1     Running   0          3m50s
calico-typha-768795f74d-clpvm              1/1     Running   0          3m49s
coredns-7f8cbcb969-f7chc                   1/1     Running   0          10m
coredns-7f8cbcb969-fv9fv                   1/1     Running   0          10m
etcd-k8s-master01                          1/1     Running   0          11m
etcd-k8s-master02                          1/1     Running   0          6m
etcd-k8s-master03                          1/1     Running   0          7m7s
kube-apiserver-k8s-master01                1/1     Running   0          11m
kube-apiserver-k8s-master02                1/1     Running   0          5m48s
kube-apiserver-k8s-master03                1/1     Running   0          7m
kube-controller-manager-k8s-master01       1/1     Running   0          11m
kube-controller-manager-k8s-master02       1/1     Running   0          6m2s
kube-controller-manager-k8s-master03       1/1     Running   0          5m54s
kube-proxy-jcm25                           1/1     Running   0          6m10s
kube-proxy-q8msx                           1/1     Running   0          10m
kube-proxy-rtwdz                           1/1     Running   0          7m53s
kube-proxy-s2xvg                           1/1     Running   0          7m18s
kube-scheduler-k8s-master01                1/1     Running   0          11m
kube-scheduler-k8s-master02                1/1     Running   0          5m59s
kube-scheduler-k8s-master03                1/1     Running   0          7m6s

# kubectl  get nodes
NAME           STATUS   ROLES           AGE     VERSION
k8s-master01   Ready    control-plane   11m     v1.25.6
k8s-master02   Ready    control-plane   6m25s   v1.25.6
k8s-master03   Ready    control-plane   7m33s   v1.25.6
k8s-node01     Ready    <none>          8m8s    v1.25.6
```

###  1.10 Metrics部署

在新版的Kubernetes中，系统资源的采集使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。

将Master01节点的front-proxy-ca.crt复制到所有Node节点：

```sh
# scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node01:/etc/kubernetes/pki/front-proxy-ca.crt
# scp /etc/kubernetes/pki/front-proxy-ca.crt k8s-node(其他节点自行拷贝):/etc/kubernetes/pki/front-proxy-ca.crt
```

安装metrics server：

```sh
# cd /root/k8s-ha-install/kubeadm-metrics-server

# kubectl create -f comp.yaml 
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
```

查看Metrics Server状态：

```sh
# kubectl get pod -n kube-system -l k8s-app=metrics-server
NAME                              READY   STATUS    RESTARTS   AGE
metrics-server-74db45c9df-xstzf   1/1     Running   0          3m3s
```

状态正常后，查看度量指标：

```sh
# kubectl top node
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   153m         3%     1701Mi          44%       
k8s-master02   125m         3%     1693Mi          44%       
k8s-master03   129m         3%     1590Mi          41%       
k8s-node01     73m          1%     989Mi           25%       
     
# kubectl top po -A
NAMESPACE     NAME                                       CPU(cores)   MEMORY(bytes)
kube-system   calico-kube-controllers-86d8c4fb68-4wk7q   3m           16Mi
kube-system   calico-node-8cqhj                          37m          126Mi
kube-system   calico-node-9dq92                          78m          97Mi
kube-system   calico-node-dw6tq                          52m          111Mi
kube-system   calico-node-g94pp                          74m          99Mi
kube-system   calico-typha-768795f74d-clpvm              8m           23Mi
kube-system   coredns-7f8cbcb969-f7chc                   2m           13Mi
kube-system   coredns-7f8cbcb969-fv9fv                   3m           12Mi
kube-system   etcd-k8s-master01                          79m          72Mi
kube-system   etcd-k8s-master02                          77m          68Mi
kube-system   etcd-k8s-master03                          53m          68Mi
kube-system   kube-apiserver-k8s-master01                63m          311Mi
kube-system   kube-apiserver-k8s-master02                64m          314Mi
kube-system   kube-apiserver-k8s-master03                44m          389Mi
kube-system   kube-controller-manager-k8s-master01       24m          59Mi
kube-system   kube-controller-manager-k8s-master02       4m           29Mi
kube-system   kube-controller-manager-k8s-master03       2m           28Mi
kube-system   kube-proxy-jcm25                           1m           18Mi
kube-system   kube-proxy-q8msx                           1m           19Mi
kube-system   kube-proxy-rtwdz                           1m           21Mi
kube-system   kube-proxy-s2xvg                           1m           18Mi
kube-system   kube-scheduler-k8s-master01                5m           27Mi
kube-system   kube-scheduler-k8s-master02                4m           24Mi
kube-system   kube-scheduler-k8s-master03                4m           23Mi
kube-system   metrics-server-74db45c9df-xstzf            11m          26Mi
```

###  1.11 Dashboard部署

Kubernetes官方提供了简单的图形化展示，可用于展示集群中的各类资源，同时也可以通过Dashboard实时查看Pod的日志和在容器中执行一些命令等。

#### 1.11.1 通过YAML部署

##### 1 安装Dashboard

在Master01节点创建Dashboard,v1.25.6 版本的集群需要安装最新的 2.0+ 版本的 Dashboard,（该命令包含管理员用户的创建）：

```sh
# wget https://raw.githubusercontent.com/cby-chen/Kubernetes/main/yaml/dashboard.yaml


##目前最新版本v2.6.0 

# vim dashboard.yaml

----
spec:
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30443
  type: NodePort
  selector:
    k8s-app: kubernetes-dashboard
----


# kubectl apply -f dashboard.yaml
namespace/kubernetes-dashboard created
serviceaccount/kubernetes-dashboard created
service/kubernetes-dashboard created
secret/kubernetes-dashboard-certs created
secret/kubernetes-dashboard-csrf created
secret/kubernetes-dashboard-key-holder created
configmap/kubernetes-dashboard-settings created
role.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created
rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created
deployment.apps/kubernetes-dashboard created
service/dashboard-metrics-scraper created
deployment.apps/dashboard-metrics-scraper created

## 创建用户：
# wget https://raw.githubusercontent.com/cby-chen/Kubernetes/main/yaml/dashboard-user.yaml
# kubectl apply -f dashboard-user.yaml
```



##### 2 登录Dashboard

创建完成后，可以查看Dashboard Service的端口号（NodePort）：

```sh
# kubectl get svc kubernetes-dashboard -n kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard   NodePort   192.168.250.65   <none>        443:30443/TCP   36s
```

查看登录的Token值：

```sh
# kubectl -n kubernetes-dashboard create token admin-user
eyJhbGciOiJSUzI1NiIsImtpZCI6Ii1fbkh2Um5JS1VoSFRQSkEzZUJwRVNlUmFBaGwwZnVpUkVCeXk3Z2pGTkUifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjc1OTE0MjY0LCJpYXQiOjE2NzU5MTA2NjQsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiNzczMDAyYmEtZmNmMS00YWI0LWE0NGItN2FlNDlmN2M5MDU4In19LCJuYmYiOjE2NzU5MTA2NjQsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.pA7B6BSvQzhCQl2t6BnUTOg6GoDRKVcKEynwTDGxHY8Zh88y2VWdXq60zs5FeaxyUFLNDrj2UWX74TM_Hw7H4hPKX_PXXvsO7Du3ykWzCeIhYV2TZCUnal5iQlsS_A83QkWgXqojL1c_s00K3PNyEqqfL8D61jJLOzb4YihTUnyOLnJ8SQq9EDTxZPsVp07hiZ76gnFijvAp8bsClS5WP0lj0riI2Oi1cL7IE0HD4aoW7oHDLEsvwqjlaon7xwA6OtQZfDOEMjptIgDLppKMdJyxeWZaw_4825ByU06_YY60OcwR8tEFv93fvSSCZadIxJhf1nmJDprEi414ATIKrg
```



访问https://node01IP:30443/就能看到登录界面。

输入token：

```
eyJhbGciOiJSUzI1NiIsImtpZCI6Ii1fbImh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY........
```







#### 1.11.2 通过helm部署

Chart 仓库地址：https://artifacthub.io/packages/helm/k8s-dashboard/kubernetes-dashboard

```sh
# helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
# helm repo update
## 下载到本地安装
# helm search repo kubernetes-dashboard/kubernetes-dashboard
NAME                                            CHART VERSION   APP VERSION     DESCRIPTION
kubernetes-dashboard/kubernetes-dashboard       6.0.0           2.7.0           General-purpose web UI for Kubernetes clusters

# helm pull kubernetes-dashboard/kubernetes-dashboard --version 6.0.0 --untar

# cat values-pro.yaml
extraArgs:
  - --token-ttl=0
  - --system-banner="Welcome to Kubernetes Cluster. 操作需谨慎！！！"
service:
  type: NodePort
  nodePort: 30443
rbac:
  clusterReadOnlyRole: true
metricsScraper:
  enabled: true

# helm install kubernetes-dashboard kubernetes-dashboard -f values-pro.yaml --namespace kubernetes-dashboard --create-namespace

NAME: kubernetes-dashboard
LAST DEPLOYED: Thu Feb  9 10:52:32 2023
NAMESPACE: kubernetes-dashboard
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
*********************************************************************************
*** PLEASE BE PATIENT: kubernetes-dashboard may take a few minutes to install ***
*********************************************************************************

Get the Kubernetes Dashboard URL by running:
  export NODE_PORT=$(kubectl get -n kubernetes-dashboard -o jsonpath="{.spec.ports[0].nodePort}" services kubernetes-dashboard)
  export NODE_IP=$(kubectl get nodes -o jsonpath="{.items[0].status.addresses[0].address}")
  echo https://$NODE_IP:$NODE_PORT/

```



检查kubernetes-dashboard部署状态

```sh
# kubectl get pods,svc -n kubernetes-dashboard | grep kubernetes-dashboard
pod/kubernetes-dashboard-5486c8f5bf-xff6r   1/1     Running   0          45s
service/kubernetes-dashboard   NodePort   192.168.48.88   <none>        443:30443/TCP   45s
```

设置登录的token信息

```sh
# cat >kubernetes-dashboard-admin-user.yaml<<EOF
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard


---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

# kubectl apply -f kubernetes-dashboard-admin-user.yaml
serviceaccount/admin-user unchanged
clusterrolebinding.rbac.authorization.k8s.io/admin-user unchanged

# kubectl -n kubernetes-dashboard create token admin-user
eyJhbGciOiJSUzI1NiIsImtpZCI6Ii1fbkh2Um5JS1VoSFRQSkEzZUJwRVNlUmFBaGwwZnVpUkVCeXk3Z2pGTkUifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNjc1OTE2NTAwLCJpYXQiOjE2NzU5MTI5MDAsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJhZG1pbi11c2VyIiwidWlkIjoiN2VkNGQxNjItMzlkOC00ODc2LTk0ZjgtZTliYzUxNThmYmNhIn19LCJuYmYiOjE2NzU5MTI5MDAsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbi11c2VyIn0.CX98KwrW01fNSN0SJruN31bmflhdlueTXSfTfYh0IdvYFxuJbWGY1Zuslcb8JxowS4TW5ovqB71-3Sad9346oVwpel1fJwy2kGFopOQnK6AiY67XhuxIduDqYHqPLzETFp5n7ij4_Miv9bvt5-TG4u4arLpPEOFibUKOc4Z3k_wcRujO_re6JP8ElZzUrGh9EZ0BpdUaPrMiYFAyqXn6Uu5kC9_c5nV_ORYFXda3uX1mHxAN07_osNoYsTJsHXM46UX_5SwrQGfL3Xtgh7GiC-o9ZEawVtk9Vqkm335b8hxJEJp-bwsQrMUBUTelNYcf0KmCXHAQ8R1XilgDNZOlIg
```

访问https://node01IP:30443/就能看到登录界面。

![](../../assets/static/kubernetes/1/kubernetes-dashboard.png)



输入token：

```
eyJhbGciOiJSUzI1........
```

**Clean up and next steps**

Remove the admin `ServiceAccount` and `ClusterRoleBinding`.

```
kubectl -n kubernetes-dashboard delete serviceaccount admin-user
kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user
```





###  1.12 注意事项

#### 1.12.1 配置方面

和二进制安装方式不同的是，使用Kubeadm安装的Kubernetes集群，

大部分组件都是以容器的方式运行的，比如kube-apiserver、kube-proxy等，可以通过kubectl get po -n kube-system查看系统的Pod。

Master节点的组件kube-apiserver、kube-scheduler、kube-controller-manager、etcd的配置和启动文件默认都放置在/etc/kubernetes/manifests，以静态Pod的方式启动。

比如apiserver的kube-apiserver.yaml，该YAML文件更改后，kubelet会自动刷新配置，也就是会重启Pod，无须其他的操作。

另外需要注意的是，kube-proxy是以DaemonSet（守护进程集，会在所有符合条件的节点上部署一个Pod）的方式安装的，其配置文件使用Kubernetes的ConfigMap进行管理，并非宿主机上的某个文件，如果需要修改kube-proxy的配置，需要更改该ConfigMap之后，再对kube-proxy进行滚动更新才会生效。

比如将kube-proxy代理模式改为ipvs模式，可以通过kubectl命令编辑kube-proxy配置文件的ConfigMap。以下操作在Master01上执行。

```sh
# kubectl edit cm kube-proxy -n kube-system
mode: "ipvs" # mode更改为ipvs
```

更新Kube-Proxy的Pod：

```sh
# kubectl patch daemonset kube-proxy -p "{\"spec\":{\"template\":{\"metadata\":{\"annotations\":{\"date\":\"`date +%s`\"}}}}}" -n kube-system
daemonset.apps/kube-proxy patched

## 或者重启kube-proxy
# kubectl get pod -n kube-system | grep kube-proxy |awk '{system("kubectl delete pod "$1" -n kube-system")}'
```

验证Kube-Proxy模式：

```sh
# curl 127.0.0.1:10249/proxyMode
ipvs
```

#### 1.12.2 污点方面

Kubeadm安装后，会自动添加NoSchedule的污点，所以Master节点默认不允许部署非系统Pod，可以通过删除污点的方式允许部署。

首先查看污点：

```sh
## 查看污点
# kubectl describe node -l node-role.kubernetes.io/master= | grep -B 3 Taints
```

然后删除污点（生产环境最好不要在Master节点部署其他Pod）：

```sh
## 删除污点
# kubectl taint node -l node-role.kubernetes.io/master node-role.kubernetes.io/master:NoSchedule-

## 查看污点
# kubectl describe node -l node-role.kubernetes.io/master= | grep Taints
```

#### 1.12.3 设置标签

```sh
## 设置标签
# kubectl label node gitee-k8s-w02 monitor=prometheus
node/gitee-k8s-w02 labeled

## 带有prometheus标识符的标签
# kubectl label nodes gitee-k8s-w02 node-role.kubernetes.io/prometheus="true"


## 删除标签
# kubectl label nodes gitee-k8s-w02 monitor-
node/gitee-k8s-w02 labeled

## 带有prometheus标识符的标签
# kubectl label nodes gitee-k8s-w02 node-role.kubernetes.io/prometheus-


## 修改一个label的值，需要加上–overwrite参数：
# kubectl label nodes k8s-test01 gpu=false --overwrite


## 查看所有的lables信息
# kubectl get nodes --show-labels

## 或者
# kubectl describe nodes k8s-test01
```

除了自己定义的标签之外，Kubernetes还会为每个节点自动生成系统级标签。

- Kubernetes.io/hostname：机器名称，例如，gitee-k8s-w29。
- Kubernetes.io/os：系统名称，例如，Linux/Windows。
- Kubernetes.io/arch：架构名称，例如，amd64。

只有使用公有云厂商自家的Kubernetes时才会有以下标签，私有Kubernetes集群没有这些标签。

- failure-domain.beta.Kubernetes.io/region：地域名称。
- failure-domain.beta.Kubernetes.io/zone：地域下的区域名称。
- beta.Kubernetes.io/instance-type：使用的cloudprovider名称。



###  1.13小结

本章完成了使用Kubeadm方式安装高可用的Kubernetes集群，至此读者已经打开了Kubernetes的第一扇大门。

在安装过程中，如果有任何疑问，可以直接在GitHub上提问，如果对概念不理解，直接翻到对应的基础概念部分学习即可。

虽然Kubeadm的安装方式比较简单，并且是官方推荐的安装方式，但是Kubeadm安装的Kubernetes集群证书有效期默认只有一年，到期后需要进行升级或者更新证书，当然有兴趣的读者也可以更改Kubeadm的源码，将证书更改为100年，但是这不是推荐的方式。

目前Kubernetes保持一年3个大版本的更新，推荐读者采用升级的方式更新证书。





## 2. Nginx+Keepalived 搭建高可用kubernetes集群（centos7）

基本环境配置与上面一致

### 2.1 高可用组件安装

#### 1. 安装Nginx和Keepalived

```sh
## 在3个master节点上执行
# rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm
# yum install nginx keepalived -y
```

#### 2. Nginx配置

所有master 节点配置

```sh
# cat > /etc/nginx/nginx.conf << "EOF"
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log;
pid /run/nginx.pid;
include /usr/share/nginx/modules/*.conf;
events {
   worker_connections 1024;
}
# 四层负载均衡，为两台Master apiserver组件提供负载均衡
stream {
   log_format  main  '$remote_addr $upstream_addr - [$time_local] $status $upstream_bytes_sent';
   access_log  /var/log/nginx/k8s-access.log  main;
   upstream k8s-apiserver {
   # Master1 APISERVER IP:PORT
      server 192.168.1.190:6443;
   # Master2 APISERVER IP:PORT
      server 192.168.1.191:6443;
   # Master3 APISERVER IP:PORT
      server 192.168.1.192:6443;
   }
   server {
      listen 16443;
      proxy_pass k8s-apiserver;
   }
}
http {
   log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                     '$status $body_bytes_sent "$http_referer" '
                     '"$http_user_agent" "$http_x_forwarded_for"';
   access_log  /var/log/nginx/access.log  main;
   sendfile            on;
   tcp_nopush          on;
   tcp_nodelay         on;
   keepalive_timeout   65;
   types_hash_max_size 2048;
   include             /etc/nginx/mime.types;
   default_type        application/octet-stream;
   server {
       listen       80 default_server;
       server_name  _;
       location / {
       }
   }
}
EOF
```

> 【温馨提示】如果只保证高可用，不配置 k8s-apiserver 负载均衡的话，可以不装 nginx，但是最好还是配置一下 k8s-apiserver 负载均衡。

#### 3. Keepalived配置

所有master节点配置

```sh
# cat > /etc/keepalived/keepalived.conf << EOF
global_defs {
  notification_email {
    acassen@firewall.loc
    failover@firewall.loc
    sysadmin@firewall.loc
  }
  notification_email_from fage@qq.com
  smtp_server 127.0.0.1
  smtp_connect_timeout 30
  router_id NGINX_MASTER
}
vrrp_script check_nginx {
   script "/etc/keepalived/check_apiserver.sh"
   interval 5
   weight -5
   fall 2  
   rise 1
}
vrrp_instance VI_1 {
   state MASTER		# 备机 BACKUP
   interface eth0
   virtual_router_id 51 		 # VRRP 路由 ID实例，每个实例是唯一的
   priority 100    				 # 优先级，备服务器设置 99 		master1:100  master2、master3:99
   advert_int 1    				 # 指定VRRP 心跳包通告间隔时间，默认1秒
   authentication {
       auth_type PASS
       auth_pass 1111
   }
   # 虚拟IP
   virtual_ipaddress {
       192.168.1.193
   }
   track_script {
       check_nginx
   }
}
EOF
```



#### 4. 健康检查脚本

```sh
cat > /etc/keepalived/check_apiserver.sh  << "EOF"
#!/bin/bash
count=$(ps -ef |grep nginx |egrep -cv "grep|$$")
if [ "$count" -eq 0 ];then
   exit 1
else
   exit 0
fi
EOF
chmod +x /etc/keepalived/check_apiserver.sh
```



#### 5.启动

```sh
systemctl daemon-reload
systemctl restart nginx && systemctl enable --now nginx 
systemctl restart keepalived && systemctl enable --now keepalived 
```

## 

## 3. kube-vip 搭建高可用 Kubernetes 集群

> 参考文献
>
> https://www.qikqiak.com/k8strain2/maintain/ha/







##  4. 开发环境部署

### 4.1 单master版本

两个命令从空白系统到单 master 节点 K8s 集群。

https://gitee.com/atompi/install-single-master-K8s

> 参考文献
>
> Kubernetes 集群环境安装
>
> [https://www.qikqiak.com/k8strain/basic/install/](https://gitee.com/link?target=https%3A%2F%2Fwww.qikqiak.com%2Fk8strain%2Fbasic%2Finstall%2F)
>
> https://docs.youdianzhishi.com/k8s/basic/install/







### 4.2 高可用Ha方案

可用于生产环境的 Kubernetes HA 部署方案，基于 kubeadm + Ansible + Keepalived + Haproxy

https://gitee.com/atompi/Prod-K8S-HA-Installer









## 5.三方部署方案

### 5.1 kubeode

https://gitee.com/q7104475/kubeode?_from=gitee_search



### 5.2 kainstall

[https://github.com/lework/kainstall](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Flework%2Fkainstall)

https://gitee.com/uxvim/kainstall



### 5.3 kubeasz

[https://github.com/easzlab/kubeasz](https://gitee.com/link?target=https%3A%2F%2Fgithub.com%2Feaszlab%2Fkubeasz)



### 5.4 基于SealOS安装HA集群 (推荐)

https://www.kubegems.io/docs/installation/kubernetes-install/sealos



### 5.5  基于Kind安装 (实验性质)

https://www.kubegems.io/docs/installation/kubernetes-install/kind



### 5.6 基于KuboardSpray 安装kubernetes

 https://www.kuboard.cn/install/install-k8s.html







## 6.清理

如果你的集群安装过程中遇到了其他问题，我们可以使用下面的命令来进行重置：

```sh
➜  ~ kubeadm reset
➜  ~ ifconfig cni0 down && ip link delete cni0
➜  ~ ifconfig flannel.1 down && ip link delete flannel.1
➜  ~ rm -rf /var/lib/cni/
```

