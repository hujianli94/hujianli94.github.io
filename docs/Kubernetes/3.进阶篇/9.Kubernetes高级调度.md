# Kubernetes高级调度

在前面的章节已经学习过Kubernetes的调度基础，实现了将自己的服务部署到Kubernetes。但是在生产环境中，调度远远比自己想象的要复杂。 比如：

- 某些程序只能部署在固定的几台机器上。
- 某些机器只能部署指定的Pod。
- 节点挂了怎么快速恢复。
- 节点挂了如何让影响度最小。
- 在线Debug没有命令怎么办。

上述只是列举了一些需求，真实的情况可能更为复杂，所以我们需要更 高级的调度方式、更实用的功能来解决上述问题。

幸运的是，Kubernetes原生的很多功能均能解决上述问题，本章我们就来学习Kubernetes更高级的调度。




## 1. 初始化容器InitContainer
InitContainer是Kubernetes的初始化容器（也可称之为Init容器），它是一种特殊的容器，在Pod内的应用容器启动之前运行，可以包括一些应用镜像中不存在的实用工具和安装脚本，用以在程序启动时进行初始化，比如创 建文件、修改内核参数、等待依赖程序启动等。


每个Pod中可以包含多个容器，同时Pod也可以有一个或多个先于应用容器启动的Init容器，在Pod定义中和containers同级，按顺序逐个执行，
当所有的Init容器运行完成时，Kubernetes才会启动Pod内的普通容器。

Init容器与普通的容器非常像，除了如下几点：

- 它们总是运行到完成。

- 上一个运行完成才会运行下一个。

- 如果Pod的Init容器失败，Kubernetes会不断地重启该Pod，直到Init容器成功为止 ， 但是Pod对应的restartPolicy值为Never， Kubernetes不会重新启动Pod。
  


为Pod设置Init容器需要在Pod的spec中添加initContainers字段，该字 段和应用的containers数组同级相邻，配置方式和containers中的普通容器 差不多，Init容器支持应用容器的全部字段和特性，包括资源限制、数据卷 和安全设置。

但是Init容器对资源请求和限制的处理稍有不同，Init容器不支持lifecycle、livenessProbe、readinessProbe和startupProbe，因为它 们必须在Pod就绪之前运行完成。




在生产环境中，为了应用的安全和优化镜像的体积，业务镜像一般不会 安装高危工具和并不常用的运维工具，比如curl、sed、awk、python或dig等，同时建议使用非root用户去启动容器。


但是某些应用启动之前可能需要检测依赖的服务有没有成功运行，或者需要高权限去修改一些系统级配置，
而这些检测或配置更改都是一次性的，所以在制作业务镜像时没有必要为了一次配置的变更去安装一个配置工具，更没有必要因为使用一次高权限而把整个镜像改成以root身份运行。


考虑到上述问题和需求，Kubernetes引入了初始化容器的概念，Init容器具有与应用容器分离的单独镜像，其启动相关代码具有如下优势：

- Init容器可以包含安装过程中应用容器中不存在的实用工具或个性化代码。
- Init容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。
- Init容器可以以root身份运行，执行一些高权限命令。
- Init容器相关操作执行完成后就会退出，不会给业务容器带来安全隐患。



由于Init容器必须在应用容器启动之前运行完成，因此Init容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足一组先决条件，Pod内的所有应用容器才会并行启动。




### 1.1 示例1：等待依赖服务启动

有时某些服务需要依赖其他组件才能启动，比如后端应用需要数据库启动之后，应用才能正常启动，所以此时需要检测数据库实例是否正常，
待数据库可以正常使用时，再启动后端应用，此时可以使用初始化容器进行控制，对应的YAML文件如下（部分代码）：
```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: init-test
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: init
    spec:
      containers:
      - name: nginx-init
        image: nginx:1.7.9
        ports:
        - containerPort: 80
      initContainers:
      - name: init-mydb
        image: busybox
        command: ['sh', '-c', 'until nslookup mysql; do echo waiting for mysql; sleep 2; done;']
      restartPolicy: Always
```
上面的示例是打算启动一个Nginx服务，在启动服务之前来判断MySQL的服务是否启动。通过nslookup检测mysql服务是否成功启动。如果mysql服务启动了之后，nslookup 检测域名也会成功，后续会启动Nginx容器；

否则，将会等待两秒之后，再次检测。

但是，这种方式我在使用过程中，并没有成功。我的环境是为mysql配置了Deploy和Service，在关闭pod的情况下通过nslookup来检测域名还是可以成功。

后面我直接使用curl来判断端口服务是否可用。
```yaml
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: init-test
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: init
    spec:
      containers:
      - name: nginx-init
        image: nginx:1.7.9
        ports:
        - containerPort: 80
      initContainers:
      - name: init-mydb
        image: curlimages/curl:7.68.0
        command: ['sh', '-c', 'until curl 192.168.10.254:6379;; do echo waiting for mysql; sleep 2; done;']
      restartPolicy: Always
```

Init container容器中使用curl镜像，命令通过curl + 服务地址,服务地址就是你的MySQL服务地址，直接来判断服务是否可用正常使用。

当MySQL服务没有正常启动时，错误信息：
```
curl: (7) Failed to connect to 192.168.10.254 port 6379: Connection refused，init container
```
会一直循环检测。


到服务正常启动之后，
```
It looks like you are trying to access mysql over HTTP on the native driver port.
```
之后服务就可以正常启动了。



### 1.2 示例2：服务注册

注册这个Pod的IP到远程服务器，通过在命令中调用API，类似如下（部分代码）：
```yaml
      initContainers:
      - command:
        - sh 
        - -c 
        - |
          curl -X POST http://$MANAGEMENT_SERVICE_HOST: $MANAGEMENT_SERVICE_PORT/REGISTER \
          -d 'instance=$(<POD_NAME>)&ip=$(<POD_IP>)'
        env:
        - name: TZ
          value: Asia/Shanghai
        - name: LANG
          value: C.UTF-8
        image: curl
        imagePullPolicy: Always
        name: init_curl 
      restartPolicy: Always
```

### 1.3 示例3：克隆Git代码到容器中

```yaml
     initContainers:
      - command:
        - sh 
        - -c 
        - |
          git clone https://github.com/dotbalo/kubenetes-guide.git
        env:
        - name: TZ
          value: Asia/Shanghai
        - name: LANG
          value: C.UTF-8
        image: alpine/git:latest
        imagePullPolicy: Always
        name: init_git
      restartPolicy: Always

```



### 1.4 示例4：多个初始化容器使用

如前面所述，一个Pod中可以运行多个Init容器。比如一个应用需要等待该应用所在Namespace的一个Service对应的Pod启动，也需要一个DB的 Service启动，当两个InitContainer都成功后，才可以启动业务应用容器， 此时可以参考如下配置：
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  # 业务应用容器
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  # 初始化容器列表
  initContainers:
  # 第一个初始化容器，等待当前Namespace下的myservice启动
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
  # 第二个初始化容器，等待DB的Service启动
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]
```


此时可以查看该Pod的初始化容器的日志：
```shell
# 查看第一个初始化容器
$ kubectl logs myapp-container -c init-myservice  

# 查看第二个初始化容器
$ kubectl logs myapp-container -c init-mydb
```

此时Init容器将会一直等待mydb和myservice的Service可用。当mydb和myservice可用后，my-app的Pod进入Running状态：
```shell
$ kubectl get -f myapp.yaml
```

其他示例（如sleep指定的时间、根据当前Pod环境变量使用jinja生成配 置文件）在此不再演示，只需要找到对应的镜像，之后更改执行命令即可。




## 2. 临时容器Ephemeral Containers

临时容器功能在Kubernetes 1.16 后的版本才可以使用，并且在1.16~1.18版本和1.18+版本的使用方法不太一致，使用时需要多加注意。


（1）在1.16~1.18版本中临时容器的使用方法

临时容器是使用Pod的ephemeralcontainers子资源创建的，
可以使用kubectl --raw向一个正在运行的Pod注入一个临时容器，假如向kube-system命名空间下的metrics-server容器添加一个busybox的临时容器。


首先查看metrics-server的Pod名称：
```shell
$ kubectl get pod -n kube-system -l k8s-app=metrics-server
```

接着创建一个ec.json文件，声明临时容器的配置，内容如下：
```json
{
   "apiVersion": "v1",
    "kind":"EphemeralContainers",
    "metadata":{"name": "metrics-server-89786cd84-t9dwm"},
   "ephemeralContainers": [{
       "command": [
           "sh"
        ],
       "image": "busybox",
       "imagePullPolicy": "IfNotPresent",
       "name":"debugger",
       "stdin": true,
        "tty":true,
       "terminationMessagePolicy": "File"
    }]
}
```
需要注意的地方：

- metadata.name：被注入容器的名字。
- ephemeralContainers[].image：临时容器使用的镜像。
- ephemeralContainers[].name：临时容器的名称。

之后使用kubectl replace --raw命令更新已运行的临时容器metrics-server-89786cd84-t9dwm：

```shell
$ kubectl replace --raw /api/v1/namespaces/kube-system/pods/ metrics-server-89786cd84-t9dwm/ephemeralcontainers -f ec.json
```

然后就可以使用以下exec或者attach命令连接新的临时容器（-c指定为临时容器的名称）：

```shell
$ kubectl attach -it metrics-server-89786cd84-t9dwm -n kube- system -c debugger
```

如果启用了进程命名空间共享，则可以在临时容器中查看该Pod所有容器中的进程。

例如，运行上述attach操作后，在调试器容器时进行ps和netstat操作：

```shell
$ ps aux

$ netstat -lntp
```

在临时容器busybox中执行ps aux能看到其他容器的进程，执行netstat也能看到Pod中所有容器的启动端口号，可以看到在业务容器没有基本工具的前提下，使用临时容器大大提高了排查故障的效率。


（2）在1.18+版本中临时容器的使用方法 
Kubernetes版本低于1.18时，使用临时容器在线Debug的过程还是比较麻烦的，在1.18版本之后（包括1.18),使用临时容器在线Debug就相对简单了，可以直接使用kubectl alpha debug进行调试，比如上述操作可以用一条命令代替：


```shell
$ kubectl -n kube-system alpha debug metrics-server-89786cd84- t9dwm -i --image=busybox
```

!!! Warning "**注意**"

    
     1.18版本也需要开启临时容器功能。1.19以上版本kubectl命令不用再加alpha字段。



!!! abstract "使用临时调试容器来进行调试"

    https://kubernetes.io/zh-cn/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container



!!! abstract "Kubernetes中使用临时容器进行故障排查的方法"

    https://www.zhangshengrong.com/p/OQNzr70eNR/


## 3. 自动扩缩容HPA


在集群安装的第1、2章，我们安装了一个叫Metrics Server的组件，该组件在集群中负责采集Pod和Node的度量指标，
比如Pod的CPU、内存使用率和节点的内存、CPU使用率，而且安装的Dashboard可以展示CPU、内存信息也是 依靠Metrics Server的。

当然，该组件不仅仅是用来展示数据的，还可以使用Metrics Server提供的数据结合Kubernetes的HPA功能实现Pod的自动扩容和缩容。


### 3.1 什么是HPA
HPA（Horizontal Pod Autoscaler，水平Pod自动伸缩器）可以根据观察到的CPU、内存使用率或自定义度量标准来自动扩展或缩容Pod的数量。

注意： HPA不适用于无法缩放的对象，比如DaemonSet。

HPA控制器会定期调整RC或Deployment的副本数，以使观察到的平均CPU 利用率与用户指定的目标相匹配。


HPA需要Metrics Server（项目地址：https://github.com/kubernetes-
sigs/metrics-server）获取度量指标，由于在高可用集群安装中已经安装了Metrics Server，所以本节的实践部分无须再次安装。



### 3.2 HPA实践--实现Web服务器的自动伸缩特性

在生产环境中，总会有一些意想不到的事情发生，比如公司网站流量突 然升高，此时之前创建的Pod已不足以支撑所有的访问，
而运维人员也不可能 24小时守着业务服务，这时就可以通过配置HPA，实现负载过高的情况下自动扩容Pod副本数以分摊高并发的流量，当流量恢复正常后，HPA会自动缩减Pod的数量。



本节将测试实现一个Web服务器的自动伸缩特性，具体步骤如下：

hpa-demo.yml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpa-demo
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: 50Mi
            cpu: 50m
```

然后重新更新 Deployment，重新创建 HPA 对象：

```shell
$ kubectl apply -f hpa-demo.yml
deployment.apps/hpa-demo configured

$ kubectl get pods -o wide -l app=nginx
NAME                        READY   STATUS    RESTARTS   AGE     IP            NODE         NOMINATED NODE   READINESS GATES
hpa-demo-69968bb59f-twtdp   1/1     Running   0          4m11s   10.244.4.97   ydzs-node4   <none>           <none>

$ kubectl delete hpa hpa-demo
horizontalpodautoscaler.autoscaling "hpa-demo" deleted

$ kubectl autoscale deployment hpa-demo --cpu-percent=10 --min=1 --max=10
horizontalpodautoscaler.autoscaling/hpa-demo autoscaled

$ kubectl describe hpa hpa-demo
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Tue, 19 Nov 2019 17:23:49 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       1 current / 1 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:           <none>

$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    1         10        1          52s 
```

现在可以看到 HPA 资源对象已经正常了，现在我们来增大负载进行测试，我们来创建一个 busybox 的 Pod，并且循环访问上面创建的 Pod：

```shell
$ kubectl run -it --image busybox test-hpa --restart=Never --rm /bin/sh
If you don't see a command prompt, try pressing enter.
/ # while true; do wget -q -O- http://10.244.4.97; done
```

如下可以看到，HPA 已经开始工作：

```shell
$  kubectl get hpa
NAME       REFERENCE             TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   338%/10%   1         10        1          5m15s

$ kubectl get pods -l app=nginx --watch
NAME                        READY   STATUS              RESTARTS   AGE
hpa-demo-69968bb59f-8hjnn   1/1     Running             0          22s
hpa-demo-69968bb59f-9ss9f   1/1     Running             0          22s
hpa-demo-69968bb59f-bllsd   1/1     Running             0          22s
hpa-demo-69968bb59f-lnh8k   1/1     Running             0          37s
hpa-demo-69968bb59f-r8zfh   1/1     Running             0          22s
hpa-demo-69968bb59f-twtdp   1/1     Running             0          6m43s
hpa-demo-69968bb59f-w792g   1/1     Running             0          37s
hpa-demo-69968bb59f-zlxkp   1/1     Running             0          37s
hpa-demo-69968bb59f-znp6q   0/1     ContainerCreating   0          6s
hpa-demo-69968bb59f-ztnvx   1/1     Running             0          6s
```

我们可以看到已经自动拉起了很多新的 Pod，最后定格在了我们上面设置的 10 个 Pod，同时查看资源 hpa-demo 的副本数量，副本数量已经从原来的1变成了10个：

```shell
$ kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   10/10   10           10          17m
```

查看 HPA 资源的对象了解工作过程：

```shell
$ kubectl describe hpa hpa-demo
Name:                                                  hpa-demo
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Tue, 19 Nov 2019 17:23:49 +0800
Reference:                                             Deployment/hpa-demo
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 10%
Min replicas:                                          1
Max replicas:                                          10
Deployment pods:                                       10 current / 10 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  5m45s  horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  5m30s  horizontal-pod-autoscaler  New size: 8; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  5m14s  horizontal-pod-autoscaler  New size: 10; reason: cpu resource utilization (percentage of request) above target
```

同样的这个时候我们来关掉 busybox 来减少负载，然后等待一段时间观察下 HPA 和 Deployment 对象：

```shell
$ kubectl get hpa
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
hpa-demo   Deployment/hpa-demo   0%/10%    1         10        1          14m

$ kubectl get deployment hpa-demo
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
hpa-demo   1/1     1            1           24m
```





**缩放间隙**

从Kubernetes v1.12版本开始我们可以通过设置kube-controller-manager组件的`--horizontal-pod-autoscaler-downscale-stabilization`参数来设置一个持续时间，
用于指定在当前操作完成后，HPA 必须等待多长时间才能执行另一次缩放操作。

默认为5分钟，也就是默认需要等待5分钟后才会开始自动缩放。

可以看到副本数量已经由10变为1，当前我们只是演示了CPU使用率这一个指标，在后面的课程中我们还会学习到根据自定义的监控指标来自动对 Pod 进行扩缩容。



!!! question "Pod水平自动扩缩"

    https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale/



!!! question "HorizontalPodAutoscaler演练"
    
    https://kubernetes.io/zh-cn/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/








## 4. Taint和Toleration

在生产环境中，经常会有这样的需求：

- Master节点只部署系统组件容器，比如Calico、Metrics Server、 Dashboard等，不应该部署业务应用。

- 新添加节点不应该立马就允许部署业务容器，也就是新节点需要经过完整性及稳定性测试才可以被允许调度。

- 某些节点可能需要进行系统升级或者其他维护，可能会引起节点上的容器不可用，此时需要将该节点上的Pod漂移至其他节点，再进行维护。

- 有一些GPU服务器或其他专用节点服务器，除了指定的Pod外，并不想让它们部署其他Pod。



面对这样的需求，Kubernetes抽象了污点(Taint)和容忍(Toleration)的概念，可以非常方便地实现这些需求。

### 4.1 容忍和污点的基本概念

Taint作用在节点上，能够使节点排斥一类特定的Pod，也就是不能“兼 容”该节点的污点的Pod。

对应的Toleration作用在Pod上，意为容忍，也就是可以兼容某类污点。



Taint和Toleration相互配合可以用来避免Pod被分配到不合适的节点，比如有一批GPU服务器只能部署要使用GPU的Pod。

每个节点上都可以应用一个或多个Taint，这表示对于那些不能容忍这些Taint的Pod是不能部署在该服务器上的。

如果Pod配置了Toleration，则表示这些Pod可以被调度到设置了Taint的节点上，当然没有设置Taint的节点也是可以部署的。

给节点增加一个Taint也很简单，直接使用kubectl taint命令即可，比如给k8s-node01节点设置一个Taint：

```shell
# kubectl taint nodes k8s-node01 taintKey=taintValue:NoSchedule
node/k8s-node01 tainted
```

上述命令给k8s-node01增加了一个Taint，它的taintKey(可以设置为其他值)对应的就是Taint的键，
taintValue(可以设置为其他值)对应就是Taint的值,Effect对应的就是NoSchedule， 顾名思义，也就是这个Taint的"影响"相当于Taint的"级别"。


这表明只有和这个Taint相匹配的Toleration的Pod才能够被分配到k8s-node01节点上。

按如下方式在PodSpec中定义Pod的Toleration，就可以将Pod部署到该节点上。


!!! tip "方式一"

    > 完全匹配，operator为Equal，比如能容忍key名为taintKey、value为taintValue、effect为NoSchedule的Taint（Toleration可以定义多个）：

```
tolerations:
- key: "taintKey"
  operator: "Equal"
  value: "taintValue"
  effect: "NoSchedule"
```

!!! tip "方式二"

    > 不完全匹配，operator为Exists，比如能容忍key名为taintKey、effect为NoSchedule的Taint，不考虑Taint的value是什么：


```
tolerations:
- key: "taintKey"
  operator: "Exists"
  effect: "NoSchedule"
```

当然，还可以匹配更大的范围，比如能容忍key名为taintKey的Taint：

``` 
- key: "taintKey"
  operator: "Exists"
```

综上可知， 如果Pod的Toleration配置的operator为Exists(此时toleration不指定value），
那么一个Toleration和一个Taint相匹配是指它们有一样的key和effect，如果operator是Equal，则它们的value也应该相等。


注意两种特殊情况：
- 如果一个Toleration的key、value和effect均为空，且operator为Exists，表示这个Toleration与任意的key、value和effect都匹
  配，即这个Toleration能容忍任意的Taint：
  
``` 
tolerations:
- operator: "Exists"
```

- 如果一个Toleration的effect为空，则这个Toleration可以容忍任何key为名taintKey的Taint，无论该Taint的value和effect是什么：
``` 
tolerations:
- key: "taintKey"
  operator: "Exists"
```

上述例子使用到effect的一个值NoSchedule（禁止调度的意思，一般该节点要维护或者刚添加节点会配置该Taint），
也可以使用PreferNoSchedule，该值定义尽量避免将Pod调度到存在其不能容忍的Taint 的节点上，但并不是强制的，也就是说，一个没有配置Toleration的Pod会优
先部署至其他节点，没有其他可以调度的节点时，还是可以部署到effect为 PreferNoSchedule的节点上的，NoSchedule没有这种机制。

Effect的值还可 以设置为NoExecute，如果一个节点的Taint的Effect配置为NoExecute，那么已经在这个节点上的Pod没有配置容忍该Taint的Toleration，
这个节点上的Pod会在指定时间内被驱逐到其他节点上，但Effect为NoSchedule或PreferNoSchedule时不会驱逐已经在该节点上的Pod。



一个节点可以设置多个Taint，也可以给一个Pod添加多个Toleration。

Kubernetes处理多个Taint和Toleration的过程就像一个过滤器：
从一个节点 的所有Taint开始遍历，过滤掉那些Pod中存在与之相匹配的Toleration的Taint，余下未被过滤的Taint的effect值决定了Pod是否会被分配到该节点， 特别是以下情况：

- 如果未被过滤的Taint中存在一个以上effect值为NoSchedule的Taint，则Kubernetes不会将Pod分配到该节点。
  
- 如果未被过滤的Taint中不存在effect值为NoExecute的Taint，但是存在effect值为PreferNoSchedule的Taint，则Kubernetes会尝试将Pod分配到该节点。
  
- 如果未被过滤的Taint中存在一个以上effect值为NoExecute的Taint，则Kubernetes不会将Pod分配到该节点(如果Pod还未在节点上运行)，或者将Pod从该节点驱逐(如果Pod已经在节点上运行）。



例如，假设给一个节点添加了以下Taint：
```shell
kubectl taint nodes k8s-node01 key1=value1:NoSchedule
kubectl taint nodes k8s-node01 key1=value1:NoExecute
kubectl taint nodes k8s-node01 key2=value2:NoSchedule
```

然后存在一个Pod，它有两个Toleration：

``` 
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
  
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
```
在上述例子中，该Pod不会被分配到上述节点，因为没有匹配第3个Taint。

但是如果给节点添加上述3个Taint之前，该Pod已经在上述节点中运行，那么它不会被驱逐，还会继续运行在这个节点上，
因为Effect为NoSchedule的Taint只是禁止让没有匹配该Taint的Pod不会被调度到该节点上，不会影响已经部署的Pod。


通常情况下，如果给一个节点添加了一个effect值为NoExecute的Taint，则任何不能容忍这个Taint的Pod都会马上被驱逐，任何可以容忍这个
Taint的Pod都不会被驱逐。

但是，如果Pod存在一个effect值为NoExecute的Toleration指定了可选属性tolerationSeconds的值，则该值表示在给节点添加了上述Taint之后，Pod还能继续在该节点上运行的时间，例如：
```
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
```
表示如果这个Pod正在某个节点运行，然后一个Effect为NoExecute的Taint被添加到其所在的节点，那么Pod还将继续在节点上运行3600秒，然后被驱逐。

如果在此之前上述Taint被删除了，则Pod不会被驱逐。

删除一个Taint和删除Label类似，使用减号（‒）即可：
```shell
# kubectl taint nodes k8s-node01 key1:NoExecute-
```

查看某个节点的Taint可以使用describe：
```shell
# kubectl describe node k8s-node01 | grep Taint
Taints:            key=value:NoSchedule
```



### 4.2 使用案例
通过Taint和Toleration可以灵活地让Pod避开某些节点或者将Pod从某些节点驱逐。 下面是几种使用案例。

（1）专用节点
如果想将某些节点专门分配给特定的一组用户使用，可以给这些节点添加一个Taint( kubectl taint nodes nodename dedicated=groupName:NoSchedule），
然后给这组用户的Pod添加一个相对应的Toleration，那么拥有上述Toleration的Pod就能够被分配到上述专用节点，同时也能够被分配到集群中的其他节点。

如果希望这些Pod只能分配到上述专用节点中，那么还需要给这些专用节点另外添加一个和上述Taint类似的Label（例如dedicated=groupName），然后给Pod增加节点亲和性要求或者使 用NodeSelector，就能将Pod只分配到添加了dedicated=groupName标签的节点上。


（2）特殊硬件的节点
在部分节点上配备了特殊硬件（比如GPU）的集群中，我们只允许特定的Pod才能部署在这些节点上。
这时可以使用Taint进行控制，添加Taint，如`kubectl taint nodes nodename special=true:NoSchedule` 或者 
`kubectl taint nodes nodename special=true:PreferNoSchedule`，然后给需要部署在这些节点上的Pod添加相匹配的Toleration即可。


（3）基于Taint的驱逐
属于alpha特性，在每个Pod中配置在节点出现问题时的驱逐行为，参考下一小节。



!!! tip "标签、污点设置举例"

```shell
# 设置标签
kubectl label nodes 172.18.0.76 node_role=giteePraefect

# 设置污点
kubectl taint nodes 172.18.0.76 GiteePraefectOnly=yes:NoSchedule


# 删除标签
kubectl label nodes 172.18.0.66 node_role-

# 删除污点
kubectl taint node 172.18.0.66 GiteeInternalOnly:NoSchedule-


# 查看标签
kubectl get node --show-labels
kubectl get node 172.18.0.66 --show-labels|grep giteeInternal


# 查看污点
kubectl describe nodes 172.18.0.66|grep -A 5 Taints
```


### 4.3 基于Taint的驱逐

之前提到过Taint的effect值NoExecute，它会影响已经在节点上运行的Pod。

如果Pod不能忍受effect值为NoExecute的Taint，那么Pod将会被马上驱逐。
如果能够忍受effect值为NoExecute的Taint，但是在Toleration定义中没有指定tolerationSeconds，则Pod还会一直在这个节点上运行。


在Kubernetes 1.6版以后已经支持（alpha）当某种条件为真时，Node Controller会自动给节点添加一个Taint，用以表示节点的问题。
当前内置的Taint包括：


- node.kubernetes.io/not-ready：节点未准备好，相当于节点状态 Ready的值为False。
- node.kubernetes.io/unreachable： Node Controller访问不到节点，相当于节点状态Ready的值为Unknown。
- node.kubernetes.io/out-of-disk：节点磁盘耗尽。
- node.kubernetes.io/memory-pressure：节点存在内存压力。
- node.kubernetes.io/disk-pressure：节点存在磁盘压力。
- node.kubernetes.io/network-unavailable：节点网络不可达。
- node.kubernetes.io/unschedulable：节点不可调度。
- node.cloudprovider.kubernetes.io/uninitialized：如果Kubelet 启动时指定了一个外部的cloudprovider，它将给当前节点添加一个Taint将其标记为不可用。在cloud-controller-manager的一个Controller初始化这个节点后，Kubelet将删除这个Taint。

使用这个alpha功能特性结合tolerationSeconds，Pod就可以指定当节点出现一个或全部上述问题时，Pod还能在这个节点上运行多长时间。


比如，一个使用了很多本地状态的应用程序在网络断开时，仍然希望停 留在当前节点上运行一段时间，愿意等待网络恢复以避免被驱逐。
在这种情 况下，Pod的Toleration可以这样配置：

```
tolerations:
- key: "node.cloudprovider.kubernetes.io/uninitialized"
  operator: "Exists"
  effect: "NoExecute"
  tolerationSeconds: 6000
```

---

!!! info "注意事项"

    Kubernetes会自动给Pod添加一个key为node.kubernetes.io/not-ready的 Toleration并配置tolerationSeconds=300，
    同样也会给Pod添加一个key为 `node.kubernetes.io/unreachable`的Toleration并配置 tolerationSeconds=300，除非用户自定义了上述key，否则会采用这个默认设置，
    意思就是说如果节点由于某些原因造成不可用，默认情况下Pod会 在5分钟后才会漂移至其他节点，对于要求高可用率的Pod按需修改这两处
    的tolerationSeconds为较短的值，就可以在节点故障后快速进行故障恢复，当然也不能太短，因为可能某些节点是由于网络波动造成的不可用，所以该值可以根据实际需求场景进行设置。
---


默认情况下，自动添加Toleration的机制保证了在其中一种问题被检测到时，Pod默认还能继续停留在当前节点运行5分钟。

这两个默认Toleration是由DefaultTolerationSeconds admission controller添加的。

DaemonSet中的Pod被创建时，针对以下Taint自动添加的NoExecute的Toleration将不会指定tolerationSeconds：
- node.alpha.kubernetes.io/unreachable。
- node.kubernetes.io/not-ready。

这保证了出现上述问题时，DaemonSet中的Pod永远不会被驱逐，当然DaemonSet的Pod不需要进行漂移。




## 5. Affinity亲和力

