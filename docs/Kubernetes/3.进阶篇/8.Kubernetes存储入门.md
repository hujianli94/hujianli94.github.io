# Kubernetes存储入门

前面已经讲解了Kubernetes的大部分知识点，相信读者已经慢慢领悟到了Kubernetes的精髓。但是在生产环境中，还不足以满足企业内各种复杂的场景，因为我们并没有讲到任何关于数据存储和数据交换的内容。

而数据是一个企业发展的核心，在生产中是尤为重要的一部分，所以本章将带领读者学习Kubernetes另一个比较重要并且稍微有些难度的知识点——数据持久化Volume。






## 1. Volume的概念

对于大多数项目而言，数据文件的存储是非常常见的需求，比如存储用户上传的头像、上传的文件以及数据库的数据。

在Kubernetes中，由于应用的部署具有高度的可扩展性和编排能力（不像传统架构部署在固定的位置），因此把数据存放在本地是非常不可取的，这样做也无法保障数据的安全。



如果读者对云原生应用有所了解，就会知道其中的一个概念——把有状态应用变成无状态应用，意指把数据从应用中剥离出来，把产生的数据文件或者缓存信息都放在“云端”，
比如常用的NFS（生产环境极不建议使用，因为NFS存在单点故障，推荐使用分布式存储或者公有云的NAS服务）、Ceph、GlusterFS（可能会停止维护）、Minio等。

在传统架构中，如果要使用这些存储，需要提前在宿主机挂载，然后程序才能访问，在实际使用时，经常碰到新加节点忘记挂载存储导致的一系列问题。而Kubernetes在设计之初就考虑了这些问题，并抽象出Volume的概念用于解决数据存储的问题。



在容器中的磁盘文件是短暂的，当容器崩溃时，Kubelet会重新启动容器，但容器运行时产生的数据文件都将会丢失，之后容器会以最干净的状态启动。

另外，当一个Pod运行多个容器时，各个容器可能需要共享一些文件，诸如此类的需求都可以使用Volume解决。





Docker也有卷的概念，但是在Docker中，卷只是磁盘上或另一个容器中的目录，其生命周期不受管理。

虽然Docker已经提供了卷驱动程序，但是功能非常有限，例如从Docker 1.7版本开始，每个容器只允许一个卷驱动程序，并且无法将一些特殊的参数传递给后端存储。



从本质上讲，和虚拟机或者物理机一样，卷被挂载后，在容器中也只是一个目录，可能包含一些数据，Pod中的容器也可以对其进行增删改查操作，使用方式和裸机挂载几乎没有区别。

要使用卷也非常简单，和其他参数类似，Pod只需要通过.spec.volumes字段指定为Pod提供的卷，
然后在容器中配置块，使用.spec.containers.volumeMounts字段指定卷挂载的目录即可。



## 2. Volume的类型

在传统架构中，企业内可能有自己的存储平台，比如NFS、Ceph、GlusterFS、Minio等。如果读者的环境在公有云，可以使用公有云提供的NAS、对象存储等。在Kubernetes中，Volume也支持配置以上存储，用于挂载到Pod中实现数据的持久化。

Kubernetes Volume支持的卷的类型有很多，以下为常用的卷：

- CephFS。
- GlusterFS。
- ISCSI。
- Cinder。
- NFS。
- RBD。
- HostPath。

当然，也支持一些Kubernetes独有的类型：

- ConfigMap：用于存储配置文件。
- Secret：用于存储敏感数据。
- EmptyDir：用于一个Pod内多个容器的数据共享。
- PersistentVolumeClaim：对PersistentVolume的申请。

以上列举的是一些比较常用的类型，其他支持的类型可以查看Volume的官方文档：

https://kubernetes.io/docs/concepts/storage/volumes/

接下来通过几个比较常用的类型的Volume配置演示一下Volume的使用。



## 3. 示例1: 通过emptyDir共享数据

EmptyDir是一个特殊的Volume类型，与上述Volume不同的是，

**如果删除Pod，emptyDir卷中的数据也将被删除，所以一般emptyDir用于Pod中的不同Container共享数据，**

比如一个Pod存在两个容器A和B，容器A需要使用容器B产生的数据，此时可以采用emptyDir共享数据，类似的使用如Filebeat收集容器内程序产生的日志。



使用emptyDir卷的示例如下，直接指定emptyDir为{}即可：

nginx-empty.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.15.2
        imagePullPolicy: IfNotPresent
        name: nginx
        volumeMounts:
        - mountPath: /opt
          name: share-volume
      - image: nginx:1.15.2
        imagePullPolicy: IfNotPresent
        name: nginx2
        command:
        - sh
        - -c
        - sleep 3600
        volumeMounts:
        - mountPath: /mnt
          name: share-volume
      volumes:
      - name: share-volume
        emptyDir: {}
          #medium: Memory
```

此部署文件创建一个Deployment，采用spec.volumes字段配置了一个名字为share-volume、类型为emptyDir的Volume，同时里面包含两个容器nginx和nginx2，
并将该Volume挂载到了/opt和/mnt目录下，此时/opt和/mnt目录的数据就实现了共享。

默认情况下，emptyDir支持节点上的任何介质，可能是SSD、磁盘或网络存储，具体取决于自身的环境。

可以将emptyDir.medium字段设置为Memory，让Kubernetes使用tmpfs（内存支持的文件系统），
虽然tmpfs非常快，但是tmpfs在节点重启时，数据同样会被清除，并且设置的大小会被计入Container的内存限制中。



## 4. 示例2: 使用HostPath挂载宿主机文件

**HostPath卷可将节点上的文件或目录挂载到Pod上，用于实现Pod和宿主机之间的数据共享**，


**HostPath 数据存放在主机上的文件不会被销毁，会永久保存。Pod销毁后，若又在这台机器上重建，则可以读取原来的内容，但如果机器出现故障或者Pod被调度到其他机器上，就无法读取原来的内容了。**


常用的示例有挂载宿主机的时区至Pod，或者将Pod的日志文件挂载到宿主机等。

以下为使用hostPath卷的示例，实现将主机的/etc/timezone文件挂载到Pod的/etc/timezone（挂载路径可以不一致）：


nginx-hostPath.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.15.2
        imagePullPolicy: IfNotPresent
        name: nginx
        volumeMounts:
        - mountPath: /opt
          name: share-volume
        - mountPath: /etc/timezone
          name: timezone
      - image: nginx:1.15.2
        imagePullPolicy: IfNotPresent
        name: nginx2
        command:
        - sh
        - -c
        - sleep 3600
        volumeMounts:
        - mountPath: /mnt
          name: share-volume
      volumes:
      - name: share-volume
        emptyDir: {}
          #medium: Memory
      - name: timezone
        hostPath:
          path: /etc/timezone
          type: File
```

在配置HostPath时，有一个type的参数，用于表达不同的挂载类型，HostPath卷常用的type（类型）如下：

- type为空字符串：默认选项，意味着挂载hostPath卷之前不会执行任何检查。

- DirectoryOrCreate：如果给定的path不存在任何东西，那么将根据需要创建一个权限为0755的空目录，和Kubelet具有相同的组和权限
- Directory：目录必须存在于给定的路径下。
- FileOrCreate：如果给定的路径不存储任何内容，则会根据需要创建一个空文件，权限设置为0644，和Kubelet具有相同的组和所有权。
- File：文件，必须存在于给定路径中。
- Socket：UNIX套接字，必须存在于给定路径中。
- CharDevice：字符设备，必须存在于给定路径中。
- BlockDevice：块设备，必须存在于给定路径中。



## 5. 示例3: 挂载NFS至容器

在生产环境中，考虑到数据的高可用性，仍然不建议使用NFS作为后端存储。

因为NFS存在单点故障，很多企业并非对其实现高可用，并且NFS的性能存在很大的瓶颈。

所以生产中，建议使用分布式存储，在公有云中建议使用公有云提供的NAS存储来替代NFS，并且NAS性能更好，可用性更高。

NFS作为一个比较流行的远端存储工具，在非生产环境中是一个比较好用的选择，可以快速地搭建使用。


接下来演示如何使用Volume挂载NFS为容器提供持久化存储。

### 5.1 安装NFS

=== "Debian/Ubuntu"


    ```sh
    $ apt install nfs-kernel-server

    # 配置好NFS共享目录后使用如下命令重启服务
    $ service rpcbind restart
    $ service nfs-kernel-server restart
    ```



=== "CentOS/RHEL/Fedora"


    ```sh
    $ yum install -y nfs-utils rpcbind

    # 配置好NFS共享目录后使用如下命令重启服务
    $ service rpcbind restart
    $ service nfs restart
    ```




服务端应用安装完毕后，创建一个目录，将其作为NFS共享目录，以便客户端可以访问共享目录中的内容。

```sh
$ mkdir -p /data/nfs
```


在开始之前，需要提前配置NFS的权限和数据目录，具体可以参考NFS的文档，此处只提供NFS的exporter配置，

假设NFS提供的数据目录为/data/nfs，授权可挂载该目录的IP段为192.168.0.0/24：



```sh
$ vim /etc/exports
/data/nfs/ 192.168.0.0/24(rw,sync,no_subtree_check,no_root_squash)
# 或者如下
# /data/nfs            * (rw,sync,insecure,no_subtree_check,no_root_squash)

```

最后小括号中的参数为权限设置，

- rw表示允许读写访问，
- sync表示所有数据在请求时写入共享目录，
- insecure表示NFS通过1024以上的端口进行发送，
- no_root_squash表示root用户对根目录具有完全的管理访问权限,
- no_subtree_check表示不检查父目录的权限。



最后，通过以下命令，检查服务器端是否正常加载了/etc/exports的配置。

```sh
$ sudo showmount -e localhost
或者
$ exportfs
```


> 使用容器的方式部署nfs参考如下
> 
> https://github.com/ehough/docker-nfs-server
> 
> https://gitee.com/atompi/nfs-server-docker.git 
> 
> https://github.com/sjiveson/nfs-server-alpine.git 
> 
> https://github.com/maggie0002/docker-nfs.git



和emptyDir、HostPath的配置方法类似，NFS的Volume配置也是在Volumes字段中配置的，和emptyDir不同的是，NFS属于持久化存储的一种，在Pod删除或者重启后，数据依旧会存储在NFS节点上。



NFS 是 Network File System 的缩写，即网络文件系统。Kubernetes 中通过简单地配置就可以挂载 NFS 到 Pod 中，而 NFS 中的数据是可以永久保存的，同时 NFS 支持同时写操作。

配置NFS也相对简单，代码如下（挂载方式和hostPath、emptyDir类似，在此不再演示）：

```yaml
volumes:
- name: nfs
  nfs:
    # FIXME: use the right hostname
    server: 10.254.234.223
    path: "/"
```

- Server：NFS服务器的IP。
- Path：NFS服务器的共享目录。


示例如下

```yaml
      volumeMounts:
      - name: localtime
        mountPath: /etc/localtime
      - name: grms-config-toml
        mountPath: /app/config.toml
        subPath: config.toml
  volumes:
  - name: localtime
    hostPath:
      path: /usr/share/zoneinfo/Asia/Shanghai
  - name: grms-config-toml
    nfs:
      # FIXME: use the right hostname
      server: 192.168.240.23
      path: "/confcenter-data/data/conf/config.toml"
```


完整yaml示例如下：

为了演示NFS存储卷的使用方式，首先，创建 exampledeployfornfs.yml 文件。


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: exampledeployfornfs
spec:
  replicas: 2
  selector:
    matchLabels:
      example: examplefornfs
  template:
    metadata:
      labels:
        example: examplefornfs
    spec:
      containers:
      - name: containerfornfs
        image: busybox
        imagePullPolicy: IfNotPresent
        command: ['sh', '-c']
        args: ['echo "The host is $(hostname)" >> /dir/data; sleep 3600']
        volumeMounts:
          - name: nfsdata
            mountPath: /dir
      volumes:
      - name: nfsdata
        nfs:
          path: /data/nfs
          server: 192.168.1.60
```


!!! Warning "**注意**"

    Kubernetes所有的节点都需要安装上 nfs-utils 才可以正常挂载NFS。



安装NFS客户端


=== "Debian/Ubuntu"


    ```sh
    $ apt install nfs-common
    ```


=== "CentOS/RHEL/Fedora"


    ```sh
    $ yum install -y nfs-utils
    ```



安装成功后，可以输入以下命令，检查是否能访问远端的NFS服务器。

```sh
$ sudo showmount -e {NFS服务器IP地址}
```




本节演示了类型为emptyDir、hostPath、NFS的Volume的使用，在配置管理ConfigMap、Secret时也讲解了类型为Secret和ConfigMap的使用，后面的章节将为读者讲解persistentVolumeClaim以及更高级的存储配置的使用。


对于其他类型的Volume配置，原理和配置方法类似，

!!! example "具体参考"

    > https://kubernetes.io/docs/concepts/storage/volumes/。



由于网络存储卷使用的是不同于Kubernetes的额外系统，因此从使用角度来说，网络存储卷存在两个问题。

- 存储卷数据清理问题，需要人工清理。

- 在Pod模板中需要配置所使用存储的细节参数，于是与所使用的存储方案产生高度耦合。若基础设施和应用配置之间没有分离，则不利于维护。

要解决以上两个问题，就需要用到持久存储卷。



## 6. PersistentVolume

上一节讲解了Volume的概念和使用，虽然Volume已经可以接入大部分存储后端，但是实际使用时还有诸多问题，比如：

- 当某个数据卷不再被挂载使用时，里面的数据如何处理？
- 如果想要实现只读挂载如何处理？
- 如果想要只能有一个Pod挂载如何处理？

如上所述，对于很多复杂的需求Volume可能难以实现，并且无法对存储卷的生命周期进行管理。另一个很大的问题是，在企业内使用Kubernetes的不仅仅是Kubernetes管理员，可能还有开发人员、测试人员以及初学Kubernetes的技术人员，对于Kubernetes的Volume或者相关存储平台的配置参数并不了解，所以无法自行完成存储的配置。

为此，Kubernetes引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim。PersistentVolume（简称PV）是由Kubernetes管理员设置的存储，PersistentVolumeClaim（简称PVC）是对PV的请求，表示需要什么类型的PV。它们同样是集群中的一类资源，但其生命周期比较独立，管理员可以单独对PV进行增删改查，不受Pod的影响，生命周期可能比挂载它的其他资源还要长。

如果一个Kubernetes集群的使用者并非只有Kubernetes管理员，那么可以通过提前创建PV，用以解决对存储概念不是很了解的技术人员对存储的需求。和单独配置Volume类似，PV也可以使用NFS、GFS、CEPH等常用的存储后端，并且可以提供更加高级的配置，比如访问模式、空间大小以及回收策略等。

目前PV的提供方式有两种：**静态或动态。静态PV由管理员提前创建，动态PV无须提前创建**（动态PV在8.8节进行讲解）。



### 6.1 PV回收策略

当用户使用完卷时，可以从API中删除PVC对象，从而允许回收资源。回收策略会告诉PV如何处理该卷。

目前回收策略可以设置为Retain、Recycle和Delete：

- Retain：保留，该策略允许手动回收资源，当删除PVC时，PV仍然存在，Volume被视为已释放，管理员可以手动回收卷。
- Recycle：回收，如果Volume插件支持，Recycle策略会对卷执行rm -rf清理该PV，并使其可用于下一个新的PVC，但是本策略将来会被弃用，目前只有NFS和HostPath支持该策略。
- Delete：删除，如果Volume插件支持，删除PVC时会同时删除PV，动态卷默认为Delete，目前支持Delete的存储后端包括AWS EBS、GCE PD、Azure Disk、OpenStack Cinder等。



### 6.2 PV访问策略

在实际使用PV时，可能针对不同的应用会有不同的访问策略，比如某类Pod可以读写，某类Pod只能读，或者需要配置是否可以被多个不同的Pod同时读写等，此时可以使用PV的访问策略进行简单控制，目前支持的访问策略如下：

- ReadWriteOnce：可以被单节点以读写模式挂载，命令行中可以被缩写为RWO。
- ReadOnlyMany：可以被多个节点以只读模式挂载，命令行中可以被缩写为ROX。
- ReadWriteMany：可以被多个节点以读写模式挂载，命令行中可以被缩写为RWX。
- ReadWriteOncePod：只能被一个Pod以读写的模式挂载，命令中可以被缩写为RWOP（1.22以上版本）。



虽然PV在创建时可以指定不同的访问策略，但是也要后端的存储支持才行。

比如一般情况下大部分块存储是不支持ReadWriteMany的，

具体后端存储支持的访问模式可以参考：https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes。



在企业内，可能存储很多不同类型的存储，比如NFS、Ceph、GlusterFS等，针对不同类型的后端存储具有不同的配置方式，这也是对集群管理员的一种挑战，因为集群管理员需要对每种存储都有所了解。接下来看一下几个常用的PV配置示例。



### 6.3 基于NFS的PV

创建一个基于NFS的PV（PV目前没有Namespace隔离，不需要指定命名空间，在任意命名空间下创建的PV均可以在其他Namespace使用）：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs-slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

- capacity：容量配置。
- volumeMode：卷的模式，目前支持Filesystem（文件系统）和Block（块），其中Block类型需要后端存储支持，默认为文件系统。l accessModes：该PV的访问模式。
- storageClassName：PV的类，一个特定类型的PV只能绑定到特定类别的PVC。
- persistentVolumeReclaimPolicy：回收策略。
- mountOptions：非必需，新版本中已弃用。
- nfs：NFS服务配置，包括以下两个选项。
  - path：NFS上的共享目录。
  - server：NFS的IP地址。



!!! Warning "**注意**"
  
    一般情况下，企业内的NFS很有可能是没有高可用性的，所以在企业内部要谨慎使用NFS作为后台存储，可以使用公有云的NAS服务，和NFS的协议一致，或者使用支持文件系统类型的分布式存储，比如Ceph等。





### 6.4 基于HostPath的PV

可以创建一个基于hostPath的PV，和配置NFS的PV类似，只需要配置hostPath字段即可，其他配置基本一致：

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```



配置参数说明：

- hostPath：宿主机路径配置。


!!! Warning "**注意**"
  
    使用hostPath类型需要固定Pod所在的节点，防止Pod漂移造成数据丢失。



### 6.5 基于Ceph RBD的PV

Ceph可能是目前企业内最常用的一种分布式存储之一，同时支持文件系统、块存储及对象存储，和上述NFS和HostPath相比，具有高可用性和读写高效性。
接下来看一下Ceph RBD类型的PV配置：

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-rbd-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
      - 192.168.1.123:6789
      - 192.168.1.124:6789
      - 192.168.1.125:6789
    pool: rbd
    image: ceph-rbd-pv-test
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
```

配置参数说明：

- monitors：Ceph的monitor节点的IP。
- pool：所用Ceph Pool的名称，可以使用ceph osd pool ls查看。
- image：Ceph块设备中的磁盘映像文件，可以使用rbd create POOL_NAME/IMAGE_NAME--size 1024创建，使用rbd list POOL_NAME查看。
- user：Rados的用户名，默认是admin。
- secretRef：用于验证Ceph身份的密钥。
- fsType：文件类型，可以是Ext4、XFS等。
- readOnly：是否是只读挂载。



!!! Warning "**注意**"
  
    Ceph的Pool和Image需要提前创建才能使用。
    
    虽然前面讲述了RBD类型的PV配置示例，但是在实际使用时，大多数Ceph存储的使用都是采用动态存储的方式，很少通过静态方式去管理。

    同时，Kubernetes所有的节点都需要安装ceph-common才能正常挂载Ceph。



### 6.6 PV的状态

在创建PV后，可以通过kubectl get pv查看已经创建的PV的状态：

```sh
$ kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS                        REASON   AGE
gitlab-postgresql-pv                       200Gi      RWO            Retain           Bound    kube-ops/gitlab-postgresql-pvc   managed-nfs-storage                          115d
gitlab-pv                                  500Gi      RWO            Retain           Bound    kube-ops/gitlab-pvc              managed-nfs-storage  
```

其中有一个字段STATUS表示当前PV的状态，会有以下几种状态：

- Available：可用，没有被PVC绑定的空闲资源。
- Bound：已绑定，已经被PVC绑定。
- Released：已释放，PVC被删除，但是资源还未被重新使用。
- Failed：失败，自动回收失败。





## 7. PersistentVolumeClaim

由Kubernetes管理员提前创建了PV，我们应该怎样使用它呢？

这里介绍Kubernetes的另一个概念PersistentVolumeClaim（ 简 称 PVC）。

PVC是其他技术人员在Kubernetes上对存储的申请，它可以标明一个程序需要用到什么样的后端存储、多大的空间以及以什么访问模式进行挂载。
这一点和Pod的QoS配置类似，Pod消耗节点资源，PVC消耗PV资源，Pod可以请求特定级别的资源（CPU和内存），PVC可以请求特定的大小和访问模式的PV。

例如申请一个大小为5Gi且只能被一个Pod只读访问的存储。


在实际使用时，虽然用户通过PVC获取存储支持，但是用户可能需要具有不同性质的PV来解决不同的问题，比如使用SSD硬盘来提高性能。
所以集群管理员需要根据不同的存储后端来提供各种PV，而不仅仅是大小和访问模式的区别，并且无须让用户了解这些卷的具体实现方式和存储类型，达到了存储
的解藕，降低了存储使用的复杂度。

在8.6节的PV部分，我们列举了几个常用的存储类型的配置方式，接下来看一下如何让PVC和这些PV绑定。

!!! Warning "**注意**"
    
    PVC和PV进行绑定的前提条件是一些参数必须匹配，比如accessModes、storageClassName、volumeMode都需要相同，
    并且PVC的storage需要小于等于PV的storage配置。



### 7.1 PVC的创建

此时可以创建一个PVC即可与其绑定（注意PV和PVC加粗的部分，PV和PVC进行绑定并非是名字相同，而是StorageClassName相同且其他参数一致才可以进行绑定），代码如下：

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: task-pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: task-pv-claim
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```



!!! Warning "**注意**"

    > PVC具有命名空间隔离性，上述代码没有添加namespace参数，会默认创建在default命名空间。
    > 
    > 如果需要给其他命名空间的Pod使用，就应将其创建在指定的命名空间。



同样的方式也可以给NFS类型的PV创建一个PVC：

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pvc-nfs
spec:
  storageClassName: nfs-slow
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
```

从上述两个简单的例子可以看出，PVC的定义和后端存储并没有关系。

对于有存储需求的技术人员，直接定义PVC即可绑定一块PV，之后就可以供Pod使用，而不用去关心具体的实现细节，大大降低了存储的复杂度。

接下来我们看一下PVC如何提供给Pod使用。




### 7.2 PVC的使用

上述创建了PV，并使用PVC与其绑定，现在还差一步就能让程序使用这块存储，那就是将PVC挂载到Pod。和之前的挂载方式类似，PVC的挂载也是通过volumes字段进行配置的，只不过之前需要根据不同的存储后端填写很多复杂的参数，而使用PVC进行挂载时，只填写PVC的名字即可，不需要再关心任何的存储细节，这样即使不是Kubernetes管理员，不懂存储的其他技术人员想要使用存储，也可以非常简单地进行配置和使用。

比如我们将之前创建的hostPath类型的PVC挂载到Pod中，可以看到只需要配置一个persistentVolumeClaim类型的volumes，claimName配置为PVC的名称即可：

```yaml
kind: Pod
apiVersion: v1
metadata:
  name: task-pv-pod
spec:
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
       claimName: task-pv-claim
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
```

!!! Warning "**注意**"

      > claimName需要和上述定义的PVC名称task-pv-claim一致。



## 8.动态存储StorageClass

虽然使用PV和PVC能屏蔽一些存储使用上的细节，降低了存储使用的复杂度，但是也会有另一个问题无法解决。

当公司Kubernetes集群很多，并且使用它们的技术人员过多时，对于PV的创建是一个很耗时、耗力的工作，并且达到一定规模后，过多的PV将难以维护。

所以就需要某种机制用于自动管理PV的生命周期，比如创建、删除、自动扩容等，于是Kubernetes就设计了一个名为StorageClass（缩写为SC，没有命名空间隔离性）的东西，通过它可以动态管理集群中的PV，这样Kubernetes管理员就无须浪费大量的时间在PV的管理中。

在Kubernetes中，管理员可以只创建StorageClass“链接”到后端不同的存储，比如Ceph、GlusterFS、OpenStack的Cinder、其他公有云提供的存储等，之后有存储需求的技术人员，创建一个PVC指向对应的StorageClass即可，StorageClass会自动创建PV供Pod使用，也可以使用StatefulSet的volumeClaimTemplate自动分别为每个Pod申请一个PVC。


接下来看一下如何定义一个StorageClass。

### 8.1 定义StorageClass

定义一个StorageClass的示例如下：

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: slow
provisioner: kubernetes.io/glusterfs
parameters:
  resturl: "http://127.0.0.1:8081"
  clusterid: "630372ccdc720a92c681fb928f27b53f"
  restauthenabled: "true"
  restuser: "admin"
  secretNamespace: "default"
  secretName: "heketi-secret"
  gidMin: "40000"
  gidMax: "50000"
  volumetype: "replicate:3"
```


该示例是使用StorageClass链接GlusterFS的一个示例，PV和Volume可以直接配置GlusterFS，其配置参数和StorageClass的parameters类似，只不过PV是单独供PVC使用的，

Volume是针对某个Pod使用的，而StorageClass是一个全局的配置，没有命名空间隔离性，任何命名空间下有存储需求的应用都可以配置对应的StorageClass来获取存储。

StorageClass还提供了一些更高级的配置，比如provisioner、parameters、reclaimPolicy、allowVolumeExpansion等。

- Provisioner：直译过来是供应商的意思，也就是通过该参数来标明对于PV的创建使用哪个插件来执行，比如创建GlusterFS类型的PV，需要使用GlusterFS的插件才能创建，必须指定此字段。


> 目前官方支持的Provisioner可以在此处查看：https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner。



!!! Warning "**注意**"

    > Provisioner不仅限于官方列出的内部provisioner，还可以运行和指定外部供应商。
    > 
    > 例如，NFS不提供内部配置程序，但是可以使用外部配置程序，外部配置方式参见网址（其他类型的存储可以在其官方文档或者GitHub进行查找）：
    > 
    > https://github.com/kubernetes-incubator/external-storage。



- Parameters：针对后端配置的不同参数，比如上述的GlusterFS类型的StorageClass，可以指定数据的副本数（replicate:3）、Gluster REST服务/Heketi服务的URL等。该参数的定义取决于provisioner，不同的provisioner配置的参数不同。如果该参数省略会使用默认值。
- ReclaimPolicy：回收策略，可以是Delete、Retain，默认为Delete，也就是PVC被删除后，对应的PV如何处理。
- MountOptions：通过StorageClass动态创建的PV可以使用MountOptions指定挂载参数。如果指定的卷插件不支持指定的挂载选项，就不会被创建成功，因此在设置时需要进行确认。
- AllowVolumeExpansion：是否允许对PV进行扩容，需要后端存储支持，一般不推荐进行缩容。目前支持扩容的存储类型可以在以下文档查看：https://kubernetes.io/docs/concepts/storage/storage-classes/#allow-volume-expansion。



参考文献：

[StorageClass的安装和使用](https://my-docker-k8s-blog.readthedocs.io/en/latest/05.Kubernetes%E5%85%A5%E9%97%A8%E5%88%B0%E5%AE%9E%E8%B7%B5/7.%E5%AD%98%E5%82%A8%E4%B8%8E%E9%85%8D%E7%BD%AE/4.StorageClass.html)




该小节通过一个简单的示例来说明如何配置一个StorageClass，对于其他不同类型的存储配置方式只是parameters不一样，使用方式都是一样的。

> 其他类型的存储可以参考官方文档：https://kubernetes.io/docs/concepts/storage/storage-classes/#parameters。

接下来通过一个Ceph RBD的示例来演示一下StorageClass的使用方法。


### 8.2 整合StorageClass和Ceph RBD

1. Kubernetes集群的安装可以参考本书安装篇
2. Ceph集群的搭建可以参考其官方文档：https://docs.ceph.com/en/latest/install/

本小节内容假定实验环境已经有了Kubernetes集群和Ceph集群。

#### 8.2.1 部署Ceph RBD Provisioner

provi-cephrbd.yaml

```yaml
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["kube-dns","coredns"]
    verbs: ["list", "get"]
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rbd-provisioner
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: rbd-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: rbd-provisioner
  apiGroup: rbac.authorization.k8s.io

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rbd-provisioner
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-provisioner
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-provisioner
subjects:
- kind: ServiceAccount
  name: rbd-provisioner
  namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rbd-provisioner
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rbd-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rbd-provisioner
    spec:
      containers:
      - name: rbd-provisioner
        image: "registry.cn-beijing.aliyuncs.com/dotbalo/rbd-provisioner:latest"
        env:
        - name: PROVISIONER_NAME
          value: ceph.com/rbd
```

首先安装Ceph RBD的Provisioner，直接执行以下命令即可安装Ceph RBD Provisioner，并创建其相关的权限：

```sh
$ kubectl  apply -f provi-cephrbd.yaml
serviceaccount/rbd-provisioner created
clusterrole.rbac.authorization.k8s.io/rbd-provisioner created
clusterrolebinding.rbac.authorization.k8s.io/rbd-provisioner created
role.rbac.authorization.k8s.io/rbd-provisioner created
rolebinding.rbac.authorization.k8s.io/rbd-provisioner created
deployment.apps/rbd-provisioner created
```

查看Pod状态：

```sh
$ kubectl get pods -n kube-system -l app=rbd-provisioner
```



#### 8.2.2 创建Ceph Pool

接下来创建一个Kubernetes专用的Pool为Pod提供PV，当然也可以使用已经存在的Pool。

登录Ceph的管理端，创建一个名为rbdfork8s、pg_num为128（生产环境根据实际情况配置pg_num，具体可以参考Ceph文档）的Pool：

```sh
$ ceph osd pool create rbdfork8s 128
pool 'rbdfork8s' created
```

对新建的Pool进行单独授权：

```sh
$ ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=rbdfork8s'
added key for client.kube
```

初始化该Pool（不是新建的Pool无须初始化）：

```sh
$ ceph osd pool application enable rbdfork8s rbd
enabled application 'rbd' on pool 'rbdfork8s'

$ rbd pool init rbdfork8s
```

查看client.kube的Key：

```sh
$ ceph auth get-key client.kube
AQB/eopgaqSRJxAAYDs2gKTXSpcpY2m+yHJaJw==
```

在Kubernetes中创建具有该Key的Secret，供StorageClass配置使用：

```sh
$ kubectl create secret generic ceph-k8s-secret \
--type="kubernetes.io/rbd" \
--from-literal=key='AQB/eopgaqSRJxAAYDs2gKTXSpcpY2m+yHJaJw==' \
-n kube-system
```

admin用户的Key也要创建一个Secret供StorageClass使用，查看ceph admin的Key：

```sh
$ ceph auth get-key client.admin
AQA8HP9ezBHPDBAAdL+NteK0d1mjXZV47r4I7w==
```

将Key保存在Kubernetes的Secret中：

```sh
$ kubectl create secret generic ceph-admin-secret \
--type="kubernetes.io/rbd" \
--from-literal=key='AQA8HP9ezBHPDBAAdL+NteK0d1mjXZV47r4I7w==' \
-n kube-system

$ kubectl get secret -n kube-system ceph-admin-secret
```



#### 8.2.3 创建StorageClass

在Ceph的管理端查看Monitor的节点信息：

```sh
$ ceph mon dump
```

将打印的结果中的Monitor的IP地址填写到StorageClass中，一般为IP地址+6789端口（替换以下文件的monitors参数）：

rbd-sc.yaml

```yaml
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ceph-rbd
provisioner: ceph.com/rbd
parameters:
  monitors: x.x.x.x:6789,x.x.x.x:6789,x.x.x.x:6789
  pool: rbdfork8s
  adminId: admin
  adminSecretNamespace: kube-system
  adminSecretName: ceph-admin-secret
  userId: kube
  userSecretNamespace: kube-system
  userSecretName: ceph-k8s-secret
  imageFormat: "2"
  imageFeatures: layering
```

创建该StorageClass：

```sh
$ kubectl create -f rbd-sc.yaml
$ kubectl get sc
```



#### 8.2.4 创建PVC

之后创建一个PVC指向该StorageClass（注意storageClassName为上述创建的StorageClass的名字）：

rbd-pvc.yaml

```yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: rbd-pvc-test
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: ceph-rbd
  resources:
    requests:
      storage: 100Mi

```




```sh
$ kubectl create -f rbd-pvc.yaml 
```

创建完成之后，StorageClass会自动创建PV，然后与PVC进行绑定。


## 9.存储的未来：CSI


前面讲解了Kubernetes存储的多种使用方式，比如Volumes直连、PV和动态存储StorageClass，虽然PV降低了其他人员使用存储 的复杂性 ， StorageClass也降低了Kubernetes管理员管理PV的难度。

但是在Kubernetes 发展中还有一个棘手的问题，就是市面上目前有很多存储提供商，比如NFS、 Ceph、NAS等，它们都属于不同的“厂商”，由不同的团队维护，而每个公司根据自己的实际情况选择不同的存储，

Kubernetes作为一个比较流行的云原 生操作系统，想要有更好的兼容性，就需要去兼容这些存储，这就意味着 Kubernetes的维护人员需要学习每种存储的使用方法，才能开发出使用某类 存储的功能，这对于Kubernetes维护人员来说将是一种挑战，同时随着存储的增多，Kubernetes的代码会显得越来越臃肿、难以维护，所以要有一种方式来解决这类问题。


在之前的章节提到过，Kubernetes作为一个容器的编排平台，能够调度 多种不同的容器引擎，比如Containerd、Docker（1.24版本之前）、CRI-O 等。

为了能够兼容更多的容器引擎，无须Kubernetes管理员编写多余的代 码，Kubernetes提出了CRI（Container Runtime Interface，容器运行时接 口）的概念，任何开发容器引擎的公司只需要兼容这个接口，就可以被 Kubernetes调度。



为实现上述目标，Kubernetes又提出了一个新的标准CSI（Container
Storage Interface，容器存储接口），该标准相当于在容器编排平台和存储 之间提供了一个“桥梁”，只要存储平台实现了这个接口，就能为编排平台
（不限于Kubernetes）提供存储功能。这种存储的实现需要存储厂商提供符合CSI标准的驱动，只要配置StorageClass链接该驱动即可为Kubernetes提供 存储。
CSI标准将存储的代码下沉到了存储厂商，而Kubernetes维护人员无须 再关注存储细节，只需要关注Kubernetes核心代码即可。

这类开发方式称为out-of-tree，意思就是在Kubernetes核心代码外维护实现各类存储的接入，
之前的Volume 、 PV 、 StorageClass称为in-tree的开发方式，意思是在Kubernetes内部维护存储的接入。

相比之下，out-of-tree是更佳的实现方式，于是现在很多in-tree的实现方式都在慢慢地向out-of-tree迁移。


!!! tip "提示"
  
    外置存储驱动除CSI外，还有一种实现方式，称为Flex Volume，由于已经过时，本书不再介绍。针对存储的接入，应该更倾向于CSI的方式。


以上简单讲解了CSI的概念，下面通过示例了解一下CSI具体的实现方式。其实CSI的配置也是比较简单的，可以简单理解为以下几个步骤：

1）找到对应存储的CSI驱动及安装方式。
2）在Kubernetes集群中安装该驱动。
3）配置StorageClass链接至该驱动即可使用。

接下来通过两个简单的例子，即CSI对接CephFS和Ceph RBD来学习CSI具体的使用方法。


### 9.1 通过CSI连接CephFS
首先我们介绍如何通过CSI配置Ceph的文件存储，
Ceph CSI的文档可以在https://github.com/ceph/ceph-csi查看，
CephFS CSI的部署文档可以通过链接：https://github.com/ceph/ceph-csi/blob/devel/docs/deploy-cephfs.md 
查看（该链接可能会有所变化，可以从https://github.com/ceph/ceph-csi的目录中查找）。

CephFS和Ceph RBD的部署方式分为两种：
Helm安装和直接使用YAML进行安装。

由于两者涉及的YAML文件过多，在此使用Helm（Helm是Kubernetes的一种包管理工具，可以很方便地对复杂的平台进行一键式安装，Helm官方文
档：https://helm.sh/）进行安装。

首先需要在Kubernetes（本小节实验需要有Kubernetes和Ceph环境）的控制端（也就是执行Kubectl命令的机器）安装Helm的客户端工具，只需要下载Helm的二进制包即可：

```shell

$ curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
Downloading https://get.helm.sh/helm-v3.5.4-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
```

查看安装的Helm版本：
```shell
$ helm version
version.BuildInfo{Version:"v3.5.4", GitCommit:"fe51cd1e31e6a202cba7dead9552a6d418ded79a", GitTreeState:"clean", GoVersion:"go1.15.11"}
```


1．CephFS驱动安装
Helm安装完成后，进入本书该小节的文档目录，然后进入install文件夹，安装CephFS CSI：
```shell
$ kubectl create namespace ceph-csi-cephfs
namespace/ceph-csi-cephfs created

$ helm install --namespace "ceph-csi-cephfs" "ceph-csi-cephfs" .
```


!!! Warning "**注意**"

    > 由于Helm包默认的镜像地址是gcr.io仓库，国内可能无法访问，因此本书所用的包已经为读者将相关gcr镜像同步到了阿里云，可以直接安装。
    > 如果 使用的是新的Helm包，需要自行同步gcr镜像至阿里云或公司内的镜像仓库，新版安装文档：
    > https://github.com/ceph/ceph-csi/blob/devel/charts/ceph-csi-cephfs/README.md


查看CSI驱动Pod的状态：
```shell
$ kubectl get pod -n ceph-csi-cephfs
```

2．Ceph配置

之后需要在Ceph中创建一个Pool为Kubernetes集群提供存储，当然也可以使用已经存在的Pool。 

首先查看当前Ceph集群有无Filesystem类型的Pool，如果有则无须再创建，可以直接使用：

```shell
$ ceph fs ls
name: sharefs, metadata pool: sharefs-metadata, data pools: [sharefs-data0 ]
```

如果没有的话，可以创建一个文件系统类型的Pool（大小根据实际情况进行配置）：
```shell
$ ceph osd pool create sharefs-data0 128 128
$ ceph osd pool create sharefs-metadata 64 64
$ ceph fs new sharefs sharefs-metadata sharefs-data0
$ ceph fs ls
name: sharefs, metadata pool: sharefs-metadata, data pools: [sharefs-data0 ]
```

Kubernetes使用Ceph作为后端存储，会对Ceph集群的卷进行相关操作， 比如创建、删除、更改等，所以Kubernetes需要有相关的权限，
此时可以把 Ceph的认证信息保存在Secret中，之后配置StorageClass使用该Secret即可。

查看client.admin的Key：
```shell
$ ceph auth get-key client.admin
AQA8HP9ezBHPDBAAdL+NteK0d1mjXZV47r4I7w==
```

创建Secret：
```shell
$ kubectl create secret generic csi-cephfs-secret \
--type="kubernetes.io/cephfs" \
--from-literal=adminKey='AQA8HP9ezBHPDBAAdL+NteK0d1mjXZV47r4I7w==' \
--from-literal-adminID='admin' \
--namespace=ceph-csi-cephfs
```
查看并记录集群的fsid：
```shell
$ ceph fsid
48ddd55b-28ce-43f3-92a8-d17d9ad2c0de
```

查看Ceph的Monitors节点信息：

```shell
$ ceph mon dump
```
结果中的Monitor的IP+6789端口即为Ceph Monitor的地址，之后需要在StorageClass使用。



3．创建文件共享型StorageClass

创建连接Ceph的信息，其中clusterID是上述查询的fsid，monitors是上述查询的Monitor地址，subvolumeGroup相当于子目录：

ceph-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "48ddd55b-28ce-43f3-92a8-d17d9ad2c0de",
        "monitors": [
          "xxx:6789",
          "xxx:6789",
          "xxx:6789"
        ],
        "cephFS": {
          "subvolumeGroup": "cephfs-k8s-csi"
        }
      }
    ]
metadata:
  name: ceph-csi-config
```

```shell
$ kubectl create -f ceph-configmap.yaml -n ceph-csi-cephfs
```

创建StorageClass：

cephfs-csi-sc.yaml

```yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: csi-cephfs-sc
provisioner: cephfs.csi.ceph.com
parameters:
  clusterID: 48ddd55b-28ce-43f3-92a8-d17d9ad2c0de 

  fsName: sharefs

  pool: sharefs-data0

  # The secrets have to contain user and/or Ceph admin credentials.
  csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-cephfs
  csi.storage.k8s.io/controller-expand-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-cephfs
  csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-cephfs

  # (optional) The driver can use either ceph-fuse (fuse) or
  # ceph kernelclient (kernel).
  # If omitted, default volume mounter will be used - this is
  # determined by probing for ceph-fuse and mount.ceph
  # mounter: kernel

  # (optional) Prefix to use for naming subvolumes.
  # If omitted, defaults to "csi-vol-".
  # volumeNamePrefix: "foo-bar-"

reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
  - debug

```

- clusterID：Ceph集群的fsid。
- fsName：Ceph文件系统的名字。
- pool：Data Pool的名字。

csi.storage.k8s.io/provisioner-secret-name：连接信息的Secret，其他的类似。

- reclaimPolicy：回收策略，默认是删除。
- allowVolumeExpansion：允许扩容，需要后端存储支持。

创建StorageClass：
```shell
$ kubectl create -f cephfs-csi-sc.yaml
storageclass.storage.k8s.io/csi-cephfs-sc created
```

查看StorageClass：
```shell
$ kubectl get sc csi-cephfs-sc
```

4．Ceph CSI验证
创建一个PVC指向该StorageClass（storageClassName参数），查看一下是否能正常创建PV：

**vim pvc.yaml**

```yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: cephfs-pvc-test-csi
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: csi-cephfs-sc
  resources:
    requests:
      storage: 100Mi
```


test-pvc-dp.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-cephfs
  name: test-cephfs
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-cephfs
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: test-cephfs
    spec:
      containers:
      - command:
        - sh
        - -c
        - sleep 36000
        image: registry.cn-beijing.aliyuncs.com/dotbalo/debug-tools
        name: test-cephfs
        volumeMounts:
        - mountPath: /mnt
          name: cephfs-pvc-test
      volumes:
      - name: cephfs-pvc-test
        persistentVolumeClaim:
          claimName: cephfs-pvc-test-csi

```


```shell
$ kubectl create -f pvc.yaml

$ kubectl get pvc
```
可以看到已经成功创建了一个名为pvc-9cad122e-632e-408e-bce1-69186ae71cb6的PV，
状态也变成了Bound，之后可以创建一个Pod进行数据读写测试，将PV挂载到/mnt目录，可以按需修改：

test-pvc-dp.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-rbd
  name: test-rbd
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-rbd
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: test-rbd
    spec:
      containers:
      - command:
        - sh
        - -c
        - sleep 36000
        image: registry.cn-beijing.aliyuncs.com/dotbalo/debug-tools
        name: test-rbd
        volumeMounts:
        - mountPath: /mnt
          name: rbd-pvc-test
      volumes:
      - name: rbd-pvc-test
        persistentVolumeClaim:
          claimName: rbd-pvc-test-csi
```
创建该Deployment：
```shell
$ kubectl apply -f test-pvc-dp.yaml
$ kubectl get pod -l app=test-cephfs
```
在其中的副本进行数据写入：
```shell
$ kubectl exec test-cephfs-811238bfb7-splxx -- bash
# echo "hello, csi" > /mnt/test.txt 
# exit
```

在另一个副本中查看数据：
```shell
$ kubectl exec test-cephfs-85f8f8bfb7-splxg -- cat /mnt/test.txt
hello, csi
```






### 9.2 通过CSI连接Ceph RBD
和CephFS CSI类似，安装Ceph RBD CSI的步骤几乎一样，只是对应的安装文件不一样 ， 

> RBD CSI的文档可以通过https://github.com/ceph/ceph-csi/blob/devel/docs/deploy-rbd.md查看， 
> 通过Helm安装文档地址 ： https://github.com/ceph/ceph-csi/blob/devel/charts/ceph-csi-rbd/README.md。


1．Ceph RBD驱动安装

同样，该Helm包默认的镜像地址也是gcr.io，笔者已经为读者同步了相关镜像，放在了本小节的install目录，可以直接进行安装（Helm安装可以参 考8.9.1节）：

```shell
$ kubectl create namespace "ceph-csi-rbd"
namespace/ceph-csi-rbd created

# 注意命令最后的一个点
$ helm install --namespace "ceph-csi-rbd" "ceph-csi-rbd" .
NAME: ceph-csi-rbd
...
NOTES:
Examples on how to configure a storage class and start using thedriver are here:
https://github.com/ceph/ceph-csi/tree/v3.3.1/examples/rbd

```

查看Pod状态
```shell
$ kubectl get po -n ceph-csi-rbd
```




2．Ceph配置

创建Ceph Pool，如果有Pool的话可以不用创建（大小根据实际情况进行配置）：

```shell
$ ceph osd pool create rbdfork8s 128
pool 'rbdfork8s' created
```

相对于管理员用户，RBD可以单独授权一个低权限的用户管理Pool，比如 创建一个kube用户，对osd有查看权限，对rbdfork8s有读写权限（具体权限 配置可以参考Ceph文档）：

```shell
$ ceph auth add client.kube mon 'allow r' osd 'allow rwx pool=rbdfork8s'
added key for client.kube
```


初始化Pool，注意如果不是新建的就不用初始化：
```shell
$ ceph osd pool application enable rbdfork8s rbd
enabled application 'rbd' on pool 'rbdfork8s'

$ rbd pool init rbdfork8s
```

查看client.kube的key：
```shell
$ ceph auth get-key client.kube
AQB/eopgaqSRJxAAYDs2gKTXSpcpY2m+yHJaJw==
```

创建Secret：
```shell
$ kubectl create secret generic csi-rbd-secret \
--type="kubernetes.io/rbd" \
--from-literal=userKey="AQB/eopgaqSRJxAAYDs2gKTXSpcpY2m+yHJaJw==" \
--from-literal=userID="kube" \
--namespace=ceph-csi-rbd
```


查看并记录集群的fsid：
```shell
$ ceph fsid
48ddd55b-28ce-43f3-92a8-d17d9ad2c0de
```

查看Ceph的Monitors节点信息：
```shell
$ ceph mon dump
```

结果中的Monitor的IP+6789端口即为Ceph Monitor的地址，之后需要在StorageClass中使用。


3．创建StorageClass

创建StorageClass和CephFS的类似，具体细节可以参考CephFS CSI的StorageClass配置：

ceph-configmap.yaml

```yaml
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "48ddd55b-28ce-43f3-92a8-d17d9ad2c0de",
        "monitors": [
          "xxx:6789",
          "xxx:6789",
          "xxx:6789"
        ],
        "cephFS": {
          "subvolumeGroup": "cephrbd-k8s-csi"
        }
      }
    ]
metadata:
  name: ceph-csi-config

```
```shell
$ kubectl create -f ceph-configmap.yaml -n ceph-csi-rbd
```


创建StorageClass：

rbd-csi-sc.yaml

```yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: 48ddd55b-28ce-43f3-92a8-d17d9ad2c0de 
   pool: rbdfork8s
   imageFeatures: layering

   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: ceph-csi-rbd 
   csi.storage.k8s.io/controller-expand-secret-name: csi-rbd-secret
   csi.storage.k8s.io/controller-expand-secret-namespace: ceph-csi-rbd
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: ceph-csi-rbd
   csi.storage.k8s.io/fstype: ext4
reclaimPolicy: Delete
allowVolumeExpansion: true
mountOptions:
   - discard
```

4．Ceph RBD验证

创建PVC，查看是否能自动创建PV，创建的配置和之前并无太大区别，只是storageClassName换成了RBD的StorageClass名称：

pvc.yaml
```yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: rbd-pvc-test-csi
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: csi-rbd-sc
  resources:
    requests:
      storage: 100Mi

```



test-pvc-dp.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-rbd
  name: test-rbd
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-rbd
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: test-rbd
    spec:
      containers:
      - command:
        - sh
        - -c
        - sleep 36000
        image: registry.cn-beijing.aliyuncs.com/dotbalo/debug-tools
        name: test-rbd
        volumeMounts:
        - mountPath: /mnt
          name: rbd-pvc-test
      volumes:
      - name: rbd-pvc-test
        persistentVolumeClaim:
          claimName: rbd-pvc-test-csi

```


创建该Deployment：

```shell
$ kubectl apply -f test-pvc-dp.yaml
$ kubectl get pod -l app=test-cephfs
```
在其中的副本进行数据写入：
```shell
$ kubectl exec test-cephfs-811238bfb7-splxx -- bash
# echo "hello, csi rbd" > /mnt/test.txt 
# exit
```

在另一个副本中查看数据：
```shell
$ kubectl exec test-cephfs-85f8f8bfb7-splxg -- cat /mnt/test.txt
hello, csi rbd
```



K8S使用ceph-csi持久化存储之RBD

> https://mp.weixin.qq.com/s?__biz=MzAxMjk0MTYzNw==&mid=2247484067



## 10. 小结
本章主要讲解了Kubernetes存储编排功能，目前存储的方向更偏向于CSI的管理方式。

所以读者想要Kubernetes集群接入存储平台，首选的方式应该是CSI。

目前大部分存储平台的供应商都提供了CSI的驱动，读者直接安装即可使用（需要在GitHub或者对应存储平台的官方网站进行查找），
CSI的管理 方式大大降低了存储的复杂度，可以更加方便地实现存储的管理，比如创建 存储、删除、扩容、备份等。


相对于配置分离，存储分离也是云原生设计非常重要的因素，即有状态应用无状态化——把存储交给远端。

在之前的程序设计中，可能会把一些数据放在本地或者内存中，因为之前的部署方式不会具有很大的变化，所以这种架构不会有大的问题。

但是现在的架构具有很高的弹性，我们往往并不知道服务到底部署在了哪台服务器、哪个位置，所以仍旧将程序的数据放置于本地和内存中是不可取的方式。



对于文件型存储，可以使用对象存储的方式将文件直接存储在对象存储平台上。

对于缓存数据，可以使用类似Redis的中间件进行存取。

这样的话，我们的程序将会彻底变成无状态的形式，无论如何部署、重启、迁移都不会 造成数据的丢失，所以在设计程序时也要有此类意识，将有状态设计变成无状态设计。



参考文献：

https://www.qikqiak.com/k8strain2/storage/ceph/

https://imszz.com/p/4aa1a279/
