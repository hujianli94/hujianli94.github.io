# 1.network-namespace

## 1.1 网络虚拟化基石：network namespace

顾名思义，Linux 的 namespace（名字空间）的作用就是“隔离内核资源”。

在 Linux 的世界里，文件系统挂载点、主机名、POSIX 进程间通信消息队列、进程 PID 数字空间、IP 地址、user ID 数字空间等全局系统资源被 namespace 分割，装到一个个抽象的独立空间里。

而隔离上述系统资源的 namespace 分别是 `Mount namespace`、`UTS namespace`、`IPC namespace`、`PID namespace`、`network namespace` 和 `user namespace`。

对进程来说，要想使用 namespace 里面的资源，首先要“进入”（具体操作方法，下文会介绍）到这个 namespace，而且还无法跨 namespace 访问资源。Linux 的 namespace 给里面的进程造成了两个错觉：

（1）它是系统里唯一的进程。
（2）它独享系统的所有资源。

> 默认情况下，Linux 进程处在和宿主机相同的 namespace，即初始的根 namespace 里，默认享有全局系统资源。

Linux 内核自 2.4.19 版本接纳第一个 namespace：Mount namespace（用于隔离文件系统挂载点）起，到 3.8 版本的 user namespace（用于隔离用户权限），总共实现了上文提到的 6 种不同类型的 namespace。

尽管 Linux 的 namespace 隔离技术很早便存在于内核中，而且它就是为 Linux 的容器技术而设计的，但它一直鲜为人知。

直到 Docker 引领的容器技术革命爆发，它才进入普罗大众的视线——Docker 容器作为一项轻量级的虚拟化技术，它的隔离能力来自 Linux 内核的 namespace 技术。

说到 network namespace，它在 Linux 内核 2.6 版本引入，作用是隔离 Linux 系统的设备，以及 IP 地址、端口、路由表、防火墙规则等网络资源。

因此，每个网络 namespace 里都有自己的网络设备（如 IP 地址、路由表、端口范围、/proc/net 目录等）。

从网络的角度看，networknamespace 使得容器非常有用，一个直观的例子就是：由于每个容器都有自己的（虚拟）网络设备，并且容器里的进程可以放心地绑定在端口上而不必担心冲突，这就使得在一个主机上同时运行多个监听 80 端口的 Web 服务器变为可能，如图 1-1 所示。

network namespace 示意图
![image](https://cdn.jsdelivr.net/gh/hujianli94/picx-images-hosting@master/image.7i050dc8r0.webp){: .zoom}

## 1.2 初识 network namespace

和其他 namespace 一样，network namespace 可以通过系统调用来创建，我们可以调用 Linux 的 clone（）（其实是 UNIX 系统调用 fork（）的延伸）API 创建一个通用的 namespace，然后传入 CLONE_NEWNET 参数

表面创建一个 network namespace。高阶读者可以参考下文的 C 代码创建一个 network namespace。

与其他 namespace 需要读者自己写 C 语言代码调用系统 API 才能创建不同，network namespace 的增删改查功能已经集成到 Linux 的 ip 工具的 netns 子命令中，因此大大降低了初学者的体验门
槛。

下面先介绍几条简单的网络 namespace 管理的命令。

创建一个名为 netns1 的 network namespace 可以使用以下命令：

```sh
# ip netns add netns1
```

当 ip 命令创建了一个 network namespace 时，系统会在 `/var/run/netns` 路径下面生成一个挂载点。

挂载点的作用一方面是方便对 namespace 的管理，另一方面是使 namespace 即使没有进程运行也能继续存在。

一个 network namespace 被创建出来后，可以使用 ip netns exec 命令进入，做一些网络查询/配置的工作。

```sh
# ip netns exec netns1 ip link list
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
```

如上所示，就是进入 netns1 这个 network namespace 查询网卡信息的命令。

目前，我们没有任何配置，因此只有一块系统默认的本地回环设备 lo。

想查看系统中有哪些 network namespace，可以使用以下命令：

```sh
# ip netns list
netns1
```

想删除 network namespace，可以通过以下命令实现：

```sh
# ip netns delete netns1
```

注意，上面这条命令实际上并没有删除 netns1 这个 network namespace，它只是移除了这个 network namespace 对应的挂载点（下文会解释）。

只要里面还有进程运行着，network namespace 便会一直存在。

## 1.3 配置 network namespace

当 namespace 里面的进程涉及网络通信时，namespace 里面的（虚拟）网络设备就必不可少了。

通过上文的阅读我们已经知道，一个全新的 network namespace 会附带创建一个本地回环地址。除此之外，没有任何其他的网络设备。而且，细心的读者应该已经发现，network namespace 自带的 lo 设备状态还是 DOWN 的，因此，当尝试访问本地回环地址时，网络也是不通的。下面的小测试就说明了这一点。

```sh
#进入netns1这个network namespace,.ping127.0.0,1
# ip netns exec netns1 ping 127.0.0.1
connect:Network is unreachable
```

在我们的例子中，如果想访问本地回环地址，首先需要进入 netns1 这个 network namespace，把设备状态设置成 UP。

```sh
# ip netns exec netns1 ip link set dev lo up
```

然后，尝试 ping 127.0.0.1，发现能够 ping 通。

```sh
# ip netns exec netns1 ping 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.167 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.071 ms
```

但是，仅有一个本地回环设备是没法与外界通信的。如果我们想与外界（比如主机上的网卡）进行通信，就需要在 namespace 里再创建一
对虚拟的以太网卡，即所谓的 veth pair。

顾名思义，veth pair 总是成对出现且相互连接，它就像 Linux 的双向管道（pipe），报文从 veth pair 一端进去就会由另一端收到。

关于 veth pair 更详细的介绍，参见 1.2 节，本节不再赘述。

下面的命令将创建一对虚拟以太网卡，然后把 veth pair 的一端放到 netns1 network namespace。

```sh
# ip link add veth0 type veth peer name veth1
# ip link set veth1 netns netns1
```

如上所示，我们创建了 veth0 和 veth1 这么一对虚拟以太网卡。

在默认情况下，它们都在主机的根 network namespce 中，将其中一块虚拟网卡 veth1 通过 ip link set 命令移动到 netns1 network namespace。

那么，veth0 和 veth1 之间能直接通信吗？还不能，因为这两块网卡刚创建出来还都是 DOWN 状态，需要手动把状态设置成 UP。

这个步骤的操作和上文对 lo 网卡的操作类似，只是多了一步绑定 IP 地址，如下所示：

```sh
# ip netns exec netns1 ifconfig veth1 10.1.1.1/24 up
# ifconfig veth0 10.1.1.2/24 up
```

上面两条命令首先进入 netns1 这个 network namespace，为 veth1 绑定 IP 地址 10.1.1.1/24，并把网卡的状态设置成 UP，而仍在主机根 network namespace 中的网卡 veth0 被我们绑定了 IP 地址 10.1.1.2/24。

这样一来，我们就可以 ping 通 veth pair 的任意一头了。例如，在主机上 ping 10.1.1.1（netns1 network namespace 里的网卡），如下所示：

```sh
# ping 10.1.1.1
PING 10.1.1.1 (10.1.1.1) 56(84) bytes of data.
64 bytes from 10.1.1.1: icmp_seq=1 ttl=64 time=0.040 ms
64 bytes from 10.1.1.1: icmp_seq=2 ttl=64 time=0.100 ms
```

同理，我们可以进入 netns1 network namespace 去 ping 主机上的虚拟网卡，如下所示：

```sh
# ip netns exec netns1 ping 10.1.1.2
PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.
64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.168 ms
64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.058 ms
```

另外，不同 network namespace 之间的路由表和防火墙规则等也是隔离的，因此我们刚刚创建的 netns1 network namespace 没法和主机共享路由表和防火墙，这一点通过下面的测试就能说明。

```sh
# ip netns exec netns1 route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
10.1.1.0        0.0.0.0         255.255.255.0   U     0      0        0 veth1

# ip netns exec netns1 iptables -L
Chain INPUT (policy ACCEPT)
target     prot opt source               destination

Chain FORWARD (policy ACCEPT)
target     prot opt source               destination

Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
```

如上所示，我们进入 netns1 network namespace，分别输入 route 和 iptables-L 命令，期望查询路由表和 iptables 规则，却发现空空如也。

这意味着从 netns1 network namespace 发包到因特网也是徒劳的，因为网络还不通！不信读者可以自行尝试。

想连接因特网，有若干解决方法。例如，可以在主机的根 network namespace 创建一个 Linux 网桥并绑定 veth pair 的一端到网桥上；也可以通过适当的 NAT（网络地址转换）规则并辅以 Linux 的 IP 转发功能（配置 net.ipv4.ip_forward=1）。

关于 Linux 网桥和 NAT，下文会有详细介绍，这里不再赘述。

需要注意的是，用户可以随意将虚拟网络设备分配到自定义的 network namespace 里，而连接真实硬件的物理设备则只能放在系统的根 network namesapce 中。

并且，任何一个网络设备最多只能存在于一个 network namespace 中。

进程可以通过 Linux 系统调用 clone（）、unshare（）和 setns 进入 network namespace，下面会有代码示例。

非 root 进程被分配到 network namespace 后只能访问和配置已经存在于该 network namespace 的设备。

当然，root 进程可以在 network namespace 里创建新的网络设备。

除此之外，network namespace 里的 root 进程还能把本 network namespace 的虚拟网络设备分配到其他 network namespace——这个操作路径可以从主机的根 network namespace 到用户自定义 network namespace，反之亦可。

请看下面这条命令：

```sh
# ip netns exec netns1 ip link set veth1 netns 1
```

该怎么理解上面这条看似有点复杂的命令呢？分解成两部分：

（1）ip netns exec netns1 进入 netns1 network namespace。

（2）ip link set veth1 netns 1 把 netns1 network namespace 下的 veth1 网卡挪到 PID 为 1 的进程（即 init 进程）所在的 network namespace。

通常，init 进程都在主机的根 network namespace 下运行，因此上面这条命令其实就是把 veth1 从 netns1 network namespace 移动到系统根 network namespace。

有两种途径索引 network namespace：名字（例如 netns1）或者属于该 namespace 的进程 PID，上文中用的就是后者。

对 namespace 的 root 用户而言，他们都可以把其 namespace 里的虚拟网络设备移动到其他 network namespace，甚至包括主机根 network namespace！这就带来了潜在的安全风险。

如果用户希望屏蔽这一行为，则需要结合 PID namespace 和 Mount namespace 的隔离特性做到 network namespace 之间的完全不可达，感兴趣的读者可以自行查阅相关资料。

## 1.4 实践 1 创建 network namespace

创建一个名为 ns_lrl 的 network namespace

```sh
# ip netns list
# ip netns add ns_lrl
# ip netns list
ns_lrl
```

创建了一个 network namespace 时，系统会在 /var/run/netns 路径下面生成一个挂载点

```sh
# ls /var/run/netns/ns_lrl
/var/run/netns/ns_lrl
```

挂载点的作用一方面是方便对 namespace 的管理，另一方面是使 namespace 即使没有进程运行也能继续存在。

命令空间通过下面的方式激活回环接口后

```sh
# ip netns  exec ns_lrl ip link set lo up
```

可以看到像默认 Linux namespace 一样的本地回环地址

```sh
# ip netns exec ns_lrl ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
```

做简单的 ping 测试

```sh
# ip netns exec ns_lrl  ping 127.0.0.1 -c 3
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.017 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.033 ms
64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.038 ms
```

## 1.5 实践 2 配置 network namespace 之间通信

虚拟以太网对(veth pair)

我们来看一个 Demo，两个网络命名空间如何通信,在这之前，先看一点理论，

veth pair 是 Virtual Ethernet(虚拟以太网)接口在 Linux 中的一种实现方式。veth 接口具有以下特点:

veth 是一对接口,分别位于两个不同的 network namespace 中。这一对接口通过内核实现软链接,可将不同 namespace 中的数据连接起来。数据可以直接在这一对接口间进行传输,实现了 namespace 间的数据通信。

创建两个 网络命名空间

```sh
# ip netns add net1
# ip netns add net2
# ip netns list
net2
net1
```

仅有一个本地回环设备是没法与外界通信的。如果我们想与外界(比如主机上的网卡)进行通信，就需要在 namespace 里再 创建一对虚拟的以太网卡，即所谓的 veth pair。顾名思义，veth pair 总是成对出现且相互连接，它就像 Linux 的双向管道(pipe)，报文从 veth pair 一端进去就会由另一端收到.

```sh
# ip link add veth1 netns net1 type veth peer name veth2 netns net2
# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:e2:e7:1e brd ff:ff:ff:ff:ff:ff
```

- ip link add: 创建一对接口
- veth1: 设置本地端接口名
- netns net1: 将 veth1 移动到名为 net1 的命名空间
- type veth: 指定接口类型为 veth 虚拟接口,veth 是 virtual ethernet 的缩写,即虚拟以太网接口。
- peer name veth2: 设置远端接口名
- netns net2: 将 veth2 移动到名为 net2 的命名空间

上面的操作创建了 veth1 和 veth2 这么一对虚拟以太网卡。在默认情况下，它们都在主机的根 network namespce 中，将其中一块虚拟网卡 veth1 通过 netns net1 命令移动到 net1 network namespace,另一块网卡通过 netns net2 命令移动到 命令移动到 net2 network namespace`
执行这个命令后,会在两个不同的命名空间 net1 和 net2 内各自创建一根接口:

- 在 net1 命名空间内创建接口 veth1
- 在 net2 命名空间内创建接口 veth2

这两根接口通过 peer 自动连接成一对,形成跨命名空间的虚拟链路。

在每个命名空间执行 ip link 命令，可以看到详细的信息

```sh
# ip netns exec net1 ip link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: veth1@if2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether de:44:61:8a:fb:83 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

net1 命名空间虚拟网卡 veth1 ,与名称为 net2 的命名空间相关联

```sh
# ip netns exec net2 ip link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: veth2@if2: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 3e:cd:21:ab:7f:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

net2 命名空间虚拟网卡 veth2 ,与名称为 net1 的命名空间相关联

通过 ip netns exec net1 bash 这个命令进入指定命名空间的 shell 环境，在当前 shell 中执行的命名对当前命名空间生效

```sh
# ip netns exec net1 bash
```

分配 IP 地址以及配置掩码，指定对应的虚拟网卡, 这里分配 IP 192.168.20.1/24

```sh
# ip address add 192.168.20.1/24 dev veth1
```

激活本地回环网卡

```sh
# ip link set dev lo up
```

激活虚拟网卡

```sh
# ip link set dev veth1 up
```

因为另一端 veth1 还没有打开，所以链接状态仍然显示为关闭 state DOWN

```sh
# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: veth1@if2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000
    link/ether de:44:61:8a:fb:83 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

查看路由信息，可以发现，命令空间路由相互独立,但是由于接口当前 down,这条路由实际不可用

```sh
# ip route
192.168.20.0/24 dev veth1 proto kernel scope link src 192.168.20.1
```

查看命名空间 IP 信息

```sh
# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: veth1@if2: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether de:44:61:8a:fb:83 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.20.1/24 scope global veth1
       valid_lft forever preferred_lft forever
[root@centos7-base ~]# ^C
[root@centos7-base ~]# exit
exit
```

退出第一个命名空间的 shell 环境，我们进入第二个命名空间的 shell 环境,做相同的配置 这里分配 IP 192.168.20.2/24

```sh
# ip netns exec net2 bash
# ip address add 192.168.20.2/24 dev veth2
# ip link set dev veth2 up
# ip link set dev lo up
```

这个时候，我们在看链接，状态，会发现，veth2 虚拟网卡状态为 UP 状态 state UP

```sh
# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: veth2@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 3e:cd:21:ab:7f:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

查看分配 IP 的虚拟网卡也为 UP 状态

```sh
# ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: veth2@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 3e:cd:21:ab:7f:ae brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.20.2/24 scope global veth2
       valid_lft forever preferred_lft forever
    inet6 fe80::3ccd:21ff:feab:7fae/64 scope link
       valid_lft forever preferred_lft forever
```

独立的路由信息

```sh
# ip route
192.168.20.0/24 dev veth2 proto kernel scope link src 192.168.20.2
```

回到 net1，net1 名称空间中 veth1 的链接状态也显示 UP (state UP)

```sh
# ip netns exec net1 ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: veth1@if2: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether de:44:61:8a:fb:83 brd ff:ff:ff:ff:ff:ff link-netnsid 0
```

根命名空间不知道 net1 和 net2 命名空间的 IP 配置,三者彼此隔离。

```sh
# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:e2:e7:1e brd ff:ff:ff:ff:ff:ff

# ip netns list
net2
net1
```

路由信息也为独立的路由信息

```sh
# ip route
default via 192.168.0.1 dev eth0
192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.108
```

从根网络命名空间 ping 测试到 veth1 IP 失败。这是因为 IP 192.168.20.1 属于独立的网络命名空间 net1。

```sh
# ping 192.168.20.1 -c 1
PING 192.168.20.1 (192.168.20.1) 56(84) bytes of data.
^C
--- 192.168.20.1 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
```

从 net1 和 net2 测试网络通信,命名空间使用 ping 命令。

```sh
# ip netns exec net1 ping -c 1 192.168.20.2
PING 192.168.20.2 (192.168.20.2) 56(84) bytes of data.
64 bytes from 192.168.20.2: icmp_seq=1 ttl=64 time=0.039 ms
```

输出确认 net1 名称空间中的 veth1 接口能够成功地与 net2 名称空间中的 veth2 接口通信。

```sh
# ip netns exec net2 ping -c 1 192.168.20.1
PING 192.168.20.1 (192.168.20.1) 56(84) bytes of data.
64 bytes from 192.168.20.1: icmp_seq=1 ttl=64 time=0.115 ms
```

在实际使用中，更多的是 当前的根网络命名空间和 某个网络命名空间组成 veth pair 进行通信。

```sh
[root@centos7-base ~]# ip netns add pod_ns
[root@centos7-base ~]# ip link add veth0 type veth peer name veth1
[root@centos7-base ~]# ip link set veth1 netns pod_ns
[root@centos7-base ~]# ip addr add 192.168.1.1/24 dev veth0
[root@centos7-base ~]# ip link set veth0 up
[root@centos7-base ~]# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:e2:e7:1e brd ff:ff:ff:ff:ff:ff
8: veth0@if7: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN mode DEFAULT group default qlen 1000
    link/ether 6e:44:ac:be:2a:81 brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@centos7-base ~]# ip netns exec pod_ns ip addr add 192.168.1.2/24 dev veth1
[root@centos7-base ~]# ip netns exec pod_ns ip link set veth1 up
```

查看链接状态

```sh
[root@centos7-base ~]# ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 00:0c:29:e2:e7:1e brd ff:ff:ff:ff:ff:ff
8: veth0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 6e:44:ac:be:2a:81 brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@centos7-base ~]# ip netns exec pod_ns ip link
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
7: veth1@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000
    link/ether 8a:ed:fe:e0:f8:3a brd ff:ff:ff:ff:ff:ff link-netnsid 0
[root@centos7-base ~]# ip netns exec pod_ns ip a
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
7: veth1@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 8a:ed:fe:e0:f8:3a brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.1.2/24 scope global veth1
       valid_lft forever preferred_lft forever
    inet6 fe80::88ed:feff:fee0:f83a/64 scope link
       valid_lft forever preferred_lft forever
```

在根网络命名空间对 pod_ns 网络命名空间分配 IP 进行 ping 测试

```sh
[root@centos7-base ~]# ping -c 3 192.168.1.2
PING 192.168.1.2 (192.168.1.2) 56(84) bytes of data.
64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=0.059 ms
64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=0.069 ms
```

## 1.6 容器与 host veth pair 的关系

经典容器组网模型就是 veth pair+bridge 的模式。容器中的 eth0 实际上和外面根网络命名空间上的某个 veth 是成对的(pair)关系.(这里的 bridge 主要用于 和因特网通信)

可以通过下面两种方式来获取对应关系

### 方法一

容器里面看 /sys/class/net/eth0/iflink

```sh
root@ci-base:~# docker exec -it 9abe9ad05309 bash
root@nginx:/# cat /sys/class/net/eth0/iflink
155226
root@nginx:/# exit
exit
```

然后，在主机上遍历 /sys/claas/net 下面的全部目录，查看子目录 ifindex 的值和容器里查出来的 iflink 值相当的 veth 名字，这样就找到了容器和主机的 veth pair 关系。

```sh
# grep -c 155226 /sys/class/net/*/ifindex | grep ifindex:1
/sys/class/net/veth9d13745/ifindex:1
```

### 方法二

在目标容器里执行以下命令,获取网卡索引为 94,其中 94 是 eth0 接口的 index，95 是和它成对的 veth 的 index。

```sh
root@ci-base:~# docker exec -it 9abe9ad05309 bash
root@nginx:/# ip link show eth0
155225: eth0@if155226: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether 02:42:ac:14:00:0d brd ff:ff:ff:ff:ff:ff link-netnsid 0
root@nginx:/# exit
exit
```

通过 155225 index 来定位主机上对应的虚拟网卡

```sh
root@ci-base:~# ip link show | grep 155225
155226: veth9d13745@if155225: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master br-018e2232b6de state UP mode DEFAULT group default
```

参考文献：

https://man7.org/linux/man-pages/man7/network_namespaces.7.html

https://ramesh-sahoo.medium.com/linux-network-namespace-and-five-use-cases-using-various-methods-f45b1ec5db8f

《Kubernetes 网络权威指南：基础、原理与实践》
